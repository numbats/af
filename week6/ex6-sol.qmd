---
title: "Exercise Week 6: Solutions"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, warning = FALSE)
```

```{r, message = FALSE}
library(fpp3)
library(broom)
```

# fpp3 8.8, Ex1

> Consider the the number of pigs slaughtered in Victoria, available in the `aus_livestock` dataset.

>   a. Use the `ETS()` function in R to estimate the equivalent model for simple exponential smoothing. Find the optimal values of $\alpha$ and $\ell_0$, and generate forecasts for the next four months.

```{r ex1a}
fit <- aus_livestock |>
  filter(Animal == "Pigs", State == "Victoria") |>
  model(ses = ETS(Count ~ error("A") + trend("N") + season("N")))
report(fit)
```

Optimal values are $\alpha = `r format(broom::tidy(fit)[["estimate"]][1])`$  and $\ell_0 = `r format(broom::tidy(fit)[["estimate"]][2])`$

```{r ex1b, dependson="ex1a"}
fc <- fit |> forecast(h = "4 months")
fc
fc |>
  autoplot(filter(aus_livestock, Month >= yearmonth("2010 Jan")))
```

>   b. Compute a 95% prediction interval for the first forecast using $\hat{y} \pm 1.96s$ where $s$ is the standard deviation of the residuals. Compare your interval with the interval produced by R.

```{r ex1c, dependson="ex1b"}
s <- augment(fit) |>
  pull(.resid) |>
  sd()
yhat <- fc |>
  pull(.mean) |>
  head(1)
yhat + c(-1, 1) * 1.96 * s
```

```{r ex1d, dependson="ex1b"}
fc |>
  head(1) |>
  mutate(interval = hilo(Count, 95)) |>
  pull(interval)
```

The intervals are close but not identical. This is because R estimates the variance of the residuals differently, taking account of the degrees of freedom properly (and also using a more accurate critical value rather than just 1.96).

Try the following.

```{r ex1e, dependson="ex1a"}
res <- augment(fit) |> pull(.resid)
s <- sqrt(sum(res^2) / (length(res) - NROW(tidy(fit))))
yhat + c(-1, 1) * qnorm(0.975) * s
```

# fpp3 8.8, Ex2

> Write your own function to implement simple exponential smoothing. The function should take arguments `y` (the response data), `alpha` (the smoothing parameter $\alpha$) and `level` (the initial level $\ell_0$). It should return the forecast of the next observation in the series. Does it give the same forecast as `ETS()`?

```{r}
my_ses <- function(y, alpha, level) {
  yhat <- numeric(length(y) + 1)
  yhat[1] <- level
  for (i in 2:(length(yhat))) {
    yhat[i] <- alpha * y[i - 1] + (1 - alpha) * yhat[i - 1]
  }
  return(last(yhat))
}

vic_pigs_vec <- aus_livestock |>
  filter(Animal == "Pigs", State == "Victoria") |>
  pull(Count)

ses_fc <- vic_pigs_vec |>
  my_ses(alpha = 0.3221, level = 100646.6)

c(my_ses = ses_fc, fable = fc$.mean[1])
```

Yes, the same forecasts are obtained. The slight differences are due to rounding of $\alpha$ and $\ell_0$.

# fpp3 8.8, Ex3

> Modify your function from the previous exercise to return the sum of squared errors rather than the forecast of the next observation. Then use the `optim()` function to find the optimal values of $\alpha$ and $\ell_0$. Do you get the same values as the `ETS()` function?

```{r}
my_ses_sse <- function(par, y) {
  alpha <- par[1]
  level <- par[2]
  n <- length(y)
  yhat <- numeric(n)
  yhat[1] <- level
  for (i in 2:n) {
    yhat[i] <- alpha * y[i - 1] + (1 - alpha) * yhat[i - 1]
  }
  return(sum((y - yhat)^2))
}

optim(c(0.1, vic_pigs_vec[1]), my_ses_sse, y = vic_pigs_vec)$par
tidy(fit)
```

Similar, but not identical estimates. This is due to different starting values being used.

# fpp3 8.8, Ex4

> Combine your previous two functions to produce a function that both finds the optimal values of $\alpha$ and $\ell_0$, and produces a forecast of the next observation in the series.

```{r}
my_ses <- function(y) {
  par <- optim(c(0.1, y[1]), my_ses_sse, y = y)$par
  alpha <- par[1]
  level <- par[2]
  yhat <- numeric(length(y) + 1)
  yhat[1] <- level
  for (i in 2:(length(yhat))) {
    yhat[i] <- alpha * y[i - 1] + (1 - alpha) * yhat[i - 1]
  }
  return(last(yhat))
}
my_ses(vic_pigs_vec)
```

# fpp3 8.8, Ex16

> Show that the forecast variance for an ETS(A,N,N) model is given by
$$
\sigma^2\left[1+\alpha^2(h-1)\right].
$$

An ETS(A,N,N) model is defined as
  \begin{align*}
    y_t      & = \ell_{t-1} + \varepsilon_{t} \\
    \ell_{t} & = \ell_{t-1} + \alpha\varepsilon_{t},
  \end{align*}
where $\varepsilon_t \sim \text{N}(0,\sigma^2)$, and $h$-step forecasts are  given by
$$
 \hat{y}_{T+h|T} = \ell_T.
$$
So
  \begin{align*}
     y_{T+h} & = \ell_{T+h-1} + \varepsilon_{T+h} \\
             & = \ell_{T+h-2} + \alpha \varepsilon_{T+h-1} +  \varepsilon_{T+h} \\
             & = \ell_{T+h-3} + \alpha \varepsilon_{T+h-2}  + \alpha \varepsilon_{T+h-1} +  \varepsilon_{T+h} \\
             & \dots \\
             & = \ell_{T} + \alpha \sum_{j=1}^{h-1} \varepsilon_{T+h-j} +  \varepsilon_{T+h}.
  \end{align*}
Therefore
  \begin{align*}
    \text{Var}(y_{T+h} | y_1,\dots,y_T) & = \alpha^2 \sum_{j=1}^{h-1} \sigma^2 +  \sigma^2 \\
                                        & =  \sigma^2\left[ 1 + \alpha^2 (h-1)\right ].
  \end{align*}

# fpp3 8.8, Ex17

> Write down 95\% prediction intervals for an ETS(A,N,N) model as a function of $\ell_T$, $\alpha$, $h$ and $\sigma$, assuming normally distributed errors.

Using previous result:
$$
 \ell_T \pm 1.96 \sigma \sqrt{ 1 + \alpha^2 (h-1)}
$$
