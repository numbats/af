[
  {
    "objectID": "week9/activities.html",
    "href": "week9/activities.html",
    "title": "ETC3550/ETC5550 Applied forecasting",
    "section": "",
    "text": "Find an ARIMA model for the pelt::Lynx data"
  },
  {
    "objectID": "week8/index.html",
    "href": "week8/index.html",
    "title": "Week 8: ARIMA models",
    "section": "",
    "text": "Stationarity and differencing\nUnit root tests"
  },
  {
    "objectID": "week8/index.html#what-you-will-learn-this-week",
    "href": "week8/index.html#what-you-will-learn-this-week",
    "title": "Week 8: ARIMA models",
    "section": "",
    "text": "Stationarity and differencing\nUnit root tests"
  },
  {
    "objectID": "week8/index.html#pre-class-activities",
    "href": "week8/index.html#pre-class-activities",
    "title": "Week 8: ARIMA models",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Sections 9.1-9.2 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week8/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week8/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 8: ARIMA models",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 1-5 from Section 9.11 of the book."
  },
  {
    "objectID": "week7/index.html",
    "href": "week7/index.html",
    "title": "Week 7: ETS models (part 2)",
    "section": "",
    "text": "Exponential smoothing with seasonality\nAutomatic model selection using the AICc"
  },
  {
    "objectID": "week7/index.html#what-you-will-learn-this-week",
    "href": "week7/index.html#what-you-will-learn-this-week",
    "title": "Week 7: ETS models (part 2)",
    "section": "",
    "text": "Exponential smoothing with seasonality\nAutomatic model selection using the AICc"
  },
  {
    "objectID": "week7/index.html#pre-class-activities",
    "href": "week7/index.html#pre-class-activities",
    "title": "Week 7: ETS models (part 2)",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Sections 8.3-8.4 and 8.6-8.7 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week7/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week7/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 7: ETS models (part 2)",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 7, 10-15 from Section 8.8 of the book."
  },
  {
    "objectID": "week7/index.html#monday-lecture",
    "href": "week7/index.html#monday-lecture",
    "title": "Week 7: ETS models (part 2)",
    "section": "Monday lecture",
    "text": "Monday lecture\n\n\nDownload slides"
  },
  {
    "objectID": "week7/index.html#tuesday-workshop",
    "href": "week7/index.html#tuesday-workshop",
    "title": "Week 7: ETS models (part 2)",
    "section": "Tuesday workshop",
    "text": "Tuesday workshop\nActivities for Tuesday workshop"
  },
  {
    "objectID": "week7/index.html#assignments",
    "href": "week7/index.html#assignments",
    "title": "Week 7: ETS models (part 2)",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Friday 25 April."
  },
  {
    "objectID": "week7/index.html#weekly-quiz",
    "href": "week7/index.html#weekly-quiz",
    "title": "Week 7: ETS models (part 2)",
    "section": "Weekly quiz",
    "text": "Weekly quiz\n\nWeek 7 quiz is due on Sunday 20 April."
  },
  {
    "objectID": "week6/ex6-sol.html",
    "href": "week6/ex6-sol.html",
    "title": "Exercise Week 6: Solutions",
    "section": "",
    "text": "library(fpp3)\n\n\nfpp3 8.8, Ex1\n\nConsider the the number of pigs slaughtered in Victoria, available in the aus_livestock dataset.\n\n\n\nUse the ETS() function in R to estimate the equivalent model for simple exponential smoothing. Find the optimal values of \\alpha and \\ell_0, and generate forecasts for the next four months.\n\n\n\nfit &lt;- aus_livestock |&gt;\n  filter(Animal == \"Pigs\", State == \"Victoria\") |&gt;\n  model(ses = ETS(Count ~ error(\"A\") + trend(\"N\") + season(\"N\")))\nreport(fit)\n\nSeries: Count \nModel: ETS(A,N,N) \n  Smoothing parameters:\n    alpha = 0.3221247 \n\n  Initial states:\n     l[0]\n 100646.6\n\n  sigma^2:  87480760\n\n     AIC     AICc      BIC \n13737.10 13737.14 13750.07 \n\n\nOptimal values are \\alpha = 0.3221247 and \\ell_0 = 100646.6\n\nfc &lt;- fit |&gt; forecast(h = \"4 months\")\nfc\n\n# A fable: 4 x 6 [1M]\n# Key:     Animal, State, .model [1]\n  Animal State    .model    Month\n  &lt;fct&gt;  &lt;fct&gt;    &lt;chr&gt;     &lt;mth&gt;\n1 Pigs   Victoria ses    2019 Jan\n2 Pigs   Victoria ses    2019 Feb\n3 Pigs   Victoria ses    2019 Mar\n4 Pigs   Victoria ses    2019 Apr\n# ℹ 2 more variables: Count &lt;dist&gt;, .mean &lt;dbl&gt;\n\nfc |&gt;\n  autoplot(filter(aus_livestock, Month &gt;= yearmonth(\"2010 Jan\")))\n\n\n\n\n\n\n\n\n\n\nCompute a 95% prediction interval for the first forecast using \\hat{y} \\pm 1.96s where s is the standard deviation of the residuals. Compare your interval with the interval produced by R.\n\n\n\ns &lt;- augment(fit) |&gt;\n  pull(.resid) |&gt;\n  sd()\nyhat &lt;- fc |&gt;\n  pull(.mean) |&gt;\n  head(1)\nyhat + c(-1, 1) * 1.96 * s\n\n[1]  76871.01 113502.10\n\n\n\nfc |&gt;\n  head(1) |&gt;\n  mutate(interval = hilo(Count, 95)) |&gt;\n  pull(interval)\n\n&lt;hilo[1]&gt;\n[1] [76854.79, 113518.3]95\n\n\nThe intervals are close but not identical. This is because R estimates the variance of the residuals differently, taking account of the degrees of freedom properly (and also using a more accurate critical value rather than just 1.96).\nTry the following.\n\nres &lt;- augment(fit) |&gt; pull(.resid)\ns &lt;- sqrt(sum(res^2) / (length(res) - NROW(tidy(fit))))\nyhat + c(-1, 1) * qnorm(0.975) * s\n\n[1]  76854.79 113518.33\n\n\n\n\nfpp3 8.8, Ex2\n\nWrite your own function to implement simple exponential smoothing. The function should take arguments y (the response data), alpha (the smoothing parameter \\alpha) and level (the initial level \\ell_0). It should return the forecast of the next observation in the series. Does it give the same forecast as ETS()?\n\n\nmy_ses &lt;- function(y, alpha, level) {\n  yhat &lt;- numeric(length(y) + 1)\n  yhat[1] &lt;- level\n  for (i in 2:(length(yhat))) {\n    yhat[i] &lt;- alpha * y[i - 1] + (1 - alpha) * yhat[i - 1]\n  }\n  return(last(yhat))\n}\n\nvic_pigs_vec &lt;- aus_livestock |&gt;\n  filter(Animal == \"Pigs\", State == \"Victoria\") |&gt;\n  pull(Count)\n\nses_fc &lt;- vic_pigs_vec |&gt;\n  my_ses(alpha = 0.3221, level = 100646.6)\n\nc(my_ses = ses_fc, fable = fc$.mean[1])\n\n  my_ses    fable \n95186.59 95186.56 \n\n\nYes, the same forecasts are obtained. The slight differences are due to rounding of \\alpha and \\ell_0.\n\n\nfpp3 8.8, Ex3\n\nModify your function from the previous exercise to return the sum of squared errors rather than the forecast of the next observation. Then use the optim() function to find the optimal values of \\alpha and \\ell_0. Do you get the same values as the ETS() function?\n\n\nmy_ses_sse &lt;- function(par, y) {\n  alpha &lt;- par[1]\n  level &lt;- par[2]\n  n &lt;- length(y)\n  yhat &lt;- numeric(n)\n  yhat[1] &lt;- level\n  for (i in 2:n) {\n    yhat[i] &lt;- alpha * y[i - 1] + (1 - alpha) * yhat[i - 1]\n  }\n  return(sum((y - yhat)^2))\n}\n\noptim(c(0.1, vic_pigs_vec[1]), my_ses_sse, y = vic_pigs_vec)$par\n\n[1] 3.220344e-01 1.005254e+05\n\ntidy(fit)\n\n# A tibble: 2 × 5\n  Animal State    .model term    estimate\n  &lt;fct&gt;  &lt;fct&gt;    &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;\n1 Pigs   Victoria ses    alpha      0.322\n2 Pigs   Victoria ses    l[0]  100647.   \n\n\nSimilar, but not identical estimates. This is due to different starting values being used.\n\n\nfpp3 8.8, Ex4\n\nCombine your previous two functions to produce a function that both finds the optimal values of \\alpha and \\ell_0, and produces a forecast of the next observation in the series.\n\n\nmy_ses &lt;- function(y) {\n  par &lt;- optim(c(0.1, y[1]), my_ses_sse, y = y)$par\n  alpha &lt;- par[1]\n  level &lt;- par[2]\n  yhat &lt;- numeric(length(y) + 1)\n  yhat[1] &lt;- level\n  for (i in 2:(length(yhat))) {\n    yhat[i] &lt;- alpha * y[i - 1] + (1 - alpha) * yhat[i - 1]\n  }\n  return(last(yhat))\n}\nmy_ses(vic_pigs_vec)\n\n[1] 95186.69\n\n\n\n\nfpp3 8.8, Ex5\n\nData set global_economy contains the annual Exports from many countries. Select one country to analyse.\n\nPlot the Exports series and discuss the main features of the data.\n\n\n\nglobal_economy |&gt;\n  filter(Country == \"Argentina\") |&gt;\n  autoplot(Exports)\n\n\n\n\n\n\n\n\nThere is a huge jump in Exports in 2002, due to the deregulation of the Argentinian peso. Since then, Exports (as a percentage of GDP) has gradually returned to 1990 levels.\n\n\nUse an ETS(A,N,N) model to forecast the series, and plot the forecasts.\n\n\n\netsANN &lt;- global_economy |&gt;\n  filter(Country == \"Argentina\") |&gt;\n  model(ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")))\netsANN |&gt;\n  forecast(h = 10) |&gt;\n  autoplot(global_economy)\n\n\n\n\n\n\n\n\n\n\nCompute the RMSE values for the training data.\n\n\n\naccuracy(etsANN) |&gt; select(RMSE)\n\n# A tibble: 1 × 1\n   RMSE\n  &lt;dbl&gt;\n1  2.78\n\n\n\n\nCompare the results to those from an ETS(A,A,N) model. (Remember that the trended model is using one more parameter than the simpler model.) Discuss the merits of the two forecasting methods for this data set.\n\n\n\nfit &lt;- global_economy |&gt;\n  filter(Country == \"Argentina\") |&gt;\n  model(\n    ses = ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    holt = ETS(Exports ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n  )\naccuracy(fit)\n\n# A tibble: 2 × 11\n  Country   .model .type         ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n  &lt;fct&gt;     &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Argentina ses    Training 0.0762   2.78  1.62 -1.73  15.7 0.983 0.986 0.00902\n2 Argentina holt   Training 0.00795  2.78  1.64 -2.51  15.9 0.994 0.986 0.0271 \n\n\nThere is very little difference in training RMSE between these models. So the extra parameter is not doing much.\n\n\nCompare the forecasts from both methods. Which do you think is best?\n\n\n\nfit |&gt;\n  forecast(h = 10) |&gt;\n  autoplot(global_economy)\n\n\n\n\n\n\n\n\n\nThe forecasts are similar. In this case, the simpler model is preferred.\n\n\n\nCalculate a 95% prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using R.\n\n\n\nstandard error. (from RMSE)\nmean (from forecast)\n\n\ns &lt;- accuracy(fit) |&gt; pull(RMSE)\nyhat &lt;- forecast(fit, h = 1) |&gt; pull(.mean)\n# SES\nyhat[1] + c(-1, 1) * qnorm(0.975) * s[1]\n\n[1]  5.882074 16.764136\n\n# Holt\nyhat[2] + c(-1, 1) * qnorm(0.975) * s[2]\n\n[1]  5.989515 16.872908\n\nfit |&gt;\n  forecast(h = 1) |&gt;\n  mutate(PI = hilo(Exports, level = 95))\n\n# A fable: 2 x 6 [1Y]\n# Key:     Country, .model [2]\n  Country   .model  Year\n  &lt;fct&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 Argentina ses     2018\n2 Argentina holt    2018\n# ℹ 3 more variables: Exports &lt;dist&gt;, .mean &lt;dbl&gt;, PI &lt;hilo&gt;\n\n\n\nUsing RMSE yields narrower prediction interval while using the values from hilo() function gives wider prediction interval.\nUsing RMSE has failed to take account of the degrees of freedom for each model. Compare the following\n\n\nsse &lt;- augment(fit) |&gt;\n  as_tibble() |&gt;\n  group_by(.model) |&gt;\n  summarise(s = sum(.resid^2)) |&gt;\n  pull(s)\n\nn &lt;- global_economy |&gt;\n  filter(Country == \"Argentina\") |&gt;\n    nrow()\n\n# sse method= alpha, level=&gt; 2\n# holt linear = alpha, level, trend, b =&gt; 4\n\ns &lt;- sqrt(sse / (n - c(2, 4)))\n\n# SES\nyhat[1] + c(-1, 1) * qnorm(0.975) * s[1]\n\n[1]  5.785088 16.861122\n\n# Holt\nyhat[2] + c(-1, 1) * qnorm(0.975) * s[2]\n\n[1]  5.79226 17.07016\n\n\n\n\nfpp3 8.8, Ex6\n\nForecast the Chinese GDP from the global_economy data set using an ETS model. Experiment with the various options in the ETS() function to see how much the forecasts change with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each is doing to the forecasts.\n\n\n[Hint: use h=20 when forecasting, so you can clearly see the differences between the various options when plotting the forecasts.]\n\n\nchina &lt;- global_economy |&gt;\n  filter(Country == \"China\")\nchina |&gt; autoplot(GDP)\n\n\n\n\n\n\n\n\n\nIt clearly needs a relatively strong transformation due to the increasing variance.\n\n\nchina |&gt; autoplot(box_cox(GDP, 0.2))\n\n\n\n\n\n\n\nchina |&gt; features(GDP, guerrero)\n\n# A tibble: 1 × 2\n  Country lambda_guerrero\n  &lt;fct&gt;             &lt;dbl&gt;\n1 China           -0.0345\n\n\n\nMaking \\lambda=0.2 looks ok.\nThe Guerrero method suggests an even stronger transformation. Let’s also try a log.\n\n\nfit &lt;- china |&gt;\n  model(\n    ets = ETS(GDP),\n    ets_damped = ETS(GDP ~ trend(\"Ad\")),\n    ets_bc = ETS(box_cox(GDP, 0.2)),\n    ets_log = ETS(log(GDP))\n  )\n\nfit\n\n# A mable: 1 x 5\n# Key:     Country [1]\n  Country          ets    ets_damped       ets_bc      ets_log\n  &lt;fct&gt;        &lt;model&gt;       &lt;model&gt;      &lt;model&gt;      &lt;model&gt;\n1 China   &lt;ETS(M,A,N)&gt; &lt;ETS(M,Ad,N)&gt; &lt;ETS(A,A,N)&gt; &lt;ETS(A,A,N)&gt;\n\naugment(fit)\n\n# A tsibble: 232 x 7 [1Y]\n# Key:       Country, .model [4]\n   Country .model  Year          GDP      .fitted        .resid   .innov\n   &lt;fct&gt;   &lt;chr&gt;  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 China   ets     1960 59716467625. 49001691297.  10714776328.  0.219  \n 2 China   ets     1961 50056868958. 66346643194. -16289774236. -0.246  \n 3 China   ets     1962 47209359006. 51607368186.  -4398009180. -0.0852 \n 4 China   ets     1963 50706799903. 47386494407.   3320305495.  0.0701 \n 5 China   ets     1964 59708343489. 51919091574.   7789251914.  0.150  \n 6 China   ets     1965 70436266147. 63350421234.   7085844913.  0.112  \n 7 China   ets     1966 76720285970. 76289186599.    431099371.  0.00565\n 8 China   ets     1967 72881631327. 82708375812.  -9826744486. -0.119  \n 9 China   ets     1968 70846535056. 75804820984.  -4958285928. -0.0654 \n10 China   ets     1969 79705906247. 72222259470.   7483646777.  0.104  \n# ℹ 222 more rows\n\nfit |&gt;\n  forecast(h = \"20 years\") |&gt;\n  autoplot(china, level = NULL)\n\n\n\n\n\n\n\n\n\nThe transformations have a big effect, with small lambda values creating big increases in the forecasts.\nThe damping has relatively a small effect.\n\n\n\nfpp3 8.8, Ex16\n\nShow that the forecast variance for an ETS(A,N,N) model is given by \n\\sigma^2\\left[1+\\alpha^2(h-1)\\right].\n\n\nAn ETS(A,N,N) model is defined as \\begin{align*}\n    y_t      & = \\ell_{t-1} + \\varepsilon_{t} \\\\\n    \\ell_{t} & = \\ell_{t-1} + \\alpha\\varepsilon_{t},\n  \\end{align*} where \\varepsilon_t \\sim \\text{N}(0,\\sigma^2), and h-step forecasts are given by \n\\hat{y}_{T+h|T} = \\ell_T.\n So \\begin{align*}\n     y_{T+h} & = \\ell_{T+h-1} + \\varepsilon_{T+h} \\\\\n             & = \\ell_{T+h-2} + \\alpha \\varepsilon_{T+h-1} +  \\varepsilon_{T+h} \\\\\n             & = \\ell_{T+h-3} + \\alpha \\varepsilon_{T+h-2}  + \\alpha \\varepsilon_{T+h-1} +  \\varepsilon_{T+h} \\\\\n             & \\dots \\\\\n             & = \\ell_{T} + \\alpha \\sum_{j=1}^{h-1} \\varepsilon_{T+h-j} +  \\varepsilon_{T+h}.\n  \\end{align*} Therefore \\begin{align*}\n    \\text{Var}(y_{T+h} | y_1,\\dots,y_T) & = \\alpha^2 \\sum_{j=1}^{h-1} \\sigma^2 +  \\sigma^2 \\\\\n                                        & =  \\sigma^2\\left[ 1 + \\alpha^2 (h-1)\\right ].\n  \\end{align*}\n\n\nfpp3 8.8, Ex17\n\nWrite down 95% prediction intervals for an ETS(A,N,N) model as a function of \\ell_T, \\alpha, h and \\sigma, assuming normally distributed errors.\n\nUsing previous result: \n\\ell_T \\pm 1.96 \\sigma \\sqrt{ 1 + \\alpha^2 (h-1)}"
  },
  {
    "objectID": "week5/ex5-sol.html",
    "href": "week5/ex5-sol.html",
    "title": "Exercise Week 5: Solutions",
    "section": "",
    "text": "library(fpp3)\n\n\nfpp3 5.10, Ex 8\n\nConsider the number of pigs slaughtered in New South Wales (data set aus_livestock).\n\nProduce some plots of the data in order to become familiar with it.\n\n\n\nnsw_pigs &lt;- aus_livestock |&gt;\n  filter(State == \"New South Wales\", Animal == \"Pigs\")\nnsw_pigs |&gt;\n  autoplot(Count)\n\n\n\n\n\n\n\n\nData generally follows a downward trend, however there are some periods where the amount of pigs slaughtered changes rapidly.\n\nnsw_pigs |&gt; gg_season(Count, labels = \"right\")\n\n\n\n\n\n\n\nnsw_pigs |&gt; gg_subseries(Count)\n\n\n\n\n\n\n\n\nSome seasonality is apparent, with notable increases in December and decreases during January, February and April.\n\n\nCreate a training set of 486 observations, withholding a test set of 72 observations (6 years).\n\n\n\nnsw_pigs_train &lt;- nsw_pigs |&gt; slice(1:486)\n\n\n\nTry using various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?\n\n\n\nfit &lt;- nsw_pigs_train |&gt;\n  model(\n    mean = MEAN(Count),\n    naive = NAIVE(Count),\n    snaive = SNAIVE(Count),\n    drift = RW(Count ~ drift())\n  )\nfit |&gt;\n  forecast(h = \"6 years\") |&gt;\n  accuracy(nsw_pigs)\n\n# A tibble: 4 × 12\n  .model Animal State       .type      ME   RMSE    MAE    MPE  MAPE  MASE RMSSE\n  &lt;chr&gt;  &lt;fct&gt;  &lt;fct&gt;       &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 drift  Pigs   New South … Test   -4685.  8091.  6967.  -7.36  10.1 0.657 0.557\n2 mean   Pigs   New South … Test  -39360. 39894. 39360. -55.9   55.9 3.71  2.75 \n3 naive  Pigs   New South … Test   -6138.  8941.  7840.  -9.39  11.4 0.740 0.615\n4 snaive Pigs   New South … Test   -5838. 10111.  8174.  -8.81  11.9 0.771 0.696\n# ℹ 1 more variable: ACF1 &lt;dbl&gt;\n\n\nThe drift method performed best for all measures of accuracy (although it had a larger first order auto-correlation)\n\n\nCheck the residuals of your preferred method. Do they resemble white noise?\n\n\n\nfit |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\n\nThe residuals do not appear to be white noise as the ACF plot contains many significant lags. It is also clear that the seasonal component is not captured by the drift method, as there exists a strong positive auto-correlation at lag 12 (1 year). The histogram appears to have a slightly long left tail.\n\n\nfpp3 5.10, Ex 9\n\n\nCreate a training set for household wealth (hh_budget) by withholding the last four years as a test set.\n\n\n\ntrain &lt;- hh_budget |&gt;\n  filter(Year &lt;= max(Year) - 4)\n\n\n\nFit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.\n\n\n\nfit &lt;- train |&gt;\n  model(\n    naive = NAIVE(Wealth),\n    drift = RW(Wealth ~ drift()),\n    mean = MEAN(Wealth)\n  )\nfc &lt;- fit |&gt; forecast(h = 4)\n\n\n\nCompute the accuracy of your forecasts. Which method does best?\n\n\n\nfc |&gt;\n  accuracy(hh_budget) |&gt;\n  arrange(Country, MASE)\n\n# A tibble: 12 × 11\n   .model Country   .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 drift  Australia Test   29.1  35.5  29.1  7.23  7.23 1.73  1.48   0.210 \n 2 naive  Australia Test   34.7  41.5  34.7  8.64  8.64 2.06  1.73   0.216 \n 3 mean   Australia Test   35.7  42.3  35.7  8.89  8.89 2.12  1.76   0.216 \n 4 drift  Canada    Test   33.3  37.2  33.3  6.09  6.09 1.73  1.57  -0.229 \n 5 naive  Canada    Test   46.2  51.0  46.2  8.46  8.46 2.40  2.15  -0.0799\n 6 mean   Canada    Test   90.4  92.9  90.4 16.7  16.7  4.69  3.92  -0.0799\n 7 drift  Japan     Test   14.7  17.9  14.7  2.44  2.44 0.943 0.967 -0.229 \n 8 naive  Japan     Test   36.3  37.8  36.3  6.06  6.06 2.34  2.04  -0.534 \n 9 mean   Japan     Test  100.  101.  100.  16.8  16.8  6.45  5.46  -0.534 \n10 drift  USA       Test   75.9  76.2  75.9 12.7  12.7  2.88  2.43  -0.561 \n11 naive  USA       Test   82.1  82.5  82.1 13.8  13.8  3.12  2.63  -0.423 \n12 mean   USA       Test   82.9  83.3  82.9 13.9  13.9  3.15  2.65  -0.423 \n\nfc |&gt;\n  accuracy(hh_budget) |&gt;\n  group_by(.model) |&gt;\n  summarise(MASE = mean(MASE)) |&gt;\n  ungroup() |&gt;\n  arrange(MASE)\n\n# A tibble: 3 × 2\n  .model  MASE\n  &lt;chr&gt;  &lt;dbl&gt;\n1 drift   1.82\n2 naive   2.48\n3 mean    4.10\n\n\nThe drift method is better for every country, and on average.\n\n\nDo the residuals from the best method resemble white noise?\n\n\n\nfit |&gt;\n  filter(Country == \"Australia\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt;\n  filter(Country == \"Canada\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt;\n  filter(Country == \"Japan\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt;\n  filter(Country == \"USA\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\n\nIn all cases, the residuals look like white noise.\n\n\nfpp3 5.10, Ex 10\n\n\nCreate a training set for Australian takeaway food turnover (aus_retail) by withholding the last four years as a test set.\n\n\n\ntakeaway &lt;- aus_retail |&gt;\n  filter(Industry == \"Takeaway food services\") |&gt;\n  summarise(Turnover = sum(Turnover))\ntrain &lt;- takeaway |&gt;\n  filter(Month &lt;= max(Month) - 4 * 12)\n\n\n\nFit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.\n\n\n\nfit &lt;- train |&gt;\n  model(\n    naive = NAIVE(Turnover),\n    drift = RW(Turnover ~ drift()),\n    mean = MEAN(Turnover),\n    snaive = SNAIVE(Turnover)\n  )\nfc &lt;- fit |&gt; forecast(h = \"4 years\")\n\n\n\nCompute the accuracy of your forecasts. Which method does best?\n\n\n\nfc |&gt;\n  accuracy(takeaway) |&gt;\n  arrange(MASE)\n\n# A tibble: 4 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 naive  Test  -12.4  119.  96.4 -1.49  6.66  2.30  2.25 0.613\n2 drift  Test  -93.7  130. 108.  -6.82  7.67  2.58  2.46 0.403\n3 snaive Test  177.   192. 177.  11.7  11.7   4.22  3.64 0.902\n4 mean   Test  829.   838. 829.  55.7  55.7  19.8  15.8  0.613\n\n\nThe naive method is best here.\n\n\nDo the residuals from the best method resemble white noise?\n\n\n\nfit |&gt;\n  select(naive) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\n\nThis is far from white noise. There is strong seasonality and increasing variance that has not been accounted for by the naive model.\n\n\nfpp3 5.10, Ex 12\n\ntourism contains quarterly visitor nights (in thousands) from 1998 to 2017 for 76 regions of Australia.\n\nExtract data from the Gold Coast region using filter() and aggregate total overnight trips (sum over Purpose) using summarise(). Call this new dataset gc_tourism.\n\n\n\ngc_tourism &lt;- tourism |&gt;\n  filter(Region == \"Gold Coast\") |&gt;\n  summarise(Trips = sum(Trips))\ngc_tourism\n\n# A tsibble: 80 x 2 [1Q]\n   Quarter Trips\n     &lt;qtr&gt; &lt;dbl&gt;\n 1 1998 Q1  827.\n 2 1998 Q2  681.\n 3 1998 Q3  839.\n 4 1998 Q4  820.\n 5 1999 Q1  987.\n 6 1999 Q2  751.\n 7 1999 Q3  822.\n 8 1999 Q4  914.\n 9 2000 Q1  871.\n10 2000 Q2  780.\n# ℹ 70 more rows\n\n\n\n\nUsing slice() or filter(), create three training sets for this data excluding the last 1, 2 and 3 years. For example, gc_train_1 &lt;- gc_tourism |&gt; slice(1:(n()-4)).\n\n\n\ngc_train_1 &lt;- gc_tourism |&gt; slice(1:(n() - 4))\ngc_train_2 &lt;- gc_tourism |&gt; slice(1:(n() - 8))\ngc_train_3 &lt;- gc_tourism |&gt; slice(1:(n() - 12))\n\n\n\nCompute one year of forecasts for each training set using the seasonal naïve (SNAIVE()) method. Call these gc_fc_1, gc_fc_2 and gc_fc_3, respectively.\n\n\n\ngc_fc &lt;- bind_cols(\n  gc_train_1 |&gt; model(gc_fc_1 = SNAIVE(Trips)),\n  gc_train_2 |&gt; model(gc_fc_2 = SNAIVE(Trips)),\n  gc_train_3 |&gt; model(gc_fc_3 = SNAIVE(Trips))\n) |&gt; forecast(h = \"1 year\")\n\n\ngc_fc |&gt; autoplot(gc_tourism)\n\n\n\n\n\n\n\n\n\n\nUse accuracy() to compare the test set forecast accuracy using MAPE. Comment on these.\n\n\n\ngc_fc |&gt; accuracy(gc_tourism)\n\n# A tibble: 3 × 10\n  .model  .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE   ACF1\n  &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 gc_fc_1 Test   75.1 167.  154.   6.36 15.1  2.66  2.36  -0.410\n2 gc_fc_2 Test   12.0  43.1  39.5  1.14  4.32 0.670 0.599 -0.792\n3 gc_fc_3 Test   35.8  91.4  83.9  3.56  9.07 1.46  1.30   0.239\n\n\nThe second set of forecasts are most accurate (as can be seen in the previous plot), however this is likely due to chance."
  },
  {
    "objectID": "week4/ex4-sol.html",
    "href": "week4/ex4-sol.html",
    "title": "Exercise Week 4: Solutions",
    "section": "",
    "text": "library(fpp3)"
  },
  {
    "objectID": "week4/ex4-sol.html#australian-population",
    "href": "week4/ex4-sol.html#australian-population",
    "title": "Exercise Week 4: Solutions",
    "section": "Australian population",
    "text": "Australian population\n\nglobal_economy |&gt;\n  filter(Country == \"Australia\") |&gt;\n  autoplot(Population)\n\n\n\n\n\n\n\n\nData has trend and no seasonality. Random walk with drift model is appropriate.\n\nglobal_economy |&gt;\n  filter(Country == \"Australia\") |&gt;\n  model(RW(Population ~ drift())) |&gt;\n  forecast(h = \"10 years\") |&gt;\n  autoplot(global_economy)"
  },
  {
    "objectID": "week4/ex4-sol.html#australian-clay-brick-production",
    "href": "week4/ex4-sol.html#australian-clay-brick-production",
    "title": "Exercise Week 4: Solutions",
    "section": "Australian clay brick production",
    "text": "Australian clay brick production\n\naus_production |&gt;\n  filter(!is.na(Bricks)) |&gt;\n  autoplot(Bricks) +\n  labs(title = \"Clay brick production\")\n\n\n\n\n\n\n\n\nThis data appears to have more seasonality than trend, so of the models available, seasonal naive is most appropriate.\n\naus_production |&gt;\n  filter(!is.na(Bricks)) |&gt;\n  model(SNAIVE(Bricks)) |&gt;\n  forecast(h = \"5 years\") |&gt;\n  autoplot(aus_production)"
  },
  {
    "objectID": "week4/ex4-sol.html#nsw-lambs",
    "href": "week4/ex4-sol.html#nsw-lambs",
    "title": "Exercise Week 4: Solutions",
    "section": "NSW Lambs",
    "text": "NSW Lambs\n\nnsw_lambs &lt;- aus_livestock |&gt;\n  filter(State == \"New South Wales\", Animal == \"Lambs\")\nnsw_lambs |&gt;\n  autoplot(Count)\n\n\n\n\n\n\n\n\nThis data appears to have more seasonality than trend, so of the models available, seasonal naive is most appropriate.\n\nnsw_lambs |&gt;\n  model(SNAIVE(Count)) |&gt;\n  forecast(h = \"5 years\") |&gt;\n  autoplot(nsw_lambs)"
  },
  {
    "objectID": "week4/ex4-sol.html#household-wealth",
    "href": "week4/ex4-sol.html#household-wealth",
    "title": "Exercise Week 4: Solutions",
    "section": "Household wealth",
    "text": "Household wealth\n\nhh_budget |&gt;\n  autoplot(Wealth)\n\n\n\n\n\n\n\n\nAnnual data with trend upwards, so we can use a random walk with drift.\n\nhh_budget |&gt;\n  model(RW(Wealth ~ drift())) |&gt;\n  forecast(h = \"5 years\") |&gt;\n  autoplot(hh_budget)"
  },
  {
    "objectID": "week4/ex4-sol.html#australian-takeaway-food-turnover",
    "href": "week4/ex4-sol.html#australian-takeaway-food-turnover",
    "title": "Exercise Week 4: Solutions",
    "section": "Australian takeaway food turnover",
    "text": "Australian takeaway food turnover\n\ntakeaway &lt;- aus_retail |&gt;\n  filter(Industry == \"Takeaway food services\") |&gt;\n  summarise(Turnover = sum(Turnover))\ntakeaway |&gt; autoplot(Turnover)\n\n\n\n\n\n\n\n\nThis data has strong seasonality and strong trend, so we will use a seasonal naive model with drift.\n\ntakeaway |&gt;\n  model(SNAIVE(Turnover ~ drift())) |&gt;\n  forecast(h = \"5 years\") |&gt;\n  autoplot(takeaway)\n\n\n\n\n\n\n\n\nThis is actually not one of the four benchmark methods discussed in the book, but is sometimes a useful benchmark when there is strong seasonality and strong trend.\nThe corresponding equation is \n  \\hat{y}_{T+h|T} = y_{T+h-m(k+1)} + \\frac{h}{T-m}\\sum_{t=m+1}^T(y_t - y_{t-m}),\n where m=12 and k is the integer part of (h-1)/m (i.e., the number of complete years in the forecast period prior to time T+h)."
  },
  {
    "objectID": "week4/ex4-sol.html#australian-exports",
    "href": "week4/ex4-sol.html#australian-exports",
    "title": "Exercise Week 4: Solutions",
    "section": "Australian exports",
    "text": "Australian exports\nThe data does not contain seasonality, so the naive model is more appropriate.\n\n# Extract data of interest\naus_exports &lt;- filter(global_economy, Country == \"Australia\")\n# Define and estimate a model\nfit &lt;- aus_exports |&gt; model(NAIVE(Exports))\n# Check residuals\nfit |&gt; gg_tsresiduals()\n\n\n\n\n\n\n\n\n\nThe ACF plot reveals that the first lag exceeds the significance threshold.\nThis data may still be white noise, as it is the only lag that exceeds the blue dashed lines (5% of the lines are expected to exceed it). However as it is the first lag, it is probable that there exists some real auto-correlation in the residuals that can be modelled.\nThe distribution appears normal.\nThe residual plot appears mostly random, however more observations appear to be above zero. This again, is due to the model not capturing the trend.\n\n\n# Look at some forecasts\nfit |&gt;\n  forecast() |&gt;\n  autoplot(aus_exports)\n\n\n\n\n\n\n\n\n\nThe forecasts appear reasonable as the series appears to have flattened in recent years.\nThe intervals are also reasonable — despite the assumptions behind them having been violated."
  },
  {
    "objectID": "week4/ex4-sol.html#australian-brick-production",
    "href": "week4/ex4-sol.html#australian-brick-production",
    "title": "Exercise Week 4: Solutions",
    "section": "Australian brick production",
    "text": "Australian brick production\nThe data is seasonal, so the seasonal naive model is more appropriate.\n\n# Remove the missing values at the end of the series\ntidy_bricks &lt;- aus_production |&gt;\n  filter(!is.na(Bricks))\n# Define and estimate a model\nfit &lt;- tidy_bricks |&gt;\n  model(SNAIVE(Bricks))\n# Look at the residuals\nfit |&gt; gg_tsresiduals()\n\n\n\n\n\n\n\n\n\nThe residual plot does not appear random. Periods of low production and high production are evident, leading to autocorrelation in the residuals.\nThe residuals from this model are not white noise. The ACF plot shows a strong sinusoidal pattern of decay, indicating that the residuals are auto-correlated.\nThe histogram is also not normally distributed, as it has a long left tail.\n\n\n# Look at some forecasts\nfit |&gt;\n  forecast() |&gt;\n  autoplot(tidy_bricks)\n\n\n\n\n\n\n\n\n\nThe point forecasts appear reasonable as the series appears to have flattened in recent years.\nThe intervals appear much larger than necessary."
  },
  {
    "objectID": "week3/ex3-sol.html",
    "href": "week3/ex3-sol.html",
    "title": "Exercise Week 3: Solutions",
    "section": "",
    "text": "library(fpp3)"
  },
  {
    "objectID": "week3/ex3-sol.html#united-states-gdp",
    "href": "week3/ex3-sol.html#united-states-gdp",
    "title": "Exercise Week 3: Solutions",
    "section": "United States GDP",
    "text": "United States GDP\n\nus_economy &lt;- global_economy |&gt;\n  filter(Country == \"United States\")\nus_economy |&gt;\n  autoplot(GDP)\n\n\n\n\n\n\n\n\n\nTrend appears exponential, a transformation would be useful.\n\n\nus_economy |&gt;\n  autoplot(box_cox(GDP, 0))\n\n\n\n\n\n\n\n\n\nA log transformation (Box-Cox with \\lambda = 0) appears slightly too strong.\n\n\nus_economy |&gt;\n  autoplot(box_cox(GDP, 0.3))\n\n\n\n\n\n\n\n\n\nUsing \\lambda = 0.3 looks pretty good, the trend is now almost linear.\n\nLet’s see what guerrero’s method suggests.\n\nus_economy |&gt;\n  features(GDP, features = guerrero)\n\n# A tibble: 1 × 2\n  Country       lambda_guerrero\n  &lt;fct&gt;                   &lt;dbl&gt;\n1 United States           0.282\n\n\nPretty close to \\lambda = 0.3, let’s see how it looks:\n\nus_economy |&gt;\n  autoplot(box_cox(GDP, 0.2819714))\n\n\n\n\n\n\n\n\n\nMore or less the same. Box-Cox transformations are usually insensitive to the choice of \\lambda."
  },
  {
    "objectID": "week3/ex3-sol.html#slaughter-of-victorian-bulls-bullocks-and-steers",
    "href": "week3/ex3-sol.html#slaughter-of-victorian-bulls-bullocks-and-steers",
    "title": "Exercise Week 3: Solutions",
    "section": "Slaughter of Victorian “Bulls, bullocks and steers”",
    "text": "Slaughter of Victorian “Bulls, bullocks and steers”\n\nvic_bulls &lt;- aus_livestock |&gt;\n  filter(State == \"Victoria\", Animal == \"Bulls, bullocks and steers\")\nvic_bulls |&gt;\n  autoplot(Count)\n\n\n\n\n\n\n\n\n\nVariation in the series appears to vary slightly with the number of bulls slaughtered in Victoria.\nA transformation may be useful.\n\n\nvic_bulls |&gt;\n  autoplot(log(Count))\n\n\n\n\n\n\n\n\n\nA log transformation (Box-Cox \\lambda = 0) appears to normalise most of the variation. Let’s check with guerrero’s method.\n\n\nvic_bulls |&gt;\n  features(Count, features = guerrero)\n\n# A tibble: 1 × 3\n  Animal                     State    lambda_guerrero\n  &lt;fct&gt;                      &lt;fct&gt;              &lt;dbl&gt;\n1 Bulls, bullocks and steers Victoria         -0.0446\n\n\n\nPretty close, guerrero suggests \\lambda = -0.045. This is close enough to zero, so it is probably best to just use a log transformation (allowing better interpretations)."
  },
  {
    "objectID": "week3/ex3-sol.html#victorian-electricity-demand",
    "href": "week3/ex3-sol.html#victorian-electricity-demand",
    "title": "Exercise Week 3: Solutions",
    "section": "Victorian Electricity Demand",
    "text": "Victorian Electricity Demand\n\nvic_elec |&gt;\n  autoplot(Demand)\n\n\n\n\n\n\n\n\n\nSeasonal patterns for time of day hidden due to density of ink.\nDay-of-week seasonality just visible.\nTime-of-year seasonality is clear with increasing variance in winter and high skewness in summer.\n\n\nvic_elec |&gt;\n  autoplot(box_cox(Demand, 0))\n\n\n\n\n\n\n\n\n\nA log transformation makes the variance more even and reduces the skewness.\nGuerrero’s method doesn’t work here as there are several types of seasonality."
  },
  {
    "objectID": "week3/ex3-sol.html#australian-gas-production",
    "href": "week3/ex3-sol.html#australian-gas-production",
    "title": "Exercise Week 3: Solutions",
    "section": "Australian Gas production",
    "text": "Australian Gas production\n\naus_production |&gt;\n  autoplot(Gas)\n\n\n\n\n\n\n\n\n\nVariation in seasonal pattern grows proportionally to the amount of gas produced in Australia. A transformation should work well here.\n\n\naus_production |&gt;\n  autoplot(box_cox(Gas, 0))\n\n\n\n\n\n\n\n\n\nA log transformation appears slightly too strong, where the variation in periods with smaller gas production is now larger than the variation during greater gas production.\n\n\naus_production |&gt;\n  features(Gas, features = guerrero)\n\n# A tibble: 1 × 1\n  lambda_guerrero\n            &lt;dbl&gt;\n1           0.110\n\n\n\nGuerrero’s method agrees by selecting a slightly weaker transformation. Let’s see how it looks.\n\n\naus_production |&gt;\n  autoplot(box_cox(Gas, 0.1095))\n\n\n\n\n\n\n\n\nLooking good! The variation is now constant across the series."
  },
  {
    "objectID": "week3/ex3-sol.html#australian-tobacco-production",
    "href": "week3/ex3-sol.html#australian-tobacco-production",
    "title": "Exercise Week 3: Solutions",
    "section": "Australian tobacco production",
    "text": "Australian tobacco production\n\naus_production |&gt;\n  autoplot(Tobacco)\n\n\n\n\n\n\n\n\n\nThis variation in this series appears to be mostly constant across different levels of the series.\nIf any transformation is required, it would be a weak one. This can be seen if a strong transformation (such as log) is used.\n\n\naus_production |&gt;\n  autoplot(log(Tobacco))\n\n\n\n\n\n\n\n\n\nGuerrero’s method suggests that \\lambda = 0.926 is appropriate. This is a very weak transformation, as it is close to 1 (probably best to not bother transforming this series).\n\n\naus_production |&gt;\n  features(Tobacco, features = guerrero)\n\n# A tibble: 1 × 1\n  lambda_guerrero\n            &lt;dbl&gt;\n1           0.926\n\n\n\naus_production |&gt;\n  autoplot(box_cox(Tobacco, 0.926))\n\n\n\n\n\n\n\n\n\nThis series appears very similar to the original. The transformation is having almost no effect."
  },
  {
    "objectID": "week3/ex3-sol.html#economy-passengers-between-melbourne-and-sydney",
    "href": "week3/ex3-sol.html#economy-passengers-between-melbourne-and-sydney",
    "title": "Exercise Week 3: Solutions",
    "section": "Economy passengers between Melbourne and Sydney",
    "text": "Economy passengers between Melbourne and Sydney\n\nansett |&gt;\n  filter(Airports == \"MEL-SYD\", Class == \"Economy\") |&gt;\n  autoplot(Passengers) +\n  labs(title = \"Economy passengers\", subtitle = \"MEL-SYD\")\n\n\n\n\n\n\n\n\n\nThe data does not appear to vary proportionally to the level of the series.\nThere are many periods in this time series (such as the strike and change in seat classes) that may need further attention, but this is probably better resolved with modelling rather than transformations."
  },
  {
    "objectID": "week3/ex3-sol.html#pedestrian-counts",
    "href": "week3/ex3-sol.html#pedestrian-counts",
    "title": "Exercise Week 3: Solutions",
    "section": "Pedestrian counts",
    "text": "Pedestrian counts\n\npedestrian |&gt;\n  filter(Sensor == \"Southern Cross Station\") |&gt;\n  autoplot(Count) +\n  labs(title = \"Southern Cross Pedestrians\")\n\n\n\n\n\n\n\n\n\nThere is a high skewness and some zeros (so we can’t take logs). Let’s try the log(x+1) transformation:\n\n\npedestrian |&gt;\n  filter(Sensor == \"Southern Cross Station\") |&gt;\n  autoplot(log1p(Count)) +\n  labs(title = \"Southern Cross Pedestrians\")\n\n\n\n\n\n\n\n\nThat’s roughly balanced the two tails."
  },
  {
    "objectID": "week2/ex2-sol.html",
    "href": "week2/ex2-sol.html",
    "title": "Exercise Week 2: Solutions",
    "section": "",
    "text": "Explore the following four time series: Bricks from aus_production, Lynx from pelt, Close from gafa_stock, Demand from vic_elec.\n\nUse ? (or help()) to find out about the data in each series.\nWhat is the time interval of each series?\nUse autoplot() to produce a time plot of each series.\nFor the last plot, modify the axis labels and title.\n\n\n\n\n\naus_production\n\n# A tsibble: 218 x 7 [1Q]\n   Quarter  Beer Tobacco Bricks Cement Electricity   Gas\n     &lt;qtr&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 1956 Q1   284    5225    189    465        3923     5\n 2 1956 Q2   213    5178    204    532        4436     6\n 3 1956 Q3   227    5297    208    561        4806     7\n 4 1956 Q4   308    5681    197    570        4418     6\n 5 1957 Q1   262    5577    187    529        4339     5\n 6 1957 Q2   228    5651    214    604        4811     7\n 7 1957 Q3   236    5317    227    603        5259     7\n 8 1957 Q4   320    6152    222    582        4735     6\n 9 1958 Q1   272    5758    199    554        4608     5\n10 1958 Q2   233    5641    229    620        5196     7\n# ℹ 208 more rows\n\n\nThe observations are quarterly.\n\naus_production |&gt; autoplot(Bricks)\n\n\n\n\n\n\n\n\nAn upward trend is apparent until 1980, after which the number of clay bricks being produced starts to decline. A seasonal pattern is evident in this data. Some sharp drops in some quarters can also be seen.\n\n\n\n\ninterval(pelt)\n\n&lt;interval[1]&gt;\n[1] 1Y\n\n\nObservations are made once per year.\n\npelt |&gt; autoplot(Lynx)\n\n\n\n\n\n\n\n\nCanadian lynx trappings are cyclic, as the extent of peak trappings is unpredictable, and the spacing between the peaks is irregular but approximately 10 years.\n\n\n\n\ngafa_stock\n\n# A tsibble: 5,032 x 8 [!]\n# Key:       Symbol [4]\n   Symbol Date        Open  High   Low Close Adj_Close    Volume\n   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 AAPL   2014-01-02  79.4  79.6  78.9  79.0      67.0  58671200\n 2 AAPL   2014-01-03  79.0  79.1  77.2  77.3      65.5  98116900\n 3 AAPL   2014-01-06  76.8  78.1  76.2  77.7      65.9 103152700\n 4 AAPL   2014-01-07  77.8  78.0  76.8  77.1      65.4  79302300\n 5 AAPL   2014-01-08  77.0  77.9  77.0  77.6      65.8  64632400\n 6 AAPL   2014-01-09  78.1  78.1  76.5  76.6      65.0  69787200\n 7 AAPL   2014-01-10  77.1  77.3  75.9  76.1      64.5  76244000\n 8 AAPL   2014-01-13  75.7  77.5  75.7  76.5      64.9  94623200\n 9 AAPL   2014-01-14  76.9  78.1  76.8  78.1      66.1  83140400\n10 AAPL   2014-01-15  79.1  80.0  78.8  79.6      67.5  97909700\n# ℹ 5,022 more rows\n\n\nInterval is daily. Looking closer at the data, we can see that the index is a Date variable. It also appears that observations occur only on trading days, creating lots of implicit missing values.\n\ngafa_stock |&gt;\n  autoplot(Close)\n\n\n\n\n\n\n\n\nStock prices for these technology stocks have risen for most of the series, until mid-late 2018.\nThe four stocks are on different scales, so they are not directly comparable. A plot with faceting would be better.\n\ngafa_stock |&gt;\n  ggplot(aes(x=Date, y=Close, group=Symbol)) +\n  geom_line(aes(col=Symbol)) +\n  facet_grid(Symbol ~ ., scales='free')\n\n\n\n\n\n\n\n\nThe downturn in the second half of 2018 is now very clear, with Facebook taking a big drop (about 20%) in the middle of the year.\nThe stocks tend to move roughly together, as you would expect with companies in the same industry.\n\n\n\n\nvic_elec\n\n# A tsibble: 52,608 x 5 [30m] &lt;Australia/Melbourne&gt;\n   Time                Demand Temperature Date       Holiday\n   &lt;dttm&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;lgl&gt;  \n 1 2012-01-01 00:00:00  4383.        21.4 2012-01-01 TRUE   \n 2 2012-01-01 00:30:00  4263.        21.0 2012-01-01 TRUE   \n 3 2012-01-01 01:00:00  4049.        20.7 2012-01-01 TRUE   \n 4 2012-01-01 01:30:00  3878.        20.6 2012-01-01 TRUE   \n 5 2012-01-01 02:00:00  4036.        20.4 2012-01-01 TRUE   \n 6 2012-01-01 02:30:00  3866.        20.2 2012-01-01 TRUE   \n 7 2012-01-01 03:00:00  3694.        20.1 2012-01-01 TRUE   \n 8 2012-01-01 03:30:00  3562.        19.6 2012-01-01 TRUE   \n 9 2012-01-01 04:00:00  3433.        19.1 2012-01-01 TRUE   \n10 2012-01-01 04:30:00  3359.        19.0 2012-01-01 TRUE   \n# ℹ 52,598 more rows\n\n\nData is available at 30 minute intervals.\n\nvic_elec |&gt;\n  autoplot(Demand)\n\n\n\n\n\n\n\n\nAppears to have an annual seasonal pattern, where demand is higher during summer and winter. Can’t see much detail, so let’s zoom in.\n\nvic_elec |&gt;\n  filter(yearmonth(Time) == yearmonth(\"2012 June\")) |&gt;\n  autoplot(Demand)\n\n\n\n\n\n\n\n\nAppears to have a daily pattern, where less electricity is used overnight. Also appears to have a working day effect (less demand on weekends and holidays).\n\nvic_elec |&gt; autoplot(Demand/1e3) +\n  labs(\n    x = \"Date\",\n    y = \"Demand (GW)\",\n    title = \"Half-hourly electricity demand\",\n    subtitle = \"Victoria, Australia\"\n  )\n\n\n\n\n\n\n\n\nHere the annual seasonality is clear, with high volatility in summer, and peaks in summer and winter. The weekly seasonality is also visible, but the daily seasonality is hidden due to the compression on the horizontal axis."
  },
  {
    "objectID": "week2/ex2-sol.html#bricks",
    "href": "week2/ex2-sol.html#bricks",
    "title": "Exercise Week 2: Solutions",
    "section": "",
    "text": "aus_production\n\n# A tsibble: 218 x 7 [1Q]\n   Quarter  Beer Tobacco Bricks Cement Electricity   Gas\n     &lt;qtr&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 1956 Q1   284    5225    189    465        3923     5\n 2 1956 Q2   213    5178    204    532        4436     6\n 3 1956 Q3   227    5297    208    561        4806     7\n 4 1956 Q4   308    5681    197    570        4418     6\n 5 1957 Q1   262    5577    187    529        4339     5\n 6 1957 Q2   228    5651    214    604        4811     7\n 7 1957 Q3   236    5317    227    603        5259     7\n 8 1957 Q4   320    6152    222    582        4735     6\n 9 1958 Q1   272    5758    199    554        4608     5\n10 1958 Q2   233    5641    229    620        5196     7\n# ℹ 208 more rows\n\n\nThe observations are quarterly.\n\naus_production |&gt; autoplot(Bricks)\n\n\n\n\n\n\n\n\nAn upward trend is apparent until 1980, after which the number of clay bricks being produced starts to decline. A seasonal pattern is evident in this data. Some sharp drops in some quarters can also be seen."
  },
  {
    "objectID": "week2/ex2-sol.html#lynx",
    "href": "week2/ex2-sol.html#lynx",
    "title": "Exercise Week 2: Solutions",
    "section": "",
    "text": "interval(pelt)\n\n&lt;interval[1]&gt;\n[1] 1Y\n\n\nObservations are made once per year.\n\npelt |&gt; autoplot(Lynx)\n\n\n\n\n\n\n\n\nCanadian lynx trappings are cyclic, as the extent of peak trappings is unpredictable, and the spacing between the peaks is irregular but approximately 10 years."
  },
  {
    "objectID": "week2/ex2-sol.html#close",
    "href": "week2/ex2-sol.html#close",
    "title": "Exercise Week 2: Solutions",
    "section": "",
    "text": "gafa_stock\n\n# A tsibble: 5,032 x 8 [!]\n# Key:       Symbol [4]\n   Symbol Date        Open  High   Low Close Adj_Close    Volume\n   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 AAPL   2014-01-02  79.4  79.6  78.9  79.0      67.0  58671200\n 2 AAPL   2014-01-03  79.0  79.1  77.2  77.3      65.5  98116900\n 3 AAPL   2014-01-06  76.8  78.1  76.2  77.7      65.9 103152700\n 4 AAPL   2014-01-07  77.8  78.0  76.8  77.1      65.4  79302300\n 5 AAPL   2014-01-08  77.0  77.9  77.0  77.6      65.8  64632400\n 6 AAPL   2014-01-09  78.1  78.1  76.5  76.6      65.0  69787200\n 7 AAPL   2014-01-10  77.1  77.3  75.9  76.1      64.5  76244000\n 8 AAPL   2014-01-13  75.7  77.5  75.7  76.5      64.9  94623200\n 9 AAPL   2014-01-14  76.9  78.1  76.8  78.1      66.1  83140400\n10 AAPL   2014-01-15  79.1  80.0  78.8  79.6      67.5  97909700\n# ℹ 5,022 more rows\n\n\nInterval is daily. Looking closer at the data, we can see that the index is a Date variable. It also appears that observations occur only on trading days, creating lots of implicit missing values.\n\ngafa_stock |&gt;\n  autoplot(Close)\n\n\n\n\n\n\n\n\nStock prices for these technology stocks have risen for most of the series, until mid-late 2018.\nThe four stocks are on different scales, so they are not directly comparable. A plot with faceting would be better.\n\ngafa_stock |&gt;\n  ggplot(aes(x=Date, y=Close, group=Symbol)) +\n  geom_line(aes(col=Symbol)) +\n  facet_grid(Symbol ~ ., scales='free')\n\n\n\n\n\n\n\n\nThe downturn in the second half of 2018 is now very clear, with Facebook taking a big drop (about 20%) in the middle of the year.\nThe stocks tend to move roughly together, as you would expect with companies in the same industry."
  },
  {
    "objectID": "week2/ex2-sol.html#demand",
    "href": "week2/ex2-sol.html#demand",
    "title": "Exercise Week 2: Solutions",
    "section": "",
    "text": "vic_elec\n\n# A tsibble: 52,608 x 5 [30m] &lt;Australia/Melbourne&gt;\n   Time                Demand Temperature Date       Holiday\n   &lt;dttm&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;lgl&gt;  \n 1 2012-01-01 00:00:00  4383.        21.4 2012-01-01 TRUE   \n 2 2012-01-01 00:30:00  4263.        21.0 2012-01-01 TRUE   \n 3 2012-01-01 01:00:00  4049.        20.7 2012-01-01 TRUE   \n 4 2012-01-01 01:30:00  3878.        20.6 2012-01-01 TRUE   \n 5 2012-01-01 02:00:00  4036.        20.4 2012-01-01 TRUE   \n 6 2012-01-01 02:30:00  3866.        20.2 2012-01-01 TRUE   \n 7 2012-01-01 03:00:00  3694.        20.1 2012-01-01 TRUE   \n 8 2012-01-01 03:30:00  3562.        19.6 2012-01-01 TRUE   \n 9 2012-01-01 04:00:00  3433.        19.1 2012-01-01 TRUE   \n10 2012-01-01 04:30:00  3359.        19.0 2012-01-01 TRUE   \n# ℹ 52,598 more rows\n\n\nData is available at 30 minute intervals.\n\nvic_elec |&gt;\n  autoplot(Demand)\n\n\n\n\n\n\n\n\nAppears to have an annual seasonal pattern, where demand is higher during summer and winter. Can’t see much detail, so let’s zoom in.\n\nvic_elec |&gt;\n  filter(yearmonth(Time) == yearmonth(\"2012 June\")) |&gt;\n  autoplot(Demand)\n\n\n\n\n\n\n\n\nAppears to have a daily pattern, where less electricity is used overnight. Also appears to have a working day effect (less demand on weekends and holidays).\n\nvic_elec |&gt; autoplot(Demand/1e3) +\n  labs(\n    x = \"Date\",\n    y = \"Demand (GW)\",\n    title = \"Half-hourly electricity demand\",\n    subtitle = \"Victoria, Australia\"\n  )\n\n\n\n\n\n\n\n\nHere the annual seasonality is clear, with high volatility in summer, and peaks in summer and winter. The weekly seasonality is also visible, but the daily seasonality is hidden due to the compression on the horizontal axis."
  },
  {
    "objectID": "week2/ex2-sol.html#total-private-employment-in-the-us",
    "href": "week2/ex2-sol.html#total-private-employment-in-the-us",
    "title": "Exercise Week 2: Solutions",
    "section": "Total Private Employment in the US",
    "text": "Total Private Employment in the US\n\nus_employment |&gt;\n  filter(Title == \"Total Private\") |&gt;\n  autoplot(Employed)\n\n\n\n\n\n\n\n\nThere is a strong trend and seasonality. Some cyclic behaviour is seen, with a big drop due to the global financial crisis.\n\nus_employment |&gt;\n  filter(Title == \"Total Private\") |&gt;\n  gg_season(Employed)\n\n\n\n\n\n\n\n\n\nus_employment |&gt;\n  filter(Title == \"Total Private\") |&gt;\n  gg_subseries(Employed)\n\n\n\n\n\n\n\n\n\nus_employment |&gt;\n  filter(Title == \"Total Private\") |&gt;\n  gg_lag(Employed)\n\n\n\n\n\n\n\n\n\nus_employment |&gt;\n  filter(Title == \"Total Private\") |&gt;\n  ACF(Employed) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\nIn all of these plots, the trend is so dominant that it is hard to see anything else. We need to remove the trend so we can explore the other features of the data."
  },
  {
    "objectID": "week2/ex2-sol.html#brick-production-in-australia",
    "href": "week2/ex2-sol.html#brick-production-in-australia",
    "title": "Exercise Week 2: Solutions",
    "section": "Brick production in Australia",
    "text": "Brick production in Australia\n\naus_production |&gt;\n  autoplot(Bricks)\n\n\n\n\n\n\n\n\nA positive trend in the first 20 years, and a negative trend in the next 25 years. Strong quarterly seasonality, with some cyclicity – note the recessions in the 1970s and 1980s.\n\naus_production |&gt;\n  gg_season(Bricks)\n\n\n\n\n\n\n\n\nBrick production tends to be lowest in the first quarter and peak in either quarter 2 or quarter 3.\n\naus_production |&gt;\n  gg_subseries(Bricks)\n\n\n\n\n\n\n\n\nThe decrease in the last 25 years has been weakest in Q1.\n\naus_production |&gt;\n  gg_lag(Bricks, geom='point')\n\n\n\n\n\n\n\naus_production |&gt;\n  ACF(Bricks) |&gt; autoplot()\n\n\n\n\n\n\n\n\nThe seasonality shows up as peaks at lags 4, 8, 12, 16, 20, …. The trend is seen with the slow decline on the positive side."
  },
  {
    "objectID": "week2/ex2-sol.html#snow-hare-trappings-in-canada",
    "href": "week2/ex2-sol.html#snow-hare-trappings-in-canada",
    "title": "Exercise Week 2: Solutions",
    "section": "Snow hare trappings in Canada",
    "text": "Snow hare trappings in Canada\n\npelt |&gt;\n  autoplot(Hare)\n\n\n\n\n\n\n\n\nThere is some cyclic behaviour with substantial variation in the length of the period.\n\npelt |&gt;\n  gg_lag(Hare, geom='point')\n\n\n\n\n\n\n\npelt |&gt;\n  ACF(Hare) |&gt; autoplot()\n\n\n\n\n\n\n\n\nThe cyclic period seems to have an average of about 10 (due to the local maximum in ACF at lag 10)."
  },
  {
    "objectID": "week2/ex2-sol.html#h02-sales-in-australia",
    "href": "week2/ex2-sol.html#h02-sales-in-australia",
    "title": "Exercise Week 2: Solutions",
    "section": "H02 sales in Australia",
    "text": "H02 sales in Australia\nThere are four series corresponding to H02 sales, so we will add them together.\n\nh02 &lt;- PBS |&gt;\n  filter(ATC2 == \"H02\") |&gt;\n  group_by(ATC2) |&gt;\n  summarise(Cost = sum(Cost)) |&gt;\n  ungroup()\n\n\nh02 |&gt;\n  autoplot(Cost)\n\n\n\n\n\n\n\n\nA positive trend with strong monthly seasonality, dropping suddenly every February.\n\nh02 |&gt;\n  gg_season(Cost)\n\n\n\n\n\n\n\n\n\nh02 |&gt;\n  gg_subseries(Cost)\n\n\n\n\n\n\n\n\nThe trends have been greater in the higher peaking months – this leads to increasing seasonal variation.\n\nh02 |&gt;\n  gg_lag(Cost, geom='point', lags=1:16)\n\n\n\n\n\n\n\nh02 |&gt;\n  ACF(Cost) |&gt; autoplot()\n\n\n\n\n\n\n\n\nThe large January sales show up as a separate cluster of points in the lag plots. The strong seasonality is clear in the ACF plot."
  },
  {
    "objectID": "week2/ex2-sol.html#us-gasoline-sales",
    "href": "week2/ex2-sol.html#us-gasoline-sales",
    "title": "Exercise Week 2: Solutions",
    "section": "US gasoline sales",
    "text": "US gasoline sales\n\nus_gasoline |&gt;\n  autoplot(Barrels)\n\n\n\n\n\n\n\n\nA positive trend until 2008, and then the global financial crisis led to a drop in sales until 2012. The shape of the seasonality seems to have changed over time.\n\nus_gasoline |&gt;\n  gg_season(Barrels)\n\n\n\n\n\n\n\n\nThere is a lot of noise making it hard to see the overall seasonal pattern. However, it seems to drop towards the end of quarter 4.\n\nus_gasoline |&gt;\n  gg_subseries(Barrels)\n\n\n\n\n\n\n\n\nThe blue lines are helpful in seeing the average seasonal pattern.\n\nus_gasoline |&gt;\n  gg_lag(Barrels, geom='point', lags=1:16)\n\n\n\n\n\n\n\nus_gasoline |&gt;\n  ACF(Barrels, lag_max = 150) |&gt; autoplot()\n\n\n\n\n\n\n\n\nThe seasonality is seen if we increase the lags to at least 2 years (approx 104 weeks)"
  },
  {
    "objectID": "week12/index.html",
    "href": "week12/index.html",
    "title": "Week 12: Dynamic regression",
    "section": "",
    "text": "How to combine regression models with ARIMA models to form dynamic regression models\nDynamic harmonic regression to handle complex seasonality\nLagged predictors"
  },
  {
    "objectID": "week12/index.html#what-you-will-learn-this-week",
    "href": "week12/index.html#what-you-will-learn-this-week",
    "title": "Week 12: Dynamic regression",
    "section": "",
    "text": "How to combine regression models with ARIMA models to form dynamic regression models\nDynamic harmonic regression to handle complex seasonality\nLagged predictors"
  },
  {
    "objectID": "week12/index.html#pre-class-activities",
    "href": "week12/index.html#pre-class-activities",
    "title": "Week 12: Dynamic regression",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 10 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week12/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week12/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 12: Dynamic regression",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 2-6 from Section 10.7 of the book."
  },
  {
    "objectID": "week12/index.html#post-class-activities",
    "href": "week12/index.html#post-class-activities",
    "title": "Week 12: Dynamic regression",
    "section": "Post-class activities",
    "text": "Post-class activities\n\nDo any exercises not yet finished.\nComplete past exams: [2022] [2023] [2024]\nRe-read the textbook\nListen again to all lectures"
  },
  {
    "objectID": "week11/index.html",
    "href": "week11/index.html",
    "title": "Week 11: Forecasting using regression",
    "section": "",
    "text": "Useful predictors for time series forecasting using regression\nSelecting predictors\nEx ante and ex post forecasting"
  },
  {
    "objectID": "week11/index.html#what-you-will-learn-this-week",
    "href": "week11/index.html#what-you-will-learn-this-week",
    "title": "Week 11: Forecasting using regression",
    "section": "",
    "text": "Useful predictors for time series forecasting using regression\nSelecting predictors\nEx ante and ex post forecasting"
  },
  {
    "objectID": "week11/index.html#pre-class-activities",
    "href": "week11/index.html#pre-class-activities",
    "title": "Week 11: Forecasting using regression",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 7 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week11/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week11/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 11: Forecasting using regression",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 1-7 from Section 7.10 of the book."
  },
  {
    "objectID": "week10/index.html",
    "href": "week10/index.html",
    "title": "Week 10: ARIMA models",
    "section": "",
    "text": "Seasonal ARIMA models\nComputing forecasts for ARIMA models\nARIMA vs ETS models"
  },
  {
    "objectID": "week10/index.html#what-you-will-learn-this-week",
    "href": "week10/index.html#what-you-will-learn-this-week",
    "title": "Week 10: ARIMA models",
    "section": "",
    "text": "Seasonal ARIMA models\nComputing forecasts for ARIMA models\nARIMA vs ETS models"
  },
  {
    "objectID": "week10/index.html#pre-class-activities",
    "href": "week10/index.html#pre-class-activities",
    "title": "Week 10: ARIMA models",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Sections 9.9-9.10 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week10/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week10/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 10: ARIMA models",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 11-16 from Section 9.11 of the book."
  },
  {
    "objectID": "week1/index.html",
    "href": "week1/index.html",
    "title": "Week 1: Introduction to forecasting & R",
    "section": "",
    "text": "How to think about forecasting from a statistical perspective\nWhat makes something easy or hard to forecast?\nUsing the tsibble package in R"
  },
  {
    "objectID": "week1/index.html#what-you-will-learn-this-week",
    "href": "week1/index.html#what-you-will-learn-this-week",
    "title": "Week 1: Introduction to forecasting & R",
    "section": "",
    "text": "How to think about forecasting from a statistical perspective\nWhat makes something easy or hard to forecast?\nUsing the tsibble package in R"
  },
  {
    "objectID": "week1/index.html#pre-class-activities",
    "href": "week1/index.html#pre-class-activities",
    "title": "Week 1: Introduction to forecasting & R",
    "section": "Pre-class activities",
    "text": "Pre-class activities\n\nBefore we start classes, make sure that are familiar with R, RStudio and the tidyverse packages. If you’ve already done ETC1010, then you may not need to do anything! But if you’re new to R and the tidyverse, then you will need to get yourself up-to-speed. Work through the first five modules of the StartR tutorial at startr.numbat.space. Do as much of it as you think you need. For those students new to R, it is strongly recommended that you do all five modules. For those who have previously used R, concentrate on the parts where you feel you are weakest.\nInstall R and RStudio on your personal computer. Instructions are provided at OTexts.com/fpp3/appendix-using-r.html.\nRead Chapter 1 of the textbook and watch all embedded videos\nWatch this video"
  },
  {
    "objectID": "week1/index.html#monday-lecture",
    "href": "week1/index.html#monday-lecture",
    "title": "Week 1: Introduction to forecasting & R",
    "section": "Monday lecture",
    "text": "Monday lecture\n\n\nDownload slides"
  },
  {
    "objectID": "week1/index.html#tuesday-workshop",
    "href": "week1/index.html#tuesday-workshop",
    "title": "Week 1: Introduction to forecasting & R",
    "section": "Tuesday workshop",
    "text": "Tuesday workshop\nActivities for Tuesday workshop"
  },
  {
    "objectID": "week1/index.html#check-your-understanding",
    "href": "week1/index.html#check-your-understanding",
    "title": "Week 1: Introduction to forecasting & R",
    "section": "Check your understanding",
    "text": "Check your understanding"
  },
  {
    "objectID": "week1/index.html#assignments",
    "href": "week1/index.html#assignments",
    "title": "Week 1: Introduction to forecasting & R",
    "section": "Assignments",
    "text": "Assignments\n\nForecasting Competition is due on Friday 07 March."
  },
  {
    "objectID": "other-ex-sol.html",
    "href": "other-ex-sol.html",
    "title": "Additional Exercise Solutions",
    "section": "",
    "text": "fpp3 1.8, Ex 1\n\nFor cases 3 and 4 in Section 1.5, list the possible predictor variables that might be useful, assuming that the relevant data are available.\n\nCase 3: the following predictor variables might be useful, assuming that the relevant data are available:\n\nModel and make of the vehicle\nOdometer reading\nConditions of the vehicle\nCompany the vehicle was leased to\nColor of the vehicle\nDate of sale\n\nCase 4: the following predictor variables might be useful, assuming that the relevant data are available:\n\nDay of the week\nDay of the year\nIs the day before long weekend\nIs the day in the end of long weekend\nIs the day before or in the beginning of school holidays (one variable per every state)\nIs the day in the end of school holidays (one variable per every state)\nIs the day before or in the beginning of a major sport event\nIs the day after of a major sport event\nCompetitors’ prices (relative to the price of the airline in question)\nIs there a pilot strike at some of the competitors’ airlines\nIs there a pilot strike at the airline in question\n\n\n\nfpp3 1.8, Ex 2\n\nFor case 3 in Section 1.5, describe the five steps of forecasting in the context of this project.\n\n\n1. Problem definition\n\nThe main stakeholders should be defined and everyone questioned about which way he or she can benefit from the new system. In case of the fleet company probably the group of specialists was not recognized as stakeholders which led to complications in gathering relevant information and later in finding an appropriate statistical approach and deployment of the new forecasting method.\n\n\n\n2. Gathering information\n\nData set of past sales should be obtained, including surrounding information such as the way data were gathered, possible outliers and incorrect records, special values in the data.\nExpertise knowledge should be obtained from people responsible for the sales such as seasonal price fluctuations, if there is dependency of the price on the situation in economy, also finding other possible factors which can influence the price.\n\n\n\n3. Preliminary (exploratory) analysis\n\nPossible outliers and inconsistent information should be found (for example very small, zero or even negative prices).\nGraphs which show dependency of the sale price on different predictor variables should be considered.\nDependency of the sale price on month of the year should be plot.\n\n\n\n4. Choosing and fitting models\n\nA model to start from (for example a linear model) and predictor variables which most likely affect the forecasts should be chosen. Predicting performance of the model should be evaluated.\nThe model should be changed (for example by transforming parameters, adding or removing predictor variables) and it’s performance evaluated. This should be done iteratively a few times until a satisfactory model is found.\n\n\n\n5. Using and evaluating a forecasting model\n\nThe appropriate software should be deployed to the company and relevant people should be educated how to use this software.\nForecasting accuracy should be checked against new sales. If necessary the model should be updated and then the deployed software.\n\n\n\n\nfpp3 3.7, Ex 6\n\nShow that a 3\\times 5 MA is equivalent to a 7-term weighted moving average with weights of 0.067, 0.133, 0.200, 0.200, 0.200, 0.133, and 0.067.\n\n5-term moving average: z_j = \\frac{1}{5}(y_{j-2}+y_{j-1}+y_j+y_{j+1}+y_{j+2}). 3-term moving average: u_t = \\frac{1}{3}(z_{t-1}+z_t+z_{t+1}). Substituting expression for z_j into the latter formula we get \\begin{align*}\n  u_t &= \\frac{1}{3}\\left(\\frac{1}{5}\\left(y_{t-3}+y_{t-2}+y_{t-1}+y_{t}+y_{t+1}\\right)+\\frac{1}{5}\\left(y_{t-2}+y_{t-1}+y_t+y_{t+1}+y_{t+2}\\right)+\\frac{1}{5}\\left(y_{t-1}+y_{t}+y_{t+1}+y_{t+2}+y_{t+3}\\right)\\right).\\\\\n  &= \\frac{1}{15}\\left(y_{t-3}+2y_{t-2}+3y_{t-1}+3y_{t}+3y_{t+1}+2y_{t+2}+y_{t+3}\\right),\n\\end{align*} which is a 7-term weighted moving average with weights of 0.067, 0.133, 0.200, 0.200, 0.200, 0.133, and 0.067\n\n\nfpp3 3.7, Ex 7\n\nConsider the last five years of the Gas data from aus_production.\n\n\ngas &lt;- tail(aus_production, 5*4) |&gt; select(Gas)\n\n\n\nPlot the time series. Can you identify seasonal fluctuations and/or a trend-cycle?\n\n\n\ngas &lt;- tail(aus_production, 5 * 4) |&gt; select(Gas)\ngas |&gt;\n  autoplot(Gas) + labs(y = \"Petajoules\")\n\n\n\n\n\n\n\n\nThere is some strong seasonality and a trend.\n\n\nUse classical_decomposition with type=multiplicative to calculate the trend-cycle and seasonal indices.\nDo the results support the graphical interpretation from part a?\n\n\n\ndecomp &lt;- gas |&gt;\n  model(decomp = classical_decomposition(Gas, type = \"multiplicative\")) |&gt;\n  components()\ndecomp |&gt; autoplot()\n\n\n\n\n\n\n\n\nThe decomposition has captured the seasonality and a slight trend.\n\n\nCompute and plot the seasonally adjusted data.\n\n\n\nas_tsibble(decomp) |&gt;\n  autoplot(season_adjust) +\n  labs(title = \"Seasonally adjusted data\", y = \"Petajoules\")\n\n\n\n\n\n\n\n\n\n\nChange one observation to be an outlier (e.g., add 300 to one observation), and recompute the seasonally adjusted data. What is the effect of the outlier?\nDoes it make any difference if the outlier is near the end rather than in the middle of the time series?\n\n\n\ngas |&gt;\n  mutate(Gas = if_else(Quarter == yearquarter(\"2007Q4\"), Gas + 300, Gas)) |&gt;\n  model(decomp = classical_decomposition(Gas, type = \"multiplicative\")) |&gt;\n  components() |&gt;\n  as_tsibble() |&gt;\n  autoplot(season_adjust) +\n  labs(title = \"Seasonally adjusted data\", y = \"Petajoules\")\n\n\n\n\n\n\n\n\n\nThe “seasonally adjusted” data now shows some seasonality because the outlier has affected the estimate of the seasonal component.\n\n\ngas |&gt;\n  mutate(Gas = if_else(Quarter == yearquarter(\"2010Q2\"), Gas + 300, Gas)) |&gt;\n  model(decomp = classical_decomposition(Gas, type = \"multiplicative\")) |&gt;\n  components() |&gt;\n  as_tsibble() |&gt;\n  autoplot(season_adjust) +\n  labs(title = \"Seasonally adjusted data\", y = \"Petajoules\")\n\n\n\n\n\n\n\n\nThe seasonally adjusted data now show no seasonality because the outlier is in the part of the data where the trend can’t be estimated.\n\n\nfpp3 3.7, Ex 8\n\nRecall your retail time series data (from Exercise 8 in Section 2.10). Decompose the series using X11. Does it reveal any outliers, or unusual features that you had not noticed previously?\n\n\nset.seed(12345678)\nmyseries &lt;- aus_retail |&gt;\n  filter(`Series ID` == sample(aus_retail$`Series ID`, 1))\ndecomp &lt;- myseries |&gt;\n  model(x11 = X_13ARIMA_SEATS(Turnover ~ x11())) |&gt;\n  components()\ndecomp |&gt; autoplot()\n\n\n\n\n\n\n\n\nTwo outliers are now evident in the “irregular” component — in December 1995 and July 2010.\n\n\nfpp3 5.10, Ex 7\n\nFor your retail time series (from Exercise 8 in Section 2.10):\n\nCreate a training dataset consisting of observations before 2011.\nCheck that your data have been split appropriately by producing the following plot.\nCalculate seasonal naïve forecasts using SNAIVE() applied to your training data (myseries_train).\nCheck the residuals. Do the residuals appear to be uncorrelated and normally distributed?\nProduce forecasts for the test data.\nCompare the accuracy of your forecasts against the actual values.\nHow sensitive are the accuracy measures to the amount of training data used?\n\n\n\nset.seed(12345678)\nmyseries &lt;- aus_retail |&gt;\n  filter(`Series ID` == sample(aus_retail$`Series ID`, 1))\nmyseries_train &lt;- myseries |&gt;\n  filter(year(Month) &lt; 2011)\nautoplot(myseries, Turnover) +\n  autolayer(myseries_train, Turnover, colour = \"red\")\n\n\n\n\n\n\n\n\nThe plot indicates that the training data has been extracted correctly.\n\nfit &lt;- myseries_train |&gt;\n  model(SNAIVE(Turnover))\n\n\nfit |&gt; gg_tsresiduals()\n\n\n\n\n\n\n\n\nThe residuals appear very auto-correlated as many lags exceed the significance threshold. This can also be seen in the residual plot, where there are periods of sustained high and low residuals. The distribution does not appear normally distributed, and is not centred around zero.\n\nfc &lt;- fit |&gt;\n  forecast(new_data = anti_join(myseries, myseries_train))\nfc |&gt; autoplot(myseries)\n\n\n\n\n\n\n\nbind_rows(\n  accuracy(fit),\n  accuracy(fc, myseries)\n) |&gt;\n  select(-State, -Industry, -.model)\n\n# A tibble: 2 × 9\n  .type       ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Training 0.439  1.21 0.915  5.23 12.4   1     1    0.768\n2 Test     0.836  1.55 1.24   5.94  9.06  1.36  1.28 0.601\n\n\nThe accuracy on the training data is substantially better than the out-of-sample forecast accuracy. This is common, and especially evident in this example as the model has failed to capture the trend in the data. This can be seen in the mean error, which is above zero as the model predictions do not account for the upward trend.\n\nmyseries_accuracy &lt;- function(data, last_training_year) {\n  myseries_train &lt;- data |&gt;\n    filter(year(Month) &lt;= last_training_year)\n  fit &lt;- myseries_train |&gt;\n    model(SNAIVE(Turnover))\n  fc &lt;- fit |&gt;\n    forecast(new_data = anti_join(myseries, myseries_train))\n  bind_rows(\n    accuracy(fit),\n    accuracy(fc, myseries)\n  ) |&gt;\n    mutate(last_training_year = last_training_year) |&gt;\n    select(last_training_year, .type, ME:ACF1)\n}\nas.list(2011:2017) |&gt;\n  purrr::map_dfr(myseries_accuracy, data = myseries) |&gt;\n  ggplot(aes(x = last_training_year, y = RMSE, group = .type)) +\n  geom_line(aes(col = .type))\n\n\n\n\n\n\n\n\nThe accuracy on the training data is almost unchanged when the size of the training set is increased. However, the accuracy on the test data decreases as we are averaging RMSE over the forecast horizon, and with less training data the forecasts horizons can be longer.\n\n\nfpp3 5.10, Ex 9\n\n\nCreate a training set for household wealth (hh_budget) by withholding the last four years as a test set.\n\n\n\ntrain &lt;- hh_budget |&gt;\n  filter(Year &lt;= max(Year) - 4)\n\n\n\nFit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.\n\n\n\nfit &lt;- train |&gt;\n  model(\n    naive = NAIVE(Wealth),\n    drift = RW(Wealth ~ drift()),\n    mean = MEAN(Wealth)\n  )\nfc &lt;- fit |&gt; forecast(h = 4)\n\n\n\nCompute the accuracy of your forecasts. Which method does best?\n\n\n\nfc |&gt;\n  accuracy(hh_budget) |&gt;\n  arrange(Country, MASE)\n\n# A tibble: 12 × 11\n   .model Country   .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 drift  Australia Test   29.1  35.5  29.1  7.23  7.23 1.73  1.48   0.210 \n 2 naive  Australia Test   34.7  41.5  34.7  8.64  8.64 2.06  1.73   0.216 \n 3 mean   Australia Test   35.7  42.3  35.7  8.89  8.89 2.12  1.76   0.216 \n 4 drift  Canada    Test   33.3  37.2  33.3  6.09  6.09 1.73  1.57  -0.229 \n 5 naive  Canada    Test   46.2  51.0  46.2  8.46  8.46 2.40  2.15  -0.0799\n 6 mean   Canada    Test   90.4  92.9  90.4 16.7  16.7  4.69  3.92  -0.0799\n 7 drift  Japan     Test   14.7  17.9  14.7  2.44  2.44 0.943 0.967 -0.229 \n 8 naive  Japan     Test   36.3  37.8  36.3  6.06  6.06 2.34  2.04  -0.534 \n 9 mean   Japan     Test  100.  101.  100.  16.8  16.8  6.45  5.46  -0.534 \n10 drift  USA       Test   75.9  76.2  75.9 12.7  12.7  2.88  2.43  -0.561 \n11 naive  USA       Test   82.1  82.5  82.1 13.8  13.8  3.12  2.63  -0.423 \n12 mean   USA       Test   82.9  83.3  82.9 13.9  13.9  3.15  2.65  -0.423 \n\nfc |&gt;\n  accuracy(hh_budget) |&gt;\n  group_by(.model) |&gt;\n  summarise(MASE = mean(MASE)) |&gt;\n  ungroup() |&gt;\n  arrange(MASE)\n\n# A tibble: 3 × 2\n  .model  MASE\n  &lt;chr&gt;  &lt;dbl&gt;\n1 drift   1.82\n2 naive   2.48\n3 mean    4.10\n\n\nThe drift method is better for every country, and on average.\n\n\nDo the residuals from the best method resemble white noise?\n\n\n\nfit |&gt;\n  filter(Country == \"Australia\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt;\n  filter(Country == \"Canada\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt;\n  filter(Country == \"Japan\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt;\n  filter(Country == \"USA\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\n\nIn all cases, the residuals look like white noise.\n\n\nfpp3 5.10, Ex 10\n\n\nCreate a training set for Australian takeaway food turnover (aus_retail) by withholding the last four years as a test set.\n\n\n\ntakeaway &lt;- aus_retail |&gt;\n  filter(Industry == \"Takeaway food services\") |&gt;\n  summarise(Turnover = sum(Turnover))\ntrain &lt;- takeaway |&gt;\n  filter(Month &lt;= max(Month) - 4 * 12)\n\n\n\nFit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.\n\n\n\nfit &lt;- train |&gt;\n  model(\n    naive = NAIVE(Turnover),\n    drift = RW(Turnover ~ drift()),\n    mean = MEAN(Turnover),\n    snaive = SNAIVE(Turnover)\n  )\nfc &lt;- fit |&gt; forecast(h = \"4 years\")\n\n\n\nCompute the accuracy of your forecasts. Which method does best?\n\n\n\nfc |&gt;\n  accuracy(takeaway) |&gt;\n  arrange(MASE)\n\n# A tibble: 4 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 naive  Test  -12.4  119.  96.4 -1.49  6.66  2.30  2.25 0.613\n2 drift  Test  -93.7  130. 108.  -6.82  7.67  2.58  2.46 0.403\n3 snaive Test  177.   192. 177.  11.7  11.7   4.22  3.64 0.902\n4 mean   Test  829.   838. 829.  55.7  55.7  19.8  15.8  0.613\n\n\nThe naive method is best here.\n\n\nDo the residuals from the best method resemble white noise?\n\n\n\nfit |&gt;\n  select(naive) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\n\nThis is far from white noise. There is strong seasonality and increasing variance that has not been accounted for by the naive model.\n\n\nfpp3 8.8, Ex6\n\nForecast the Chinese GDP from the global_economy data set using an ETS model. Experiment with the various options in the ETS() function to see how much the forecasts change with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each is doing to the forecasts.\n\n\n[Hint: use h=20 when forecasting, so you can clearly see the differences between the various options when plotting the forecasts.]\n\n\nchina &lt;- global_economy |&gt;\n  filter(Country == \"China\")\nchina |&gt; autoplot(GDP)\n\n\n\n\n\n\n\n\n\nIt clearly needs a relatively strong transformation due to the increasing variance.\n\n\nchina |&gt; autoplot(box_cox(GDP, 0.2))\n\n\n\n\n\n\n\nchina |&gt; features(GDP, guerrero)\n\n# A tibble: 1 × 2\n  Country lambda_guerrero\n  &lt;fct&gt;             &lt;dbl&gt;\n1 China           -0.0345\n\n\n\nMaking \\lambda=0.2 looks ok.\nThe Guerrero method suggests an even stronger transformation. Let’s also try a log.\n\n\nfit &lt;- china |&gt;\n  model(\n    ets = ETS(GDP),\n    ets_damped = ETS(GDP ~ trend(\"Ad\")),\n    ets_bc = ETS(box_cox(GDP, 0.2)),\n    ets_log = ETS(log(GDP))\n  )\n\nfit\n\n# A mable: 1 x 5\n# Key:     Country [1]\n  Country          ets    ets_damped       ets_bc      ets_log\n  &lt;fct&gt;        &lt;model&gt;       &lt;model&gt;      &lt;model&gt;      &lt;model&gt;\n1 China   &lt;ETS(M,A,N)&gt; &lt;ETS(M,Ad,N)&gt; &lt;ETS(A,A,N)&gt; &lt;ETS(A,A,N)&gt;\n\naugment(fit)\n\n# A tsibble: 232 x 7 [1Y]\n# Key:       Country, .model [4]\n   Country .model  Year          GDP      .fitted        .resid   .innov\n   &lt;fct&gt;   &lt;chr&gt;  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 China   ets     1960 59716467625. 49001691297.  10714776328.  0.219  \n 2 China   ets     1961 50056868958. 66346643194. -16289774236. -0.246  \n 3 China   ets     1962 47209359006. 51607368186.  -4398009180. -0.0852 \n 4 China   ets     1963 50706799903. 47386494407.   3320305495.  0.0701 \n 5 China   ets     1964 59708343489. 51919091574.   7789251914.  0.150  \n 6 China   ets     1965 70436266147. 63350421234.   7085844913.  0.112  \n 7 China   ets     1966 76720285970. 76289186599.    431099371.  0.00565\n 8 China   ets     1967 72881631327. 82708375812.  -9826744486. -0.119  \n 9 China   ets     1968 70846535056. 75804820984.  -4958285928. -0.0654 \n10 China   ets     1969 79705906247. 72222259470.   7483646777.  0.104  \n# ℹ 222 more rows\n\nfit |&gt;\n  forecast(h = \"20 years\") |&gt;\n  autoplot(china, level = NULL)\n\n\n\n\n\n\n\n\n\nThe transformations have a big effect, with small lambda values creating big increases in the forecasts.\nThe damping has relatively a small effect.\n\n\n\nfpp3 8.8, Ex7\n\nFind an ETS model for the Gas data from aus_production and forecast the next few years. Why is multiplicative seasonality necessary here? Experiment with making the trend damped. Does it improve the forecasts?\n\n\naus_production |&gt; autoplot(Gas)\n\n\n\n\n\n\n\n\n\nThere is a huge increase in variance as the series increases in level. =&gt; That makes it necessary to use multiplicative seasonality.\n\n\nfit &lt;- aus_production |&gt;\n  model(\n    hw = ETS(Gas ~ error(\"M\") + trend(\"A\") + season(\"M\")),\n    hwdamped = ETS(Gas ~ error(\"M\") + trend(\"Ad\") + season(\"M\")),\n  )\n\nfit |&gt; glance()\n\n# A tibble: 2 × 9\n  .model    sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE    MAE\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 hw       0.00324   -831. 1681. 1682. 1711.  21.1  32.2 0.0413\n2 hwdamped 0.00329   -832. 1684. 1685. 1718.  21.1  32.0 0.0417\n\n\n\nThe non-damped model seems to be doing slightly better here, probably because the trend is very strong over most of the historical data.\n\n\nfit |&gt;\n  select(hw) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt; tidy()\n\n# A tibble: 19 × 3\n   .model   term  estimate\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;\n 1 hw       alpha   0.653 \n 2 hw       beta    0.144 \n 3 hw       gamma   0.0978\n 4 hw       l[0]    5.95  \n 5 hw       b[0]    0.0706\n 6 hw       s[0]    0.931 \n 7 hw       s[-1]   1.18  \n 8 hw       s[-2]   1.07  \n 9 hw       s[-3]   0.816 \n10 hwdamped alpha   0.649 \n11 hwdamped beta    0.155 \n12 hwdamped gamma   0.0937\n13 hwdamped phi     0.980 \n14 hwdamped l[0]    5.86  \n15 hwdamped b[0]    0.0994\n16 hwdamped s[0]    0.928 \n17 hwdamped s[-1]   1.18  \n18 hwdamped s[-2]   1.08  \n19 hwdamped s[-3]   0.817 \n\nfit |&gt;\n  augment() |&gt;\n  filter(.model == \"hw\") |&gt;\n  features(.innov, ljung_box, lag = 24)\n\n# A tibble: 1 × 3\n  .model lb_stat lb_pvalue\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 hw        57.1  0.000161\n\n\n\nThere is still some small correlations left in the residuals, showing the model has not fully captured the available information.\nThere also appears to be some heteroskedasticity in the residuals with larger variance in the first half the series.\n\n\nfit |&gt;\n  forecast(h = 36) |&gt;\n  filter(.model == \"hw\") |&gt;\n  autoplot(aus_production)\n\n\n\n\n\n\n\n\nWhile the point forecasts look ok, the intervals are excessively wide.\n\n\nfpp3 10.7, Ex 1\n\nThis exercise uses data set LakeHuron giving the level of Lake Huron from 1875–1972.\n\n\n\nConvert the data to a tsibble object using the as_tsibble() function.\nFit a piecewise linear trend model to the Lake Huron data with a knot at 1920 and an ARMA error structure.\nForecast the level for the next 30 years. Do you think the extrapolated linear trend is realistic?\n\n\n\nhuron &lt;- as_tsibble(LakeHuron)\nfit &lt;- huron |&gt;\n  model(ARIMA(value ~ trend(knot = 1920)))\nreport(fit)\n\nSeries: value \nModel: LM w/ ARIMA(2,0,0) errors \n\nCoefficients:\n         ar1      ar2  trend(knot = 1920)trend  trend(knot = 1920)trend_46\n      0.9628  -0.3107                  -0.0572                      0.0633\ns.e.  0.0973   0.0983                   0.0161                      0.0265\n      intercept\n       580.9391\ns.e.     0.5124\n\nsigma^2 estimated as 0.4594:  log likelihood=-98.86\nAIC=209.73   AICc=210.65   BIC=225.24\n\nfit |&gt;\n  forecast(h = 30) |&gt;\n  autoplot(huron) + labs(y = \"feet\")\n\n\n\n\n\n\n\n\nIt seems unlikely that there was an increasing trend from 1973 to 2002, but the prediction intervals are very wide so they probably capture the actual values. Historical data on the level of Lake Huron can be obtained from the NOAA.\n\n\nfpp3 10.7, Ex 7\n\nFor the retail time series considered in earlier chapters:\n\nDevelop an appropriate dynamic regression model with Fourier terms for the seasonality. Use the AIC to select the number of Fourier terms to include in the model. (You will probably need to use the same Box-Cox transformation you identified previously.)\n\n\n\nset.seed(12345678)\nmyseries &lt;- aus_retail |&gt;\n  filter(\n    `Series ID` == sample(aus_retail$`Series ID`, 1),\n    Month &lt; yearmonth(\"2018 Jan\")\n  )\n\nmyseries |&gt; features(Turnover, guerrero)\n\n# A tibble: 1 × 3\n  State              Industry                                    lambda_guerrero\n  &lt;chr&gt;              &lt;chr&gt;                                                 &lt;dbl&gt;\n1 Northern Territory Clothing, footwear and personal accessory …          0.0776\n\nmyseries |&gt; autoplot(log(Turnover))\n\n\n\n\n\n\n\nfit &lt;- myseries |&gt;\n  model(\n    `K=1` = ARIMA(log(Turnover) ~ trend() + fourier(K = 1) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    `K=2` = ARIMA(log(Turnover) ~ trend() + fourier(K = 2) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    `K=3` = ARIMA(log(Turnover) ~ trend() + fourier(K = 3) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    `K=4` = ARIMA(log(Turnover) ~ trend() + fourier(K = 4) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    `K=5` = ARIMA(log(Turnover) ~ trend() + fourier(K = 5) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    `K=6` = ARIMA(log(Turnover) ~ trend() + fourier(K = 6) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1))\n  )\nglance(fit)\n\n# A tibble: 6 × 10\n  State      Industry .model  sigma2 log_lik   AIC  AICc   BIC ar_roots ma_roots\n  &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;   &lt;list&gt;  \n1 Northern … Clothin… K=1    0.00664    383. -748. -748. -713. &lt;cpl&gt;    &lt;cpl&gt;   \n2 Northern … Clothin… K=2    0.00652    389. -756. -755. -713. &lt;cpl&gt;    &lt;cpl&gt;   \n3 Northern … Clothin… K=3    0.00626    400. -774. -773. -723. &lt;cpl&gt;    &lt;cpl&gt;   \n4 Northern … Clothin… K=4    0.00596    411. -792. -791. -734. &lt;cpl&gt;    &lt;cpl&gt;   \n5 Northern … Clothin… K=5    0.00480    453. -873. -872. -811. &lt;cpl&gt;    &lt;cpl&gt;   \n6 Northern … Clothin… K=6    0.00437    470. -906. -905. -841. &lt;cpl&gt;    &lt;cpl&gt;   \n\n\nIncluding 6 harmonics minimises the AICc (and AIC/BIC) for this series.\n\nfit &lt;- transmute(fit, best = `K=6`)\n\nreport(fit)\n\nSeries: Turnover \nModel: LM w/ ARIMA(1,0,1)(1,0,0)[12] errors \nTransformation: log(Turnover) \n\nCoefficients:\n         ar1      ma1    sar1  trend()  fourier(K = 6)C1_12\n      0.9632  -0.3755  0.1761   0.0041              -0.0809\ns.e.  0.0165   0.0502  0.0542   0.0006               0.0080\n      fourier(K = 6)S1_12  fourier(K = 6)C2_12  fourier(K = 6)S2_12\n                  -0.1258               0.0381              -0.0882\ns.e.               0.0080               0.0052               0.0052\n      fourier(K = 6)C3_12  fourier(K = 6)S3_12  fourier(K = 6)C4_12\n                  -0.0206              -0.0815              -0.0294\ns.e.               0.0045               0.0045               0.0042\n      fourier(K = 6)S4_12  fourier(K = 6)C5_12  fourier(K = 6)S5_12\n                  -0.0538              -0.0554              -0.0540\ns.e.               0.0042               0.0041               0.0041\n      fourier(K = 6)C6_12  intercept\n                  -0.0230     1.3317\ns.e.               0.0029     0.1231\n\nsigma^2 estimated as 0.004368:  log likelihood=470.23\nAIC=-906.46   AICc=-904.65   BIC=-840.54\n\n\nThe chosen model is a linear trend (will be exponential after back-transforming) and fourier terms with 5 harmonics. The error model is ARIMA(1,0,1)(1,0,1).\n\n\nCheck the residuals of the fitted model. Does the residual series look like white noise?\n\n\n\ngg_tsresiduals(fit)\n\n\n\n\n\n\n\n\nThe residuals look well behaved.\n\n\nCompare the forecasts with those you obtained earlier using alternative models.\n\n\n\nfit &lt;- myseries |&gt;\n  model(\n    dynamic = ARIMA(log(Turnover) ~ trend() + fourier(K = 6) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    arima = ARIMA(log(Turnover)),\n    ets = ETS(Turnover)\n  )\nfit |&gt;\n  forecast() |&gt;\n  autoplot(filter(myseries, year(Month) &gt; 2010), level = 80, alpha = 0.5)"
  },
  {
    "objectID": "exams/index.html",
    "href": "exams/index.html",
    "title": "Exam preparation",
    "section": "",
    "text": "Past exams\n\n2021\n2022\n2023\n2024\n\nThis year’s exam will follow the same format.\n\n\nExam conditions\nThe final assessment will be an on-campus eExam. It will be closed book with the following permitted items:\n\nUp to 5 blank pages for use as working sheets.\n1 A4 double-sided page containing notes. A physical page only is allowed. It may be typed or hand-written.\n4 pre-printed answer sheets (available in the exam room) for handwritten responses (e.g., equations or diagrams).\nA physical calculator of any type or one of the following virtual calculators:\n\nInbuilt Mac/Windows calculator\nWebsite https://www.educalc.net/2336211.page\n10bii Financial Calculator for Mac by K2 Cashflow, https://apps.apple.com/au/app/10bii-financial-calculator/id473144920\n\n\nAI tools are not to be used in the exam.\nThe exam will include one page of ETS formulas. Here they are, so you know exactly what you will see on the exam. No other formulas will be provided, but you can write whatever you like on your single A4 page of notes.\n\n\n\nETS formulas"
  },
  {
    "objectID": "assignments/Project.html",
    "href": "assignments/Project.html",
    "title": "Retail Project",
    "section": "",
    "text": "Objective: To forecast a real time series using ETS and ARIMA models.\nData: The data are monthly measures of retail trade volume in Australia, obtained from the ABS. Each student will be use a different time series, selected using their student ID number as follows. This is the same series that you used in previous assignments.\nlibrary(fpp3)\nget_my_data &lt;- function(student_id) {\n  set.seed(student_id)\n  all_data &lt;- readr::read_rds(\"https://bit.ly/monashretaildata\")\n  while(TRUE) {\n    retail &lt;- filter(all_data, `Series ID` == sample(`Series ID`, 1))\n    if(!any(is.na(fill_gaps(retail)$Turnover))) return(retail)\n  }\n}\n# Replace the argument with your student ID\nretail &lt;- get_my_data(12345678)\nAssignment value: This assignment is worth 12% of the overall unit assessment. You may copy and paste material from previous assignments, but you must take into account any feedback that you have received on these assignments.\nReport:\nYou should produce forecasts of the series using ETS and ARIMA models. Write a report of your analysis in Quarto format explaining carefully what you have done and why you have done it. You may use this file as a starting point.Your report should include the following elements.\n\nProduce some plots of your series, and describe what you learn from each plot. [2 marks]\nDiscuss the statistical features of the data, including the effect of COVID-19 on your series. [2 marks]\nFind an appropriate Box-Cox transformation for your data and explain why you have chosen the particular transformation parameter \\lambda. [2 marks]\nProduce a plot of an STL decomposition of the transformed data. What do you learn from the plot? [2 marks]\nWhat differencing would be required to make the data stationary? You should use a unit-root test as part of the discussion. [2 marks]\nDescribe the methodology used to create a short-list of appropriate ARIMA models and ETS models. Include discussion of AIC values as well as results from applying the models to a test-set consisting of the last 24 months of the data provided. [6 marks]\nChoose one ARIMA model and one ETS model based on this analysis and show parameter estimates, residual diagnostics, forecasts and prediction intervals for both models. Diagnostic checking for both models should include ACF graphs and the Ljung-Box test. [8 marks]\nCompare the results obtained from each of your preferred models. Which method do you think gives the better forecasts? Explain with reference to the test-set. [2 marks]\nApply your two chosen models to the full data set, re-estimating the parameters but not changing the model structure. Produce out-of-sample point forecasts and 80% prediction intervals for each model for two years past the end of the data provided. [4 marks]\nObtain up-to-date data from the ABS website (Table 11). You may need to use the previous release of data, rather than the latest release. Compare your forecasts with the actual numbers. How well did you do? [4 marks]\nDiscuss the benefits and limitations of the models for your data. [3 marks]\nEnsure graphs are properly labelled, including appropriate units of measurement. [3 marks]\n\nETC5550 students\n\nSuppose forecasts of your series were required every year based on the most recent data available at the time. Describe the steps you would undertake each year to produce these forecasts. [5 marks]\n\nNotes\n\nYour submission must include the Quarto file (.qmd), and should run without error.\nThere will be a 5 marks penalty if file does not run without error.\nWhen using the updated ABS data set, do not edit the downloaded file in any way.\nThere is no need to provide the updated ABS data with your submission.\n\n\n\nDue: 30 May 2025  Submit"
  },
  {
    "objectID": "assignments/A2.html",
    "href": "assignments/A2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "This assignment will use the same data that you will use in the retail project later in the semester. Each student will use a different time series, selected using their student ID number as follows.\nlibrary(fpp3)\nget_my_data &lt;- function(student_id) {\n  set.seed(student_id)\n  all_data &lt;- readr::read_rds(\"https://bit.ly/monashretaildata\")\n  while(TRUE) {\n    retail &lt;- filter(all_data, `Series ID` == sample(`Series ID`, 1))\n    if(!any(is.na(fill_gaps(retail)$Turnover))) return(retail)\n  }\n}\n# Replace the argument with your student ID\nretail &lt;- get_my_data(12345678)\n\nUsing a test set of 2019–2022, fit an ETS model chosen automatically, and three benchmark methods to the training data. Which gives the best forecasts on the test set, based on RMSE?\nCheck the residuals from the best model using an ACF plot and a Ljung-Box test. Do the residuals appear to be white noise?\nNow use time-series cross-validation with a minimum sample size of 15 years, a step size of 1 year, and a forecast horizon of 5 years. Calculate the RMSE of the results. Does it change the conclusion you reach based on the test set?\nWhich of these two methods of evaluating accuracy is more reliable? Why?\n\nSubmit a Quarto (qmd) file which carries out the above analysis. You need to submit one file which implements all steps above. You may use this file as a starting point.\nTo receive full marks, the qmd file must compile without errors.\n\n\nDue: 25 April 2025  Submit"
  },
  {
    "objectID": "assignments/A1.html",
    "href": "assignments/A1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "This assignment will use the same data that you will use in the retail project later in the semester. Each student will use a different time series, selected using their student ID number as follows.\nlibrary(fpp3)\nget_my_data &lt;- function(student_id) {\n  set.seed(student_id)\n  all_data &lt;- readr::read_rds(\"https://bit.ly/monashretaildata\")\n  while(TRUE) {\n    retail &lt;- filter(all_data, `Series ID` == sample(`Series ID`, 1))\n    if(!any(is.na(fill_gaps(retail)$Turnover))) return(retail)\n  }\n}\n# Replace the argument with your student ID\nretail &lt;- get_my_data(12345678)\n\nPlot your time series using the autoplot() command. What do you learn from the plot? [1 mark]\nPlot your time series using the gg_season() command. What do you learn from the plot? [1 mark]\nPlot your time series using the gg_subseries() command. What do you learn from the plot? [1 mark]\nFind an appropriate Box-Cox transformation for your data and explain why you have chosen the particular transformation parameter \\lambda. [1.5 marks]\nProduce a plot of an STL decomposition of the transformed data. What do you learn from the plot? [1.5 marks]\n\nFor all plots, please use appropriate axis labels and titles.\nYou need to submit one Quarto (qmd) file which implements all steps above. You may use this file as a starting point.\nTo receive full marks, the qmd file must compile without errors.\n\n\nDue: 28 March 2025  Submit"
  },
  {
    "objectID": "assignments/A3.html",
    "href": "assignments/A3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "This assignment will use the same data that you will use in the retail project later in the semester. Each student will use a different time series, selected using their student ID number as follows.\nlibrary(fpp3)\nget_my_data &lt;- function(student_id) {\n  set.seed(student_id)\n  all_data &lt;- readr::read_rds(\"https://bit.ly/monashretaildata\")\n  while(TRUE) {\n    retail &lt;- filter(all_data, `Series ID` == sample(`Series ID`, 1))\n    if(!any(is.na(fill_gaps(retail)$Turnover))) return(retail)\n  }\n}\n# Replace the argument with your student ID\nretail &lt;- get_my_data(12345678)\nUse a training set up to and including 2018.\n\nWhat transformations (Box-Cox and/or differencing) would be required to make the data stationary? You should use a unit-root test as part of the discussion.\nUse a plot of the ACF and PACF of the (possibly differenced) data to determine two plausible ARIMA models for this data set.\nFit both models, along with an automatically chosen model, and produce forecasts for 2019–2022.\nWhich model is best based on AIC? Which model is best based on the test set RMSE? Which do you think is best to use for future forecasts? Why?\nCheck the residuals from your preferred model, using an ACF plot and a Ljung-Box test. Do the residuals appear to be white noise?\n\nSubmit a Quarto (qmd) file which carries out the above analysis. You need to submit one file which implements all steps above. You may use this file as a starting point.\n\n\nDue: 16 May 2025  Submit"
  },
  {
    "objectID": "assignments/competition.html",
    "href": "assignments/competition.html",
    "title": "Forecasting competition",
    "section": "",
    "text": "You must provide forecasts for the following items:\nFor each of these, give a point forecast and an 80% prediction interval, and explain in a couple of sentences how each was obtained.\nDue: 7 March 2025  Submit"
  },
  {
    "objectID": "assignments/competition.html#leaderboard",
    "href": "assignments/competition.html#leaderboard",
    "title": "Forecasting competition",
    "section": "Leaderboard",
    "text": "Leaderboard"
  },
  {
    "objectID": "assignments/competition.html#forecasts",
    "href": "assignments/competition.html#forecasts",
    "title": "Forecasting competition",
    "section": "Forecasts",
    "text": "Forecasts\n\n\n\n\n\n\nQ1\n\n\nGoogle closing stock price on 24 March 2025\n\n\n\n\n\nQ2\n\n\nMaximum temperature at Melbourne airport on 14 April 2025\n\n\n\n\n\nQ3\n\n\nDifference in points in AFL Anzac Day clash\n\n\n\n\n\nQ4\n\n\nSeasonally adjusted total employment for April 2025\n\n\n\n\n\nQ5\n\n\nGoogle closing stock price on 26 May 2025"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ETC3550/5550 Applied forecasting",
    "section": "",
    "text": "Rob J Hyndman\nEmail: Rob.Hyndman@monash.edu\n\n\n\n\n\n\n\n\n\nMitchell O’Hara-Wild\nEmail: Mitch.OHara-Wild@monash.edu\n\n\n\n\n\n\n\n\nMaliny Po\n\n\nNuwani Palihawadana\n\n\nXiefei (Sapphire) Li"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "ETC3550/5550 Applied forecasting",
    "section": "",
    "text": "Rob J Hyndman\nEmail: Rob.Hyndman@monash.edu\n\n\n\n\n\n\n\n\n\nMitchell O’Hara-Wild\nEmail: Mitch.OHara-Wild@monash.edu\n\n\n\n\n\n\n\n\nMaliny Po\n\n\nNuwani Palihawadana\n\n\nXiefei (Sapphire) Li"
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "ETC3550/5550 Applied forecasting",
    "section": "Weekly schedule",
    "text": "Weekly schedule\n\nPre-recorded videos: approximately 1 hour per week [Slides]\nTutorials: 1 hour per week\nOnline lecture: 12noon Mondays\nWorkshop: 1pm Tuesdays, Lecture Theatre S3, 16 Rainforest Walk.\nRecordings\n\n\n\n\nWeek\nTopic\nChapter\nAssignments\nQuizzes\n\n\n\n\n03 Mar\nIntroduction to forecasting and R\n1. Getting started\nForecasting Competition\n\n\n\n10 Mar\nTime series graphics\n2. Time series graphics\n\nWeek 2\n\n\n17 Mar\nTime series decomposition\n3. Time series decomposition\n\nWeek 3\n\n\n24 Mar\nSimple forecasting methods\n5. The forecaster’s toolbox\nAssignment 1\nWeek 4\n\n\n31 Mar\nAccuracy evaluation\n5. The forecaster’s toolbox\n\nWeek 5\n\n\n07 Apr\nExponential smoothing\n8. Exponential smoothing\n\nWeek 6\n\n\n14 Apr\nExponential smoothing\n8. Exponential smoothing\n\nWeek 7\n\n\n21 Apr\nMid-semester break\n\nAssignment 2\n\n\n\n28 Apr\nARIMA models\n9. ARIMA models\n\nWeek 8\n\n\n05 May\nARIMA models\n9. ARIMA models\n\nWeek 9\n\n\n12 May\nARIMA models\n9. ARIMA models\nAssignment 3\nWeek 10\n\n\n19 May\nMultiple regression and forecasting\n7. Time series regression models\n\nWeek 11\n\n\n26 May\nDynamic regression\n10. Dynamic regression models\nRetail Project"
  },
  {
    "objectID": "index.html#assessments",
    "href": "index.html#assessments",
    "title": "ETC3550/5550 Applied forecasting",
    "section": "Assessments",
    "text": "Assessments\n\nForecasting competition: 2%\nWeekly quizzes: 8%\nAssignment 1: 6%\nAssignment 2: 6%\nAssignment 3: 6%\nRetail project: 12%\nFinal exam: 60%"
  },
  {
    "objectID": "index.html#r-package-installation",
    "href": "index.html#r-package-installation",
    "title": "ETC3550/5550 Applied forecasting",
    "section": "R package installation",
    "text": "R package installation\nHere is the code to install the R packages we will be using in this unit.\ninstall.packages(c(\"tidyverse\",\"fpp3\", \"GGally\"), dependencies = TRUE)"
  },
  {
    "objectID": "week1/activities.html",
    "href": "week1/activities.html",
    "title": "Activities: Week 1",
    "section": "",
    "text": "The pedestrian dataset contains hourly pedestrian counts from 2015-01-01 to 2016-12-31 at 4 sensors in the city of Melbourne.\nThe data is shown below:\n\n\n# A tibble: 66,037 × 5\n   Sensor         Date_Time           Date        Time Count\n   &lt;chr&gt;          &lt;dttm&gt;              &lt;date&gt;     &lt;int&gt; &lt;int&gt;\n 1 Birrarung Marr 2015-01-01 00:00:00 2015-01-01     0  1630\n 2 Birrarung Marr 2015-01-01 01:00:00 2015-01-01     1   826\n 3 Birrarung Marr 2015-01-01 02:00:00 2015-01-01     2   567\n 4 Birrarung Marr 2015-01-01 03:00:00 2015-01-01     3   264\n 5 Birrarung Marr 2015-01-01 04:00:00 2015-01-01     4   139\n 6 Birrarung Marr 2015-01-01 05:00:00 2015-01-01     5    77\n 7 Birrarung Marr 2015-01-01 06:00:00 2015-01-01     6    44\n 8 Birrarung Marr 2015-01-01 07:00:00 2015-01-01     7    56\n 9 Birrarung Marr 2015-01-01 08:00:00 2015-01-01     8   113\n10 Birrarung Marr 2015-01-01 09:00:00 2015-01-01     9   166\n# ℹ 66,027 more rows\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify the index variable, key variable(s), and measured variable(s) of this dataset.\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThe index variable contains the complete time information\nThe key variable(s) identify each time series\nThe measured variable(s) are what you want to explore/forecast.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe aus_accommodation dataset contains quarterly data on Australian tourist accommodation from short-term non-residential accommodation with 15 or more rooms, 1998 Q1 - 2016 Q2. The first few lines are shown below.\n\nThe units of the measured variables are as follows:\n\nTakings are in millions of Australian dollars\nOccupancy is a percentage of rooms occupied\nCPI is an index with value 100 in 2012 Q1.\n\n\n\n\n\n\n\nYour turn!\n\n\n\nComplete the code to convert this dataset into a tsibble.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBegin by loading the fpp3 library to use its time series functions.\nlibrary(fpp3)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter loading the fpp3 package, convert the data frame into a tsibble.\nlibrary(fpp3)\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt; mutate(Date = as.Date(Date))\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember to specify the time index and key for as_tsibble() to function correctly.\nlibrary(fpp3)\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Date = as.Date(Date)) |&gt;\n  as_tsibble(key = State, index = Date)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal granularity\n\n\n\nThe previous exercise produced a dataset with daily frequency - although clearly the data is quarterly! This is because we are using a daily granularity which is inappropriate for this data.\n\n\nCommon temporal granularities can be created with these functions:\n\n\n\n\n\nGranularity\nFunction\n\n\n\n\nAnnual\nas.integer()\n\n\nQuarterly\nyearquarter()\n\n\nMonthly\nyearmonth()\n\n\nWeekly\nyearweek()\n\n\nDaily\nas_date(), ymd()\n\n\nSub-daily\nas_datetime()\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUse the appropriate granularity for the aus_accommodation dataset, and verify that the frequency is now quarterly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nStart by reading the CSV file and transform the data using mutate() and yearquarter() for the Date column.\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Quarter = yearquarter(Date))\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter transforming the Date column, make sure you convert the data frame to a tsibble.\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Quarter = yearquarter(Date)) |&gt;\n  as_tsibble(key = State, index = Quarter)\n\n\n\n\n\n\n\nThe tourism dataset contains the quarterly overnight trips from 1998 Q1 to 2016 Q4 across Australia.\nIt is disaggregated by 3 key variables:\n\nState: States and territories of Australia\nRegion: The tourism regions are formed through the aggregation of Statistical Local Areas (SLAs) which are defined by the various State and Territory tourism authorities according to their research and marketing needs\nPurpose: Stopover purpose of visit: “Holiday”, “Visiting friends and relatives”, “Business”, “Other reason”.\n\nBelow is a preview:\n\n\n# A tsibble: 24,320 x 5 [1Q]\n# Key:       Region, State, Purpose [304]\n   Quarter Region   State           Purpose  Trips\n     &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 1998 Q1 Adelaide South Australia Business  135.\n 2 1998 Q2 Adelaide South Australia Business  110.\n 3 1998 Q3 Adelaide South Australia Business  166.\n 4 1998 Q4 Adelaide South Australia Business  127.\n 5 1999 Q1 Adelaide South Australia Business  137.\n 6 1999 Q2 Adelaide South Australia Business  200.\n 7 1999 Q3 Adelaide South Australia Business  169.\n 8 1999 Q4 Adelaide South Australia Business  134.\n 9 2000 Q1 Adelaide South Australia Business  154.\n10 2000 Q2 Adelaide South Australia Business  169.\n# ℹ 24,310 more rows\n\n\nCalculate the total quarterly tourists visiting Victoria from the tourism dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo start off, filter the tourism dataset for only Victoria.\ntourism |&gt;\n  filter(State == \"Victoria\")\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter filtering, summarise the total trips for Victoria.\ntourism |&gt;\n  filter(State == \"Victoria\") |&gt;\n  summarise(Trips = sum(Trips))\n\n\n\n\n\n\n\nFind what combination of Region and Purpose had the maximum number of overnight trips on average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nStart by using as_tibble() to convert tourism back to a tibble and group it by Region and Purpose.\ntourism |&gt;\n  as_tibble() |&gt;\n  group_by(Region, Purpose)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter grouping, summarise the mean number of trips and filter for maximum trips.\ntourism |&gt;\n  as_tibble() |&gt;\n  group_by(Region, Purpose) |&gt;\n  summarise(Trips = mean(Trips), .groups = \"drop\") |&gt;\n  filter(Trips == max(Trips))\n\n\n\n\n\n\n\nCreate a new tsibble which combines the Purposes and Regions, and just has total trips by State.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo summarise the number of trips by each State, start by grouping the data by State.\ntourism |&gt;\n  group_by(State)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter grouping, use the summarise() function to sum the trips.\ntourism |&gt;\n  group_by(State) |&gt;\n  summarise(Trips = sum(Trips))"
  },
  {
    "objectID": "week1/activities.html#exercise-1",
    "href": "week1/activities.html#exercise-1",
    "title": "Activities: Week 1",
    "section": "",
    "text": "The pedestrian dataset contains hourly pedestrian counts from 2015-01-01 to 2016-12-31 at 4 sensors in the city of Melbourne.\nThe data is shown below:\n\n\n# A tibble: 66,037 × 5\n   Sensor         Date_Time           Date        Time Count\n   &lt;chr&gt;          &lt;dttm&gt;              &lt;date&gt;     &lt;int&gt; &lt;int&gt;\n 1 Birrarung Marr 2015-01-01 00:00:00 2015-01-01     0  1630\n 2 Birrarung Marr 2015-01-01 01:00:00 2015-01-01     1   826\n 3 Birrarung Marr 2015-01-01 02:00:00 2015-01-01     2   567\n 4 Birrarung Marr 2015-01-01 03:00:00 2015-01-01     3   264\n 5 Birrarung Marr 2015-01-01 04:00:00 2015-01-01     4   139\n 6 Birrarung Marr 2015-01-01 05:00:00 2015-01-01     5    77\n 7 Birrarung Marr 2015-01-01 06:00:00 2015-01-01     6    44\n 8 Birrarung Marr 2015-01-01 07:00:00 2015-01-01     7    56\n 9 Birrarung Marr 2015-01-01 08:00:00 2015-01-01     8   113\n10 Birrarung Marr 2015-01-01 09:00:00 2015-01-01     9   166\n# ℹ 66,027 more rows\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify the index variable, key variable(s), and measured variable(s) of this dataset.\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThe index variable contains the complete time information\nThe key variable(s) identify each time series\nThe measured variable(s) are what you want to explore/forecast."
  },
  {
    "objectID": "week1/activities.html#exercise-2",
    "href": "week1/activities.html#exercise-2",
    "title": "Activities: Week 1",
    "section": "",
    "text": "The aus_accommodation dataset contains quarterly data on Australian tourist accommodation from short-term non-residential accommodation with 15 or more rooms, 1998 Q1 - 2016 Q2. The first few lines are shown below.\n\nThe units of the measured variables are as follows:\n\nTakings are in millions of Australian dollars\nOccupancy is a percentage of rooms occupied\nCPI is an index with value 100 in 2012 Q1.\n\n\n\n\n\n\n\nYour turn!\n\n\n\nComplete the code to convert this dataset into a tsibble.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBegin by loading the fpp3 library to use its time series functions.\nlibrary(fpp3)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter loading the fpp3 package, convert the data frame into a tsibble.\nlibrary(fpp3)\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt; mutate(Date = as.Date(Date))\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember to specify the time index and key for as_tsibble() to function correctly.\nlibrary(fpp3)\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Date = as.Date(Date)) |&gt;\n  as_tsibble(key = State, index = Date)"
  },
  {
    "objectID": "week1/activities.html#exercise-3",
    "href": "week1/activities.html#exercise-3",
    "title": "Activities: Week 1",
    "section": "",
    "text": "Temporal granularity\n\n\n\nThe previous exercise produced a dataset with daily frequency - although clearly the data is quarterly! This is because we are using a daily granularity which is inappropriate for this data.\n\n\nCommon temporal granularities can be created with these functions:\n\n\n\n\n\nGranularity\nFunction\n\n\n\n\nAnnual\nas.integer()\n\n\nQuarterly\nyearquarter()\n\n\nMonthly\nyearmonth()\n\n\nWeekly\nyearweek()\n\n\nDaily\nas_date(), ymd()\n\n\nSub-daily\nas_datetime()\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUse the appropriate granularity for the aus_accommodation dataset, and verify that the frequency is now quarterly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nStart by reading the CSV file and transform the data using mutate() and yearquarter() for the Date column.\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Quarter = yearquarter(Date))\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter transforming the Date column, make sure you convert the data frame to a tsibble.\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Quarter = yearquarter(Date)) |&gt;\n  as_tsibble(key = State, index = Quarter)"
  },
  {
    "objectID": "week1/activities.html#exercise-4",
    "href": "week1/activities.html#exercise-4",
    "title": "Activities: Week 1",
    "section": "",
    "text": "The tourism dataset contains the quarterly overnight trips from 1998 Q1 to 2016 Q4 across Australia.\nIt is disaggregated by 3 key variables:\n\nState: States and territories of Australia\nRegion: The tourism regions are formed through the aggregation of Statistical Local Areas (SLAs) which are defined by the various State and Territory tourism authorities according to their research and marketing needs\nPurpose: Stopover purpose of visit: “Holiday”, “Visiting friends and relatives”, “Business”, “Other reason”.\n\nBelow is a preview:\n\n\n# A tsibble: 24,320 x 5 [1Q]\n# Key:       Region, State, Purpose [304]\n   Quarter Region   State           Purpose  Trips\n     &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 1998 Q1 Adelaide South Australia Business  135.\n 2 1998 Q2 Adelaide South Australia Business  110.\n 3 1998 Q3 Adelaide South Australia Business  166.\n 4 1998 Q4 Adelaide South Australia Business  127.\n 5 1999 Q1 Adelaide South Australia Business  137.\n 6 1999 Q2 Adelaide South Australia Business  200.\n 7 1999 Q3 Adelaide South Australia Business  169.\n 8 1999 Q4 Adelaide South Australia Business  134.\n 9 2000 Q1 Adelaide South Australia Business  154.\n10 2000 Q2 Adelaide South Australia Business  169.\n# ℹ 24,310 more rows\n\n\nCalculate the total quarterly tourists visiting Victoria from the tourism dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo start off, filter the tourism dataset for only Victoria.\ntourism |&gt;\n  filter(State == \"Victoria\")\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter filtering, summarise the total trips for Victoria.\ntourism |&gt;\n  filter(State == \"Victoria\") |&gt;\n  summarise(Trips = sum(Trips))"
  },
  {
    "objectID": "week1/activities.html#exercise-5",
    "href": "week1/activities.html#exercise-5",
    "title": "Activities: Week 1",
    "section": "",
    "text": "Find what combination of Region and Purpose had the maximum number of overnight trips on average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nStart by using as_tibble() to convert tourism back to a tibble and group it by Region and Purpose.\ntourism |&gt;\n  as_tibble() |&gt;\n  group_by(Region, Purpose)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter grouping, summarise the mean number of trips and filter for maximum trips.\ntourism |&gt;\n  as_tibble() |&gt;\n  group_by(Region, Purpose) |&gt;\n  summarise(Trips = mean(Trips), .groups = \"drop\") |&gt;\n  filter(Trips == max(Trips))"
  },
  {
    "objectID": "week1/activities.html#exercise-6",
    "href": "week1/activities.html#exercise-6",
    "title": "Activities: Week 1",
    "section": "",
    "text": "Create a new tsibble which combines the Purposes and Regions, and just has total trips by State.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo summarise the number of trips by each State, start by grouping the data by State.\ntourism |&gt;\n  group_by(State)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter grouping, use the summarise() function to sum the trips.\ntourism |&gt;\n  group_by(State) |&gt;\n  summarise(Trips = sum(Trips))"
  },
  {
    "objectID": "week2/activities.html",
    "href": "week2/activities.html",
    "title": "Activities: Week 2",
    "section": "",
    "text": "gg_season() with period argument\npivot_longer()\nfacet_grid()\nGGally::ggpairs()\ngg_lag()\nACF()\n\nR examples"
  },
  {
    "objectID": "week2/activities.html#more-r-functions",
    "href": "week2/activities.html#more-r-functions",
    "title": "Activities: Week 2",
    "section": "",
    "text": "gg_season() with period argument\npivot_longer()\nfacet_grid()\nGGally::ggpairs()\ngg_lag()\nACF()\n\nR examples"
  },
  {
    "objectID": "week2/activities.html#exercise-1-cyclic-data",
    "href": "week2/activities.html#exercise-1-cyclic-data",
    "title": "Activities: Week 2",
    "section": "Exercise 1: cyclic data",
    "text": "Exercise 1: cyclic data\nA famous data set containing cycles is the Canadian lynx data, contained in pelts.\n\n\n\n\n\n\n\n\nHow far apart are the peaks and troughs on average?\nProduce a lag plot for the Lynx series.\n\n\n\n\n\n\n\n\nWhich lags have the strongest correlation? Why?\nProduce an ACF plot for the Lynx series.\n\n\n\n\n\n\n\n\n\nWhy does the ACF peak around lag 10?\nWhy can’t this data be seasonal?"
  },
  {
    "objectID": "week2/activities.html#exercise-2-seasonal-and-cyclic-data",
    "href": "week2/activities.html#exercise-2-seasonal-and-cyclic-data",
    "title": "Activities: Week 2",
    "section": "Exercise 2: seasonal and cyclic data",
    "text": "Exercise 2: seasonal and cyclic data\nSometime trend, seasonality and cyclicity can occur together, such as in the Bricks production data.\n\n\n\n\n\n\n\n\n\nCan you see the seasonality in the time plot? How about the ACF plot?\nCan you see the cyclicity in the time plot? How about the ACF plot?"
  },
  {
    "objectID": "week2/activities.html#exercise-3-acf-plots",
    "href": "week2/activities.html#exercise-3-acf-plots",
    "title": "Activities: Week 2",
    "section": "Exercise 3: ACF plots",
    "text": "Exercise 3: ACF plots\nWhich time plot corresponds to which ACF plot?\n\n\n\n\n\n\n\n\n\n\n\n\nPlot 1.\n\n\n\n\n\n\n\n\n\n\n\nPlot 2.\n\n\n\n\n\n\n\n\n\n\n\nPlot 3.\n\n\n\n\n\n\n\n\n\n\n\nPlot 4."
  },
  {
    "objectID": "week2/activities.html#exercise-4-white-noise",
    "href": "week2/activities.html#exercise-4-white-noise",
    "title": "Activities: Week 2",
    "section": "Exercise 4: white noise",
    "text": "Exercise 4: white noise\nWhite noise data consists of purely random draws from the same distribution with mean zero and constant variance. y_t = \\varepsilon_t, \\qquad \\text{where $\\varepsilon_t \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(0, \\sigma^2)$}\nWhite noise data can be simulated using the rnorm() function. By setting the seed at the start of the code, we ensure the same random numbers are generated each time it is run. Change the seed to get different random numbers.\n\n\n\n\n\n\n\n\nCan you find a seed value that gives data which appear NOT to be white noise?"
  },
  {
    "objectID": "week2/activities.html#exercise-5-random-walks",
    "href": "week2/activities.html#exercise-5-random-walks",
    "title": "Activities: Week 2",
    "section": "Exercise 5: random walks",
    "text": "Exercise 5: random walks\nRandom walks are a type of time series where the value at time t is equal to the previous value plus a random amount from a white noise process. y_t = y_{t-1} + \\varepsilon_t, \\qquad \\text{where $\\varepsilon_t \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(0, \\sigma^2)$} Equivalently, we can take the cumulative sum of a white noise process. y_t = y_{0} + \\sum_{t=1}^{T} \\varepsilon_t, \\qquad \\text{where $\\varepsilon_t \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(0, \\sigma^2)$}\n\n\n\n\n\n\n\n\nExperiment by re-running the code with different seed values. Try to find a random walk which appears to have a strong positive trend, and another with a strong negative trend. Remember, it can’t really have a trend because it is simply being generated from summing random numbers which have zero mean."
  },
  {
    "objectID": "week2/activities.html#weekly-quiz",
    "href": "week2/activities.html#weekly-quiz",
    "title": "Activities: Week 2",
    "section": "Weekly quiz",
    "text": "Weekly quiz\nGo to week 2 quiz"
  },
  {
    "objectID": "week2/index.html",
    "href": "week2/index.html",
    "title": "Week 2: Time series graphics",
    "section": "",
    "text": "Different types of plots for time series including time plots, season plots, subseries plots, lag plots and ACF plots\nThe difference between seasonal patterns and cyclic patterns in time series\nWhat is “white noise” and how to identify it."
  },
  {
    "objectID": "week2/index.html#what-you-will-learn-this-week",
    "href": "week2/index.html#what-you-will-learn-this-week",
    "title": "Week 2: Time series graphics",
    "section": "",
    "text": "Different types of plots for time series including time plots, season plots, subseries plots, lag plots and ACF plots\nThe difference between seasonal patterns and cyclic patterns in time series\nWhat is “white noise” and how to identify it."
  },
  {
    "objectID": "week2/index.html#pre-class-activities",
    "href": "week2/index.html#pre-class-activities",
    "title": "Week 2: Time series graphics",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 2 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week2/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week2/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 2: Time series graphics",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\n\nComplete Exercises 1-11 from Section 2.10 of the book.\nSolutions to Exercises\nRecorded tutorial code\nCheck your understanding quiz"
  },
  {
    "objectID": "week2/index.html#monday-lecture",
    "href": "week2/index.html#monday-lecture",
    "title": "Week 2: Time series graphics",
    "section": "Monday lecture",
    "text": "Monday lecture\n\n\nDownload slides\n\nCode used in lecture"
  },
  {
    "objectID": "week2/index.html#tuesday-workshop",
    "href": "week2/index.html#tuesday-workshop",
    "title": "Week 2: Time series graphics",
    "section": "Tuesday workshop",
    "text": "Tuesday workshop\nActivities for Tuesday workshop\n\nCode used in workshop\n\n\n\n\n\n## Assignments\n\n* [Assignment 1](../assignments/A1.qmd) is due on Friday 28 March.\n\n\n## Weekly quiz\n\n* [Week 2 quiz](https://learning.monash.edu/mod/quiz/view.php?id=3868353) is due on Sunday 16 March."
  },
  {
    "objectID": "week3/activities.html",
    "href": "week3/activities.html",
    "title": "Activities: Week 3",
    "section": "",
    "text": "The first Melbourne COVID-19 lockdown was from 31 March 2020 to 12 May 2020. We are interested in whether more babies than usual were born 9 months later — in January or February 2021.\nFirst extract the data for Victoria from the aus_births dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis shows the number of births in Victoria each month from January 1975 to December 2021.\nNow produce a time plot and a season plot of the data, to better understand the trend and seasonal patterns.\n\n\n\n\n\n\n\n\nThe see-saw seasonal pattern is due to month length variation.\nSo let’s remove it by taking the daily average of births in the month.\n\n\n\n\n\n\n\n\nNow repeat the time and season plot, but using the daily average variable, rather than the monthly total.\n\n\n\n\n\n\n\n\n\nHas the peak month for having babies changed compared to the previous gg_season plot? Why?\nWhy has the unusual fluctuation after January 2020 apparently increased in size compared to the previous gg_season plot?\n\nLet’s do an STL decomposition. Experiment with the value of window and robust to see how the seasonal component changes over time. We need robust  = TRUE here, so the unusual behaviour near the end of the series does not have a strong effect on the trend or seasonal components\n\n\n\n\n\n\n\n\nWith your preferred values of window and robust, plot the remainder component\n\n\n\n\n\n\n\n\nOnce we remove the trend and seasonal component, and just look at the remainder, we can see any effects that are not simply seasonality or trend. Here we are plotting the remainder for the last year.\nHow many extra births per month than normal were there in February 2021?"
  },
  {
    "objectID": "week3/activities.html#was-there-a-covid-baby-boom-in-victoria",
    "href": "week3/activities.html#was-there-a-covid-baby-boom-in-victoria",
    "title": "Activities: Week 3",
    "section": "",
    "text": "The first Melbourne COVID-19 lockdown was from 31 March 2020 to 12 May 2020. We are interested in whether more babies than usual were born 9 months later — in January or February 2021.\nFirst extract the data for Victoria from the aus_births dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis shows the number of births in Victoria each month from January 1975 to December 2021.\nNow produce a time plot and a season plot of the data, to better understand the trend and seasonal patterns.\n\n\n\n\n\n\n\n\nThe see-saw seasonal pattern is due to month length variation.\nSo let’s remove it by taking the daily average of births in the month.\n\n\n\n\n\n\n\n\nNow repeat the time and season plot, but using the daily average variable, rather than the monthly total.\n\n\n\n\n\n\n\n\n\nHas the peak month for having babies changed compared to the previous gg_season plot? Why?\nWhy has the unusual fluctuation after January 2020 apparently increased in size compared to the previous gg_season plot?\n\nLet’s do an STL decomposition. Experiment with the value of window and robust to see how the seasonal component changes over time. We need robust  = TRUE here, so the unusual behaviour near the end of the series does not have a strong effect on the trend or seasonal components\n\n\n\n\n\n\n\n\nWith your preferred values of window and robust, plot the remainder component\n\n\n\n\n\n\n\n\nOnce we remove the trend and seasonal component, and just look at the remainder, we can see any effects that are not simply seasonality or trend. Here we are plotting the remainder for the last year.\nHow many extra births per month than normal were there in February 2021?"
  },
  {
    "objectID": "week3/activities.html#canadian-gas-production",
    "href": "week3/activities.html#canadian-gas-production",
    "title": "Activities: Week 3",
    "section": "Canadian gas production",
    "text": "Canadian gas production\nThis exercise uses the canadian_gas data (monthly Canadian gas production in billions of cubic metres, January 1960 – February 2005).\nTry to find a Box-Cox transformation to stabilise the variance. Why isn’t it possible?\n\n\n\n\n\n\n\n\nWithout using a transformation, do a time plot, season plot and subseries plot to look at the effect of the changing seasonality over time.\n\n\n\n\n\n\n\n\nDo an STL decomposition of the data. You will need to choose a seasonal window to allow for the changing shape of the seasonal component.\n\n\n\n\n\n\n\n\nHow does the seasonal shape change over time?\n\n\n\n\n\n\n\n\nCan you produce a plausible seasonally adjusted series?"
  },
  {
    "objectID": "week3/activities.html#canberra-public-transport-usage",
    "href": "week3/activities.html#canberra-public-transport-usage",
    "title": "Activities: Week 3",
    "section": "Canberra public transport usage",
    "text": "Canberra public transport usage\n\nDo Exam 2024, Question B2."
  },
  {
    "objectID": "week3/index.html",
    "href": "week3/index.html",
    "title": "Week 3: Time series decomposition",
    "section": "",
    "text": "Transforming data to remove some sources of variation\nDecomposing a time series into trend-cycle, seasonal and remainder components\nSeasonal adjustment"
  },
  {
    "objectID": "week3/index.html#what-you-will-learn-this-week",
    "href": "week3/index.html#what-you-will-learn-this-week",
    "title": "Week 3: Time series decomposition",
    "section": "",
    "text": "Transforming data to remove some sources of variation\nDecomposing a time series into trend-cycle, seasonal and remainder components\nSeasonal adjustment"
  },
  {
    "objectID": "week3/index.html#pre-class-activities",
    "href": "week3/index.html#pre-class-activities",
    "title": "Week 3: Time series decomposition",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 3 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week3/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week3/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 3: Time series decomposition",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\n\nComplete Exercises 1-5, 9-10 from Section 3.7 of the book.\nSolutions to Exercises\nRecorded tutorial code"
  },
  {
    "objectID": "week3/index.html#monday-lecture",
    "href": "week3/index.html#monday-lecture",
    "title": "Week 3: Time series decomposition",
    "section": "Monday lecture",
    "text": "Monday lecture\n\n\nDownload slides\nCode used in lecture"
  },
  {
    "objectID": "week3/index.html#tuesday-workshop",
    "href": "week3/index.html#tuesday-workshop",
    "title": "Week 3: Time series decomposition",
    "section": "Tuesday workshop",
    "text": "Tuesday workshop\nActivities for Tuesday workshop"
  },
  {
    "objectID": "week3/index.html#assignments",
    "href": "week3/index.html#assignments",
    "title": "Week 3: Time series decomposition",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Friday 28 March."
  },
  {
    "objectID": "week3/index.html#weekly-quiz",
    "href": "week3/index.html#weekly-quiz",
    "title": "Week 3: Time series decomposition",
    "section": "Weekly quiz",
    "text": "Weekly quiz\n\nWeek 3 quiz is due on Sunday 23 March."
  },
  {
    "objectID": "week4/activities.html",
    "href": "week4/activities.html",
    "title": "Activities: Week 4",
    "section": "",
    "text": "We will use the Bricks data from aus_production (Australian quarterly clay brick production 1956–-2005).\n\nUse an STL decomposition to calculate the trend-cycle and seasonal indices. (Experiment with the seasonal window.)\nCompute and plot the seasonally adjusted data.\nUse a naïve method to produce forecasts of the seasonally adjusted data.\nUse decomposition_model() to reseasonalise the results, giving forecasts for the original data.\nRepeat with a robust STL decomposition. Does it make much difference?"
  },
  {
    "objectID": "week4/activities.html#australian-brick-production",
    "href": "week4/activities.html#australian-brick-production",
    "title": "Activities: Week 4",
    "section": "",
    "text": "We will use the Bricks data from aus_production (Australian quarterly clay brick production 1956–-2005).\n\nUse an STL decomposition to calculate the trend-cycle and seasonal indices. (Experiment with the seasonal window.)\nCompute and plot the seasonally adjusted data.\nUse a naïve method to produce forecasts of the seasonally adjusted data.\nUse decomposition_model() to reseasonalise the results, giving forecasts for the original data.\nRepeat with a robust STL decomposition. Does it make much difference?"
  },
  {
    "objectID": "week4/activities.html#afghanistan-population",
    "href": "week4/activities.html#afghanistan-population",
    "title": "Activities: Week 4",
    "section": "Afghanistan population",
    "text": "Afghanistan population\nThe annual population of Afghanistan is available in the global_economy data set.\n\nPlot the data and comment on its features. Can you observe the effect of the Soviet-Afghan war?\nFit a linear trend model and compare this to a piecewise linear trend model with knots at 1980 and 1989.\nGenerate forecasts from these two models for the five years after the end of the data. Which looks better? Why?"
  },
  {
    "objectID": "week4/activities.html#olympic-running-times",
    "href": "week4/activities.html#olympic-running-times",
    "title": "Activities: Week 4",
    "section": "Olympic running times",
    "text": "Olympic running times\nData set olympic_running contains the winning times (in seconds) in each Olympic Games sprint, middle-distance and long-distance track events from 1896 to 2016.\n\nPlot the winning time against the year for each event. Describe the main features of the plot.\nFit a regression line to the data for each event. Obviously the winning times have been decreasing, but at what average rate per year?\nPredict the winning time for each race in the 2028 Olympics. Do they look reasonable?\nAdjust your model to make the trends piecewise linear. Do the forecasts look more reasonable?"
  },
  {
    "objectID": "week4/index.html",
    "href": "week4/index.html",
    "title": "Week 4: Simple forecasting methods",
    "section": "",
    "text": "Forecasting workflow\nFour benchmark forecasting methods that we will use for comparison\nLinear trends, dummy seasonality\nForecasting with transformations\nForecasting with decompositions"
  },
  {
    "objectID": "week4/index.html#what-you-will-learn-this-week",
    "href": "week4/index.html#what-you-will-learn-this-week",
    "title": "Week 4: Simple forecasting methods",
    "section": "",
    "text": "Forecasting workflow\nFour benchmark forecasting methods that we will use for comparison\nLinear trends, dummy seasonality\nForecasting with transformations\nForecasting with decompositions"
  },
  {
    "objectID": "week4/index.html#pre-class-activities",
    "href": "week4/index.html#pre-class-activities",
    "title": "Week 4: Simple forecasting methods",
    "section": "Pre-class activities",
    "text": "Pre-class activities\n\nRead Chapter 5 of the textbook, Sections 5.1 – 5.3, 5.6 – 5.7, and watch all embedded videos\nRead Chapter 7.4 of the textbook, and watch the embedded video"
  },
  {
    "objectID": "week4/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week4/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 4: Simple forecasting methods",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\n\nComplete Exercises 1-5, 11 from Section 5.11 of the book.\nComplete Exercises 2 and 6 from Section 7.10 of the book\nSolutions to Exercises\nRecorded tutorial code"
  },
  {
    "objectID": "week4/index.html#monday-lecture",
    "href": "week4/index.html#monday-lecture",
    "title": "Week 4: Simple forecasting methods",
    "section": "Monday lecture",
    "text": "Monday lecture\n\n\nDownload slides\nCode used in lecture"
  },
  {
    "objectID": "week4/index.html#tuesday-workshop",
    "href": "week4/index.html#tuesday-workshop",
    "title": "Week 4: Simple forecasting methods",
    "section": "Tuesday workshop",
    "text": "Tuesday workshop\nActivities for Tuesday workshop\nCode used in workshop"
  },
  {
    "objectID": "week4/index.html#assignments",
    "href": "week4/index.html#assignments",
    "title": "Week 4: Simple forecasting methods",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Friday 28 March."
  },
  {
    "objectID": "week4/index.html#weekly-quiz",
    "href": "week4/index.html#weekly-quiz",
    "title": "Week 4: Simple forecasting methods",
    "section": "Weekly quiz",
    "text": "Weekly quiz\n\nWeek 4 quiz is due on Sunday 30 March."
  },
  {
    "objectID": "week5/activities.html",
    "href": "week5/activities.html",
    "title": "Activities: Week 5",
    "section": "",
    "text": "Participate in these workshop questions here: https://partici.fi/61758592"
  },
  {
    "objectID": "week5/activities.html#household-budget-forecasting",
    "href": "week5/activities.html#household-budget-forecasting",
    "title": "Activities: Week 5",
    "section": "Household budget forecasting",
    "text": "Household budget forecasting\n\nCreate a training set for household wealth (hh_budget) by withholding the last four years as a test set.\nFit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.\nCompute the accuracy of your forecasts. Which method does best?\nDo the residuals from the best method resemble white noise?"
  },
  {
    "objectID": "week5/activities.html#true-or-false",
    "href": "week5/activities.html#true-or-false",
    "title": "Activities: Week 5",
    "section": "True or false?",
    "text": "True or false?\n\nGood forecast methods should have normally distributed residuals.\nA model with small residuals will give good forecasts.\nThe best measure of forecast accuracy is MAPE.\nIf your model doesn’t forecast well, you should make it more complicated.\nAlways choose the model with the best forecast accuracy as measured on the test set."
  },
  {
    "objectID": "week5/activities.html#exam-part-a",
    "href": "week5/activities.html#exam-part-a",
    "title": "Activities: Week 5",
    "section": "2024 Exam part A",
    "text": "2024 Exam part A\nDiscuss topics A2, A3, and A6"
  },
  {
    "objectID": "week5/index.html",
    "href": "week5/index.html",
    "title": "Week 5: Accuracy evaluation",
    "section": "",
    "text": "Fitted values, residuals\nDistributional forecasts\nForecast evaluation"
  },
  {
    "objectID": "week5/index.html#what-you-will-learn-this-week",
    "href": "week5/index.html#what-you-will-learn-this-week",
    "title": "Week 5: Accuracy evaluation",
    "section": "",
    "text": "Fitted values, residuals\nDistributional forecasts\nForecast evaluation"
  },
  {
    "objectID": "week5/index.html#pre-class-activities",
    "href": "week5/index.html#pre-class-activities",
    "title": "Week 5: Accuracy evaluation",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 5 of the textbook, Sections 5.4 – 5.5, 5.8 – 5.10, and watch all embedded videos"
  },
  {
    "objectID": "week5/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week5/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 5: Accuracy evaluation",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\n\nComplete Exercises 8-10, 12 from Section 5.11 of the book.\nSolutions to Exercises\nRecorded tutorial code"
  },
  {
    "objectID": "week5/index.html#monday-lecture",
    "href": "week5/index.html#monday-lecture",
    "title": "Week 5: Accuracy evaluation",
    "section": "Monday lecture",
    "text": "Monday lecture\n\n\nDownload slides\nCode used in lecture"
  },
  {
    "objectID": "week5/index.html#tuesday-workshop",
    "href": "week5/index.html#tuesday-workshop",
    "title": "Week 5: Accuracy evaluation",
    "section": "Tuesday workshop",
    "text": "Tuesday workshop\nActivities for Tuesday workshop\nCode used in workshop"
  },
  {
    "objectID": "week5/index.html#assignments",
    "href": "week5/index.html#assignments",
    "title": "Week 5: Accuracy evaluation",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Friday 25 April."
  },
  {
    "objectID": "week5/index.html#weekly-quiz",
    "href": "week5/index.html#weekly-quiz",
    "title": "Week 5: Accuracy evaluation",
    "section": "Weekly quiz",
    "text": "Weekly quiz\n\nWeek 5 quiz is due on Sunday 06 April."
  },
  {
    "objectID": "week6/activities.html",
    "href": "week6/activities.html",
    "title": "Activities: Week 6",
    "section": "",
    "text": "We will forecast the Chinese GDP from the global_economy data set using an ETS model. The following code provides a starting point.\n\nlibrary(fpp3)\nchinese_gdp &lt;- global_economy |&gt;\n  filter(Country == \"China\") |&gt;\n  mutate(GDP_pc = GDP / Population)\nchinese_gdp |&gt; autoplot(GDP_pc)\nfit &lt;- chinese_gdp |&gt;\n  model(\n    ets = ETS(GDP_pc ~ error(\"A\") + trend(\"A\", alpha = 0.3, beta = 0.3))\n  )\nreport(fit)\nfc &lt;- fit |&gt; forecast(h = 10)\nfc |&gt; autoplot(chinese_gdp)\n\nExperiment with the various options in the ETS() function to see how much the forecasts change with the parameters \\alpha and \\beta, with a damped trend, and with a Box-Cox transformation. Try to develop an intuition of what each is doing to the forecasts.\nWhat happens when:\n\n\\alpha=0?\n\\alpha=1?\n\\beta=0?\n\\beta &gt; \\alpha?\nThe trend is set to \"N\" (None)\nThe trend is set to \"Ad\" (Additive damped)\nThe error is set to \"M\" (Multiplicative)\nA strong transformation such as a logarithm (Box-Cox with \\lambda=0 is used)?\n\\alpha and \\beta are omitted?\nThe trend() term is omitted?\nEverything from ~ on is omitted?\nWhat combination of options gives you the narrowest prediction intervals? Why?\nWhat combination of options gives you the widest prediction intervals? Why?\nWhat combination of options do you think gives you the best forecasts?"
  },
  {
    "objectID": "week6/activities.html#chinese-gdp-per-capita-forecasts",
    "href": "week6/activities.html#chinese-gdp-per-capita-forecasts",
    "title": "Activities: Week 6",
    "section": "",
    "text": "We will forecast the Chinese GDP from the global_economy data set using an ETS model. The following code provides a starting point.\n\nlibrary(fpp3)\nchinese_gdp &lt;- global_economy |&gt;\n  filter(Country == \"China\") |&gt;\n  mutate(GDP_pc = GDP / Population)\nchinese_gdp |&gt; autoplot(GDP_pc)\nfit &lt;- chinese_gdp |&gt;\n  model(\n    ets = ETS(GDP_pc ~ error(\"A\") + trend(\"A\", alpha = 0.3, beta = 0.3))\n  )\nreport(fit)\nfc &lt;- fit |&gt; forecast(h = 10)\nfc |&gt; autoplot(chinese_gdp)\n\nExperiment with the various options in the ETS() function to see how much the forecasts change with the parameters \\alpha and \\beta, with a damped trend, and with a Box-Cox transformation. Try to develop an intuition of what each is doing to the forecasts.\nWhat happens when:\n\n\\alpha=0?\n\\alpha=1?\n\\beta=0?\n\\beta &gt; \\alpha?\nThe trend is set to \"N\" (None)\nThe trend is set to \"Ad\" (Additive damped)\nThe error is set to \"M\" (Multiplicative)\nA strong transformation such as a logarithm (Box-Cox with \\lambda=0 is used)?\n\\alpha and \\beta are omitted?\nThe trend() term is omitted?\nEverything from ~ on is omitted?\nWhat combination of options gives you the narrowest prediction intervals? Why?\nWhat combination of options gives you the widest prediction intervals? Why?\nWhat combination of options do you think gives you the best forecasts?"
  },
  {
    "objectID": "week6/activities.html#prediction-interval-calculation",
    "href": "week6/activities.html#prediction-interval-calculation",
    "title": "Activities: Week 6",
    "section": "Prediction interval calculation",
    "text": "Prediction interval calculation\nFor the no trend model, with \\alpha = 0.5, find the 95% prediction intervals for the next five years. Use the hilo() function to calculate them from the fc object.\nShow that these are equal to \\ell_T \\pm 1.96 \\hat\\sigma \\sqrt{1+ (h-1)/4} where \\hat\\sigma^2 is the estimated residual variance.\n\nThe components() function can give you the value of \\ell_T.\nThe glance() function can give you the value of \\sigma^2."
  },
  {
    "objectID": "week6/index.html",
    "href": "week6/index.html",
    "title": "Week 6: ETS models (part 1)",
    "section": "",
    "text": "Simple exponential smoothing\nExponential smoothing methods with trend\nExponential smoothing methods with seasonality\nETS model notation"
  },
  {
    "objectID": "week6/index.html#what-you-will-learn-this-week",
    "href": "week6/index.html#what-you-will-learn-this-week",
    "title": "Week 6: ETS models (part 1)",
    "section": "",
    "text": "Simple exponential smoothing\nExponential smoothing methods with trend\nExponential smoothing methods with seasonality\nETS model notation"
  },
  {
    "objectID": "week6/index.html#pre-class-activities",
    "href": "week6/index.html#pre-class-activities",
    "title": "Week 6: ETS models (part 1)",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Sections 8.1-8.2 and 8.5 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week6/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week6/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 6: ETS models (part 1)",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\n\nComplete Exercises 1-6, 16, 17 from Section 8.8 of the book.\nSolutions to Exercises\nRecorded tutorial code"
  },
  {
    "objectID": "week6/index.html#monday-lecture",
    "href": "week6/index.html#monday-lecture",
    "title": "Week 6: ETS models (part 1)",
    "section": "Monday lecture",
    "text": "Monday lecture\n\n\nDownload slides\nCode used in lecture"
  },
  {
    "objectID": "week6/index.html#tuesday-workshop",
    "href": "week6/index.html#tuesday-workshop",
    "title": "Week 6: ETS models (part 1)",
    "section": "Tuesday workshop",
    "text": "Tuesday workshop\nActivities for Tuesday workshop\nCode used in workshop"
  },
  {
    "objectID": "week6/index.html#assignments",
    "href": "week6/index.html#assignments",
    "title": "Week 6: ETS models (part 1)",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Friday 25 April."
  },
  {
    "objectID": "week6/index.html#weekly-quiz",
    "href": "week6/index.html#weekly-quiz",
    "title": "Week 6: ETS models (part 1)",
    "section": "Weekly quiz",
    "text": "Weekly quiz\n\nWeek 6 quiz is due on Sunday 13 April."
  },
  {
    "objectID": "week7/activities.html",
    "href": "week7/activities.html",
    "title": "Activities: Week 7",
    "section": "",
    "text": "This week we will forecast total domestic overnight trips across Australia from the tourism dataset.\n\nlibrary(fpp3)\naus_trips &lt;- tourism |&gt;\n  summarise(Trips = sum(Trips))\n\n\nPlot the data and describe the main features of the series.\nUse ETS() to choose a seasonal model for the data. What do the parameters tell you about the series?\nForecast the next two years using your ETS model.\nDecompose the series using STL and obtain the seasonally adjusted data.\nForecast the next two years of the series using an ETS(A,A,N) model applied to the seasonally adjusted data (specified using decomposition_model().)\nWhich of the two models gives the best forecasts? Which gives the best fit to the training data?\nNow set up a test set of 3 years and fit both models to the training set, along with at least one appropriate benchmark.\nCompute the forecast accuracy of the three models. Which is best?\nCheck the residuals of your preferred model using gg_tsresiduals() and Ljung-Box test."
  },
  {
    "objectID": "week9/index.html",
    "href": "week9/index.html",
    "title": "Week 9: ARIMA models",
    "section": "",
    "text": "AR, MA, ARMA and ARIMA models\nSelecting model orders manually and automatically"
  },
  {
    "objectID": "week9/index.html#what-you-will-learn-this-week",
    "href": "week9/index.html#what-you-will-learn-this-week",
    "title": "Week 9: ARIMA models",
    "section": "",
    "text": "AR, MA, ARMA and ARIMA models\nSelecting model orders manually and automatically"
  },
  {
    "objectID": "week9/index.html#pre-class-activities",
    "href": "week9/index.html#pre-class-activities",
    "title": "Week 9: ARIMA models",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Sections 9.3-9.8 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week9/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week9/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 9: ARIMA models",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 6-10 from Section 9.11 of the book."
  }
]