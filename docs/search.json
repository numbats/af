[
  {
    "objectID": "week8/activities.html",
    "href": "week8/activities.html",
    "title": "ETC3550/ETC5550 Applied forecasting",
    "section": "",
    "text": "Find an ARIMA model for the pelt::Lynx data"
  },
  {
    "objectID": "week7/index.html",
    "href": "week7/index.html",
    "title": "Week 7: ARIMA models",
    "section": "",
    "text": "Stationarity and differencing\nUnit root tests"
  },
  {
    "objectID": "week7/index.html#what-you-will-learn-this-week",
    "href": "week7/index.html#what-you-will-learn-this-week",
    "title": "Week 7: ARIMA models",
    "section": "",
    "text": "Stationarity and differencing\nUnit root tests"
  },
  {
    "objectID": "week7/index.html#pre-class-activities",
    "href": "week7/index.html#pre-class-activities",
    "title": "Week 7: ARIMA models",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Sections 9.1-9.4 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week7/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week7/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 7: ARIMA models",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 5,8-15 from Section 8.8 of the book."
  },
  {
    "objectID": "week6/index.html",
    "href": "week6/index.html",
    "title": "Week 6: Exponential smoothing",
    "section": "",
    "text": "Exponential smoothing methods with trend and seasonality\nETS models\nAutomatic model selection using the AICc"
  },
  {
    "objectID": "week6/index.html#what-you-will-learn-this-week",
    "href": "week6/index.html#what-you-will-learn-this-week",
    "title": "Week 6: Exponential smoothing",
    "section": "",
    "text": "Exponential smoothing methods with trend and seasonality\nETS models\nAutomatic model selection using the AICc"
  },
  {
    "objectID": "week6/index.html#pre-class-activities",
    "href": "week6/index.html#pre-class-activities",
    "title": "Week 6: Exponential smoothing",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 8 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week6/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week6/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 6: Exponential smoothing",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 1-4, 16, 17 from Section 8.8 of the book."
  },
  {
    "objectID": "week5/index.html",
    "href": "week5/index.html",
    "title": "Week 5: Exponential smoothing",
    "section": "",
    "text": "Simple exponential smoothing\nCorresponding ETS models"
  },
  {
    "objectID": "week5/index.html#what-you-will-learn-this-week",
    "href": "week5/index.html#what-you-will-learn-this-week",
    "title": "Week 5: Exponential smoothing",
    "section": "",
    "text": "Simple exponential smoothing\nCorresponding ETS models"
  },
  {
    "objectID": "week5/index.html#pre-class-activities",
    "href": "week5/index.html#pre-class-activities",
    "title": "Week 5: Exponential smoothing",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Sections 8.1-8.4 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week5/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week5/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 5: Exponential smoothing",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 1-6, 8, 11-12 from Section 5.11 of the book."
  },
  {
    "objectID": "week4/index.html",
    "href": "week4/index.html",
    "title": "Week 4: The forecaster’s toolbox",
    "section": "",
    "text": "Four benchmark forecasting methods that we will use for comparison\nFitted values, residuals\nForecasting with transformations"
  },
  {
    "objectID": "week4/index.html#what-you-will-learn-this-week",
    "href": "week4/index.html#what-you-will-learn-this-week",
    "title": "Week 4: The forecaster’s toolbox",
    "section": "",
    "text": "Four benchmark forecasting methods that we will use for comparison\nFitted values, residuals\nForecasting with transformations"
  },
  {
    "objectID": "week4/index.html#pre-class-activities",
    "href": "week4/index.html#pre-class-activities",
    "title": "Week 4: The forecaster’s toolbox",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 5 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week4/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week4/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 4: The forecaster’s toolbox",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 1-5, 9-10 from Section 3.7 of the book."
  },
  {
    "objectID": "week3/index.html",
    "href": "week3/index.html",
    "title": "Week 3: Time series decomposition",
    "section": "",
    "text": "Transforming data to remove some sources of variation\nDecomposing a time series into trend-cycle, seasonal and remainder components\nSeasonal adjustment"
  },
  {
    "objectID": "week3/index.html#what-you-will-learn-this-week",
    "href": "week3/index.html#what-you-will-learn-this-week",
    "title": "Week 3: Time series decomposition",
    "section": "",
    "text": "Transforming data to remove some sources of variation\nDecomposing a time series into trend-cycle, seasonal and remainder components\nSeasonal adjustment"
  },
  {
    "objectID": "week3/index.html#pre-class-activities",
    "href": "week3/index.html#pre-class-activities",
    "title": "Week 3: Time series decomposition",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 3 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week3/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week3/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 3: Time series decomposition",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 6-11 from Section 2.10 of the book."
  },
  {
    "objectID": "week3/index.html#slides-for-monday-lecture",
    "href": "week3/index.html#slides-for-monday-lecture",
    "title": "Week 3: Time series decomposition",
    "section": "Slides for Monday lecture",
    "text": "Slides for Monday lecture\n\n\nDownload pdf"
  },
  {
    "objectID": "week3/index.html#assignments",
    "href": "week3/index.html#assignments",
    "title": "Week 3: Time series decomposition",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Friday 28 March."
  },
  {
    "objectID": "week3/index.html#weekly-quiz",
    "href": "week3/index.html#weekly-quiz",
    "title": "Week 3: Time series decomposition",
    "section": "Weekly quiz",
    "text": "Weekly quiz\n\nWeek 3 quiz is due on Sunday 23 March."
  },
  {
    "objectID": "week2/activities.html",
    "href": "week2/activities.html",
    "title": "Activities: Week 2",
    "section": "",
    "text": "gg_season() with period argument\npivot_longer()\nfacet_grid()\nGGally::ggpairs()\ngg_lag()\nACF()\n\nR examples"
  },
  {
    "objectID": "week2/activities.html#more-r-functions",
    "href": "week2/activities.html#more-r-functions",
    "title": "Activities: Week 2",
    "section": "",
    "text": "gg_season() with period argument\npivot_longer()\nfacet_grid()\nGGally::ggpairs()\ngg_lag()\nACF()\n\nR examples"
  },
  {
    "objectID": "week2/activities.html#exercise-1-cyclic-data",
    "href": "week2/activities.html#exercise-1-cyclic-data",
    "title": "Activities: Week 2",
    "section": "Exercise 1: cyclic data",
    "text": "Exercise 1: cyclic data\nA famous data set containing cycles is the Canadian lynx data, contained in pelts.\n\n\n\n\n\n\n\n\nHow far apart are the peaks and troughs on average?\nProduce a lag plot for the Lynx series.\n\n\n\n\n\n\n\n\nWhich lags have the strongest correlation? Why?\nProduce an ACF plot for the Lynx series.\n\n\n\n\n\n\n\n\n\nWhy does the ACF peak around lag 10?\nWhy can’t this data be seasonal?"
  },
  {
    "objectID": "week2/activities.html#exercise-2-seasonal-and-cyclic-data",
    "href": "week2/activities.html#exercise-2-seasonal-and-cyclic-data",
    "title": "Activities: Week 2",
    "section": "Exercise 2: seasonal and cyclic data",
    "text": "Exercise 2: seasonal and cyclic data\nSometime trend, seasonality and cyclicity can occur together, such as in the Bricks production data.\n\n\n\n\n\n\n\n\n\nCan you see the seasonality in the time plot? How about the ACF plot?\nCan you see the cyclicity in the time plot? How about the ACF plot?"
  },
  {
    "objectID": "week2/activities.html#exercise-3-acf-plots",
    "href": "week2/activities.html#exercise-3-acf-plots",
    "title": "Activities: Week 2",
    "section": "Exercise 3: ACF plots",
    "text": "Exercise 3: ACF plots\nWhich time plot corresponds to which ACF plot?\n\n\n\n\n\n\n\n\n\n\n\n\nPlot 1.\n\n\n\n\n\n\n\n\n\n\n\nPlot 2.\n\n\n\n\n\n\n\n\n\n\n\nPlot 3.\n\n\n\n\n\n\n\n\n\n\n\nPlot 4."
  },
  {
    "objectID": "week2/activities.html#exercise-4-white-noise",
    "href": "week2/activities.html#exercise-4-white-noise",
    "title": "Activities: Week 2",
    "section": "Exercise 4: white noise",
    "text": "Exercise 4: white noise\nWhite noise data consists of purely random draws from the same distribution with mean zero and constant variance. y_t = \\varepsilon_t, \\qquad \\text{where $\\varepsilon_t \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(0, \\sigma^2)$}\nWhite noise data can be simulated using the rnorm() function. By setting the seed at the start of the code, we ensure the same random numbers are generated each time it is run. Change the seed to get different random numbers.\n\n\n\n\n\n\n\n\nCan you find a seed value that gives data which appear NOT to be white noise?"
  },
  {
    "objectID": "week2/activities.html#exercise-5-random-walks",
    "href": "week2/activities.html#exercise-5-random-walks",
    "title": "Activities: Week 2",
    "section": "Exercise 5: random walks",
    "text": "Exercise 5: random walks\nRandom walks are a type of time series where the value at time t is equal to the previous value plus a random amount from a white noise process. y_t = y_{t-1} + \\varepsilon_t, \\qquad \\text{where $\\varepsilon_t \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(0, \\sigma^2)$} Equivalently, we can take the cumulative sum of a white noise process. y_t = y_{0} + \\sum_{t=1}^{T} \\varepsilon_t, \\qquad \\text{where $\\varepsilon_t \\mathop{\\sim}\\limits^{\\mathrm{iid}} N(0, \\sigma^2)$}\n\n\n\n\n\n\n\n\nExperiment by re-running the code with different seed values. Try to find a random walk which appears to have a strong positive trend, and another with a strong negative trend. Remember, it can’t really have a trend because it is simply being generated from summing random numbers which have zero mean."
  },
  {
    "objectID": "week2/activities.html#weekly-quiz",
    "href": "week2/activities.html#weekly-quiz",
    "title": "Activities: Week 2",
    "section": "Weekly quiz",
    "text": "Weekly quiz\nGo to week 2 quiz"
  },
  {
    "objectID": "week12/index.html",
    "href": "week12/index.html",
    "title": "Week 12: Review",
    "section": "",
    "text": "Review Assignment 1\nReview initial case studies\nDiscuss exam"
  },
  {
    "objectID": "week12/index.html#what-you-will-learn-this-week",
    "href": "week12/index.html#what-you-will-learn-this-week",
    "title": "Week 12: Review",
    "section": "",
    "text": "Review Assignment 1\nReview initial case studies\nDiscuss exam"
  },
  {
    "objectID": "week12/index.html#pre-class-activities",
    "href": "week12/index.html#pre-class-activities",
    "title": "Week 12: Review",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nCatch up on any exercises not yet done"
  },
  {
    "objectID": "week12/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week12/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 12: Review",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 2-6 from Section 10.7 of the book."
  },
  {
    "objectID": "week12/index.html#post-class-activities",
    "href": "week12/index.html#post-class-activities",
    "title": "Week 12: Review",
    "section": "Post-class activities",
    "text": "Post-class activities\n\nDo any exercises not yet finished.\nComplete past exams: [2022] [2023] [2024]\nRe-read the textbook\nListen again to all lectures"
  },
  {
    "objectID": "week11/index.html",
    "href": "week11/index.html",
    "title": "Week 11: Dynamic regression",
    "section": "",
    "text": "How to combine regression models with ARIMA models to form dynamic regression models\nDynamic harmonic regression to handle complex seasonality\nLagged predictors"
  },
  {
    "objectID": "week11/index.html#what-you-will-learn-this-week",
    "href": "week11/index.html#what-you-will-learn-this-week",
    "title": "Week 11: Dynamic regression",
    "section": "",
    "text": "How to combine regression models with ARIMA models to form dynamic regression models\nDynamic harmonic regression to handle complex seasonality\nLagged predictors"
  },
  {
    "objectID": "week11/index.html#pre-class-activities",
    "href": "week11/index.html#pre-class-activities",
    "title": "Week 11: Dynamic regression",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 10 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week11/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week11/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 11: Dynamic regression",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 1-7 from Section 7.10 of the book."
  },
  {
    "objectID": "week10/index.html",
    "href": "week10/index.html",
    "title": "Week 10: Multiple regression and forecasting",
    "section": "",
    "text": "Useful predictors for time series forecasting using regression\nSelecting predictors\nEx ante and ex post forecasting"
  },
  {
    "objectID": "week10/index.html#what-you-will-learn-this-week",
    "href": "week10/index.html#what-you-will-learn-this-week",
    "title": "Week 10: Multiple regression and forecasting",
    "section": "",
    "text": "Useful predictors for time series forecasting using regression\nSelecting predictors\nEx ante and ex post forecasting"
  },
  {
    "objectID": "week10/index.html#pre-class-activities",
    "href": "week10/index.html#pre-class-activities",
    "title": "Week 10: Multiple regression and forecasting",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 7 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week10/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week10/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 10: Multiple regression and forecasting",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 11-16 from Section 9.11 of the book."
  },
  {
    "objectID": "week1/index.html",
    "href": "week1/index.html",
    "title": "Week 1: What is forecasting?",
    "section": "",
    "text": "How to think about forecasting from a statistical perspective\nWhat makes something easy or hard to forecast?\nUsing the tsibble package in R"
  },
  {
    "objectID": "week1/index.html#what-you-will-learn-this-week",
    "href": "week1/index.html#what-you-will-learn-this-week",
    "title": "Week 1: What is forecasting?",
    "section": "",
    "text": "How to think about forecasting from a statistical perspective\nWhat makes something easy or hard to forecast?\nUsing the tsibble package in R"
  },
  {
    "objectID": "week1/index.html#pre-class-activities",
    "href": "week1/index.html#pre-class-activities",
    "title": "Week 1: What is forecasting?",
    "section": "Pre-class activities",
    "text": "Pre-class activities\n\nBefore we start classes, make sure that are familiar with R, RStudio and the tidyverse packages. If you’ve already done ETC1010, then you may not need to do anything! But if you’re new to R and the tidyverse, then you will need to get yourself up-to-speed. Work through the first five modules of the StartR tutorial at startr.numbat.space. Do as much of it as you think you need. For those students new to R, it is strongly recommended that you do all five modules. For those who have previously used R, concentrate on the parts where you feel you are weakest.\nInstall R and RStudio on your personal computer. Instructions are provided at OTexts.com/fpp3/appendix-using-r.html.\nRead Chapter 1 of the textbook and watch all embedded videos\nWatch this video"
  },
  {
    "objectID": "week1/index.html#slides-for-monday-lecture",
    "href": "week1/index.html#slides-for-monday-lecture",
    "title": "Week 1: What is forecasting?",
    "section": "Slides for Monday lecture",
    "text": "Slides for Monday lecture\n\n\nDownload pdf"
  },
  {
    "objectID": "week1/index.html#activities-for-tuesday-workshop",
    "href": "week1/index.html#activities-for-tuesday-workshop",
    "title": "Week 1: What is forecasting?",
    "section": "Activities for Tuesday workshop",
    "text": "Activities for Tuesday workshop"
  },
  {
    "objectID": "week1/index.html#check-your-understanding",
    "href": "week1/index.html#check-your-understanding",
    "title": "Week 1: What is forecasting?",
    "section": "Check your understanding",
    "text": "Check your understanding"
  },
  {
    "objectID": "week1/index.html#assignments",
    "href": "week1/index.html#assignments",
    "title": "Week 1: What is forecasting?",
    "section": "Assignments",
    "text": "Assignments\n\nForecasting Competition is due on Friday 07 March."
  },
  {
    "objectID": "other-ex-sol.html",
    "href": "other-ex-sol.html",
    "title": "Additional Exercise Solutions",
    "section": "",
    "text": "fpp3 1.8, Ex 1\n\nFor cases 3 and 4 in Section 1.5, list the possible predictor variables that might be useful, assuming that the relevant data are available.\n\nCase 3: the following predictor variables might be useful, assuming that the relevant data are available:\n\nModel and make of the vehicle\nOdometer reading\nConditions of the vehicle\nCompany the vehicle was leased to\nColor of the vehicle\nDate of sale\n\nCase 4: the following predictor variables might be useful, assuming that the relevant data are available:\n\nDay of the week\nDay of the year\nIs the day before long weekend\nIs the day in the end of long weekend\nIs the day before or in the beginning of school holidays (one variable per every state)\nIs the day in the end of school holidays (one variable per every state)\nIs the day before or in the beginning of a major sport event\nIs the day after of a major sport event\nCompetitors’ prices (relative to the price of the airline in question)\nIs there a pilot strike at some of the competitors’ airlines\nIs there a pilot strike at the airline in question\n\n\n\nfpp3 1.8, Ex 2\n\nFor case 3 in Section 1.5, describe the five steps of forecasting in the context of this project.\n\n\n1. Problem definition\n\nThe main stakeholders should be defined and everyone questioned about which way he or she can benefit from the new system. In case of the fleet company probably the group of specialists was not recognized as stakeholders which led to complications in gathering relevant information and later in finding an appropriate statistical approach and deployment of the new forecasting method.\n\n\n\n2. Gathering information\n\nData set of past sales should be obtained, including surrounding information such as the way data were gathered, possible outliers and incorrect records, special values in the data.\nExpertise knowledge should be obtained from people responsible for the sales such as seasonal price fluctuations, if there is dependency of the price on the situation in economy, also finding other possible factors which can influence the price.\n\n\n\n3. Preliminary (exploratory) analysis\n\nPossible outliers and inconsistent information should be found (for example very small, zero or even negative prices).\nGraphs which show dependency of the sale price on different predictor variables should be considered.\nDependency of the sale price on month of the year should be plot.\n\n\n\n4. Choosing and fitting models\n\nA model to start from (for example a linear model) and predictor variables which most likely affect the forecasts should be chosen. Predicting performance of the model should be evaluated.\nThe model should be changed (for example by transforming parameters, adding or removing predictor variables) and it’s performance evaluated. This should be done iteratively a few times until a satisfactory model is found.\n\n\n\n5. Using and evaluating a forecasting model\n\nThe appropriate software should be deployed to the company and relevant people should be educated how to use this software.\nForecasting accuracy should be checked against new sales. If necessary the model should be updated and then the deployed software.\n\n\n\n\nfpp3 3.7, Ex 6\n\nShow that a 3\\times 5 MA is equivalent to a 7-term weighted moving average with weights of 0.067, 0.133, 0.200, 0.200, 0.200, 0.133, and 0.067.\n\n5-term moving average: z_j = \\frac{1}{5}(y_{j-2}+y_{j-1}+y_j+y_{j+1}+y_{j+2}). 3-term moving average: u_t = \\frac{1}{3}(z_{t-1}+z_t+z_{t+1}). Substituting expression for z_j into the latter formula we get \\begin{align*}\n  u_t &= \\frac{1}{3}\\left(\\frac{1}{5}\\left(y_{t-3}+y_{t-2}+y_{t-1}+y_{t}+y_{t+1}\\right)+\\frac{1}{5}\\left(y_{t-2}+y_{t-1}+y_t+y_{t+1}+y_{t+2}\\right)+\\frac{1}{5}\\left(y_{t-1}+y_{t}+y_{t+1}+y_{t+2}+y_{t+3}\\right)\\right).\\\\\n  &= \\frac{1}{15}\\left(y_{t-3}+2y_{t-2}+3y_{t-1}+3y_{t}+3y_{t+1}+2y_{t+2}+y_{t+3}\\right),\n\\end{align*} which is a 7-term weighted moving average with weights of 0.067, 0.133, 0.200, 0.200, 0.200, 0.133, and 0.067\n\n\nfpp3 3.7, Ex 7\n\nConsider the last five years of the Gas data from aus_production.\n\n\ngas &lt;- tail(aus_production, 5*4) |&gt; select(Gas)\n\n\n\nPlot the time series. Can you identify seasonal fluctuations and/or a trend-cycle?\n\n\n\ngas &lt;- tail(aus_production, 5 * 4) |&gt; select(Gas)\ngas |&gt;\n  autoplot(Gas) + labs(y = \"Petajoules\")\n\n\n\n\n\n\n\n\nThere is some strong seasonality and a trend.\n\n\nUse classical_decomposition with type=multiplicative to calculate the trend-cycle and seasonal indices.\nDo the results support the graphical interpretation from part a?\n\n\n\ndecomp &lt;- gas |&gt;\n  model(decomp = classical_decomposition(Gas, type = \"multiplicative\")) |&gt;\n  components()\ndecomp |&gt; autoplot()\n\n\n\n\n\n\n\n\nThe decomposition has captured the seasonality and a slight trend.\n\n\nCompute and plot the seasonally adjusted data.\n\n\n\nas_tsibble(decomp) |&gt;\n  autoplot(season_adjust) +\n  labs(title = \"Seasonally adjusted data\", y = \"Petajoules\")\n\n\n\n\n\n\n\n\n\n\nChange one observation to be an outlier (e.g., add 300 to one observation), and recompute the seasonally adjusted data. What is the effect of the outlier?\nDoes it make any difference if the outlier is near the end rather than in the middle of the time series?\n\n\n\ngas |&gt;\n  mutate(Gas = if_else(Quarter == yearquarter(\"2007Q4\"), Gas + 300, Gas)) |&gt;\n  model(decomp = classical_decomposition(Gas, type = \"multiplicative\")) |&gt;\n  components() |&gt;\n  as_tsibble() |&gt;\n  autoplot(season_adjust) +\n  labs(title = \"Seasonally adjusted data\", y = \"Petajoules\")\n\n\n\n\n\n\n\n\n\nThe “seasonally adjusted” data now shows some seasonality because the outlier has affected the estimate of the seasonal component.\n\n\ngas |&gt;\n  mutate(Gas = if_else(Quarter == yearquarter(\"2010Q2\"), Gas + 300, Gas)) |&gt;\n  model(decomp = classical_decomposition(Gas, type = \"multiplicative\")) |&gt;\n  components() |&gt;\n  as_tsibble() |&gt;\n  autoplot(season_adjust) +\n  labs(title = \"Seasonally adjusted data\", y = \"Petajoules\")\n\n\n\n\n\n\n\n\nThe seasonally adjusted data now show no seasonality because the outlier is in the part of the data where the trend can’t be estimated.\n\n\nfpp3 3.7, Ex 8\n\nRecall your retail time series data (from Exercise 8 in Section 2.10). Decompose the series using X11. Does it reveal any outliers, or unusual features that you had not noticed previously?\n\n\nset.seed(12345678)\nmyseries &lt;- aus_retail |&gt;\n  filter(`Series ID` == sample(aus_retail$`Series ID`, 1))\ndecomp &lt;- myseries |&gt;\n  model(x11 = X_13ARIMA_SEATS(Turnover ~ x11())) |&gt;\n  components()\ndecomp |&gt; autoplot()\n\n\n\n\n\n\n\n\nTwo outliers are now evident in the “irregular” component — in December 1995 and July 2010.\n\n\nfpp3 5.10, Ex 7\n\nFor your retail time series (from Exercise 8 in Section 2.10):\n\nCreate a training dataset consisting of observations before 2011.\nCheck that your data have been split appropriately by producing the following plot.\nCalculate seasonal naïve forecasts using SNAIVE() applied to your training data (myseries_train).\nCheck the residuals. Do the residuals appear to be uncorrelated and normally distributed?\nProduce forecasts for the test data.\nCompare the accuracy of your forecasts against the actual values.\nHow sensitive are the accuracy measures to the amount of training data used?\n\n\n\nset.seed(12345678)\nmyseries &lt;- aus_retail |&gt;\n  filter(`Series ID` == sample(aus_retail$`Series ID`, 1))\nmyseries_train &lt;- myseries |&gt;\n  filter(year(Month) &lt; 2011)\nautoplot(myseries, Turnover) +\n  autolayer(myseries_train, Turnover, colour = \"red\")\n\n\n\n\n\n\n\n\nThe plot indicates that the training data has been extracted correctly.\n\nfit &lt;- myseries_train |&gt;\n  model(SNAIVE(Turnover))\n\n\nfit |&gt; gg_tsresiduals()\n\n\n\n\n\n\n\n\nThe residuals appear very auto-correlated as many lags exceed the significance threshold. This can also be seen in the residual plot, where there are periods of sustained high and low residuals. The distribution does not appear normally distributed, and is not centred around zero.\n\nfc &lt;- fit |&gt;\n  forecast(new_data = anti_join(myseries, myseries_train))\nfc |&gt; autoplot(myseries)\n\n\n\n\n\n\n\nbind_rows(\n  accuracy(fit),\n  accuracy(fc, myseries)\n) |&gt;\n  select(-State, -Industry, -.model)\n\n# A tibble: 2 × 9\n  .type       ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Training 0.439  1.21 0.915  5.23 12.4   1     1    0.768\n2 Test     0.836  1.55 1.24   5.94  9.06  1.36  1.28 0.601\n\n\nThe accuracy on the training data is substantially better than the out-of-sample forecast accuracy. This is common, and especially evident in this example as the model has failed to capture the trend in the data. This can be seen in the mean error, which is above zero as the model predictions do not account for the upward trend.\n\nmyseries_accuracy &lt;- function(data, last_training_year) {\n  myseries_train &lt;- data |&gt;\n    filter(year(Month) &lt;= last_training_year)\n  fit &lt;- myseries_train |&gt;\n    model(SNAIVE(Turnover))\n  fc &lt;- fit |&gt;\n    forecast(new_data = anti_join(myseries, myseries_train))\n  bind_rows(\n    accuracy(fit),\n    accuracy(fc, myseries)\n  ) |&gt;\n    mutate(last_training_year = last_training_year) |&gt;\n    select(last_training_year, .type, ME:ACF1)\n}\nas.list(2011:2017) |&gt;\n  purrr::map_dfr(myseries_accuracy, data = myseries) |&gt;\n  ggplot(aes(x = last_training_year, y = RMSE, group = .type)) +\n  geom_line(aes(col = .type))\n\n\n\n\n\n\n\n\nThe accuracy on the training data is almost unchanged when the size of the training set is increased. However, the accuracy on the test data decreases as we are averaging RMSE over the forecast horizon, and with less training data the forecasts horizons can be longer.\n\n\nfpp3 5.10, Ex 9\n\n\nCreate a training set for household wealth (hh_budget) by withholding the last four years as a test set.\n\n\n\ntrain &lt;- hh_budget |&gt;\n  filter(Year &lt;= max(Year) - 4)\n\n\n\nFit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.\n\n\n\nfit &lt;- train |&gt;\n  model(\n    naive = NAIVE(Wealth),\n    drift = RW(Wealth ~ drift()),\n    mean = MEAN(Wealth)\n  )\nfc &lt;- fit |&gt; forecast(h = 4)\n\n\n\nCompute the accuracy of your forecasts. Which method does best?\n\n\n\nfc |&gt;\n  accuracy(hh_budget) |&gt;\n  arrange(Country, MASE)\n\n# A tibble: 12 × 11\n   .model Country   .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n   &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 drift  Australia Test   29.1  35.5  29.1  7.23  7.23 1.73  1.48   0.210 \n 2 naive  Australia Test   34.7  41.5  34.7  8.64  8.64 2.06  1.73   0.216 \n 3 mean   Australia Test   35.7  42.3  35.7  8.89  8.89 2.12  1.76   0.216 \n 4 drift  Canada    Test   33.3  37.2  33.3  6.09  6.09 1.73  1.57  -0.229 \n 5 naive  Canada    Test   46.2  51.0  46.2  8.46  8.46 2.40  2.15  -0.0799\n 6 mean   Canada    Test   90.4  92.9  90.4 16.7  16.7  4.69  3.92  -0.0799\n 7 drift  Japan     Test   14.7  17.9  14.7  2.44  2.44 0.943 0.967 -0.229 \n 8 naive  Japan     Test   36.3  37.8  36.3  6.06  6.06 2.34  2.04  -0.534 \n 9 mean   Japan     Test  100.  101.  100.  16.8  16.8  6.45  5.46  -0.534 \n10 drift  USA       Test   75.9  76.2  75.9 12.7  12.7  2.88  2.43  -0.561 \n11 naive  USA       Test   82.1  82.5  82.1 13.8  13.8  3.12  2.63  -0.423 \n12 mean   USA       Test   82.9  83.3  82.9 13.9  13.9  3.15  2.65  -0.423 \n\nfc |&gt;\n  accuracy(hh_budget) |&gt;\n  group_by(.model) |&gt;\n  summarise(MASE = mean(MASE)) |&gt;\n  ungroup() |&gt;\n  arrange(MASE)\n\n# A tibble: 3 × 2\n  .model  MASE\n  &lt;chr&gt;  &lt;dbl&gt;\n1 drift   1.82\n2 naive   2.48\n3 mean    4.10\n\n\nThe drift method is better for every country, and on average.\n\n\nDo the residuals from the best method resemble white noise?\n\n\n\nfit |&gt;\n  filter(Country == \"Australia\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt;\n  filter(Country == \"Canada\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt;\n  filter(Country == \"Japan\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt;\n  filter(Country == \"USA\") |&gt;\n  select(drift) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\n\nIn all cases, the residuals look like white noise.\n\n\nfpp3 5.10, Ex 10\n\n\nCreate a training set for Australian takeaway food turnover (aus_retail) by withholding the last four years as a test set.\n\n\n\ntakeaway &lt;- aus_retail |&gt;\n  filter(Industry == \"Takeaway food services\") |&gt;\n  summarise(Turnover = sum(Turnover))\ntrain &lt;- takeaway |&gt;\n  filter(Month &lt;= max(Month) - 4 * 12)\n\n\n\nFit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.\n\n\n\nfit &lt;- train |&gt;\n  model(\n    naive = NAIVE(Turnover),\n    drift = RW(Turnover ~ drift()),\n    mean = MEAN(Turnover),\n    snaive = SNAIVE(Turnover)\n  )\nfc &lt;- fit |&gt; forecast(h = \"4 years\")\n\n\n\nCompute the accuracy of your forecasts. Which method does best?\n\n\n\nfc |&gt;\n  accuracy(takeaway) |&gt;\n  arrange(MASE)\n\n# A tibble: 4 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 naive  Test  -12.4  119.  96.4 -1.49  6.66  2.30  2.25 0.613\n2 drift  Test  -93.7  130. 108.  -6.82  7.67  2.58  2.46 0.403\n3 snaive Test  177.   192. 177.  11.7  11.7   4.22  3.64 0.902\n4 mean   Test  829.   838. 829.  55.7  55.7  19.8  15.8  0.613\n\n\nThe naive method is best here.\n\n\nDo the residuals from the best method resemble white noise?\n\n\n\nfit |&gt;\n  select(naive) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\n\nThis is far from white noise. There is strong seasonality and increasing variance that has not been accounted for by the naive model.\n\n\nfpp3 8.8, Ex6\n\nForecast the Chinese GDP from the global_economy data set using an ETS model. Experiment with the various options in the ETS() function to see how much the forecasts change with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each is doing to the forecasts.\n\n\n[Hint: use h=20 when forecasting, so you can clearly see the differences between the various options when plotting the forecasts.]\n\n\nchina &lt;- global_economy |&gt;\n  filter(Country == \"China\")\nchina |&gt; autoplot(GDP)\n\n\n\n\n\n\n\n\n\nIt clearly needs a relatively strong transformation due to the increasing variance.\n\n\nchina |&gt; autoplot(box_cox(GDP, 0.2))\n\n\n\n\n\n\n\nchina |&gt; features(GDP, guerrero)\n\n# A tibble: 1 × 2\n  Country lambda_guerrero\n  &lt;fct&gt;             &lt;dbl&gt;\n1 China           -0.0345\n\n\n\nMaking \\lambda=0.2 looks ok.\nThe Guerrero method suggests an even stronger transformation. Let’s also try a log.\n\n\nfit &lt;- china |&gt;\n  model(\n    ets = ETS(GDP),\n    ets_damped = ETS(GDP ~ trend(\"Ad\")),\n    ets_bc = ETS(box_cox(GDP, 0.2)),\n    ets_log = ETS(log(GDP))\n  )\n\nfit\n\n# A mable: 1 x 5\n# Key:     Country [1]\n  Country          ets    ets_damped       ets_bc      ets_log\n  &lt;fct&gt;        &lt;model&gt;       &lt;model&gt;      &lt;model&gt;      &lt;model&gt;\n1 China   &lt;ETS(M,A,N)&gt; &lt;ETS(M,Ad,N)&gt; &lt;ETS(A,A,N)&gt; &lt;ETS(A,A,N)&gt;\n\naugment(fit)\n\n# A tsibble: 232 x 7 [1Y]\n# Key:       Country, .model [4]\n   Country .model  Year          GDP      .fitted        .resid   .innov\n   &lt;fct&gt;   &lt;chr&gt;  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 China   ets     1960 59716467625. 49001691297.  10714776328.  0.219  \n 2 China   ets     1961 50056868958. 66346643194. -16289774236. -0.246  \n 3 China   ets     1962 47209359006. 51607368186.  -4398009180. -0.0852 \n 4 China   ets     1963 50706799903. 47386494407.   3320305495.  0.0701 \n 5 China   ets     1964 59708343489. 51919091574.   7789251914.  0.150  \n 6 China   ets     1965 70436266147. 63350421234.   7085844913.  0.112  \n 7 China   ets     1966 76720285970. 76289186599.    431099371.  0.00565\n 8 China   ets     1967 72881631327. 82708375812.  -9826744486. -0.119  \n 9 China   ets     1968 70846535056. 75804820984.  -4958285928. -0.0654 \n10 China   ets     1969 79705906247. 72222259470.   7483646777.  0.104  \n# ℹ 222 more rows\n\nfit |&gt;\n  forecast(h = \"20 years\") |&gt;\n  autoplot(china, level = NULL)\n\n\n\n\n\n\n\n\n\nThe transformations have a big effect, with small lambda values creating big increases in the forecasts.\nThe damping has relatively a small effect.\n\n\n\nfpp3 8.8, Ex7\n\nFind an ETS model for the Gas data from aus_production and forecast the next few years. Why is multiplicative seasonality necessary here? Experiment with making the trend damped. Does it improve the forecasts?\n\n\naus_production |&gt; autoplot(Gas)\n\n\n\n\n\n\n\n\n\nThere is a huge increase in variance as the series increases in level. =&gt; That makes it necessary to use multiplicative seasonality.\n\n\nfit &lt;- aus_production |&gt;\n  model(\n    hw = ETS(Gas ~ error(\"M\") + trend(\"A\") + season(\"M\")),\n    hwdamped = ETS(Gas ~ error(\"M\") + trend(\"Ad\") + season(\"M\")),\n  )\n\nfit |&gt; glance()\n\n# A tibble: 2 × 9\n  .model    sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE    MAE\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 hw       0.00324   -831. 1681. 1682. 1711.  21.1  32.2 0.0413\n2 hwdamped 0.00329   -832. 1684. 1685. 1718.  21.1  32.0 0.0417\n\n\n\nThe non-damped model seems to be doing slightly better here, probably because the trend is very strong over most of the historical data.\n\n\nfit |&gt;\n  select(hw) |&gt;\n  gg_tsresiduals()\n\n\n\n\n\n\n\nfit |&gt; tidy()\n\n# A tibble: 19 × 3\n   .model   term  estimate\n   &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;\n 1 hw       alpha   0.653 \n 2 hw       beta    0.144 \n 3 hw       gamma   0.0978\n 4 hw       l[0]    5.95  \n 5 hw       b[0]    0.0706\n 6 hw       s[0]    0.931 \n 7 hw       s[-1]   1.18  \n 8 hw       s[-2]   1.07  \n 9 hw       s[-3]   0.816 \n10 hwdamped alpha   0.649 \n11 hwdamped beta    0.155 \n12 hwdamped gamma   0.0937\n13 hwdamped phi     0.980 \n14 hwdamped l[0]    5.86  \n15 hwdamped b[0]    0.0994\n16 hwdamped s[0]    0.928 \n17 hwdamped s[-1]   1.18  \n18 hwdamped s[-2]   1.08  \n19 hwdamped s[-3]   0.817 \n\nfit |&gt;\n  augment() |&gt;\n  filter(.model == \"hw\") |&gt;\n  features(.innov, ljung_box, lag = 24)\n\n# A tibble: 1 × 3\n  .model lb_stat lb_pvalue\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 hw        57.1  0.000161\n\n\n\nThere is still some small correlations left in the residuals, showing the model has not fully captured the available information.\nThere also appears to be some heteroskedasticity in the residuals with larger variance in the first half the series.\n\n\nfit |&gt;\n  forecast(h = 36) |&gt;\n  filter(.model == \"hw\") |&gt;\n  autoplot(aus_production)\n\n\n\n\n\n\n\n\nWhile the point forecasts look ok, the intervals are excessively wide.\n\n\nfpp3 10.7, Ex 1\n\nThis exercise uses data set LakeHuron giving the level of Lake Huron from 1875–1972.\n\n\n\nConvert the data to a tsibble object using the as_tsibble() function.\nFit a piecewise linear trend model to the Lake Huron data with a knot at 1920 and an ARMA error structure.\nForecast the level for the next 30 years. Do you think the extrapolated linear trend is realistic?\n\n\n\nhuron &lt;- as_tsibble(LakeHuron)\nfit &lt;- huron |&gt;\n  model(ARIMA(value ~ trend(knot = 1920)))\nreport(fit)\n\nSeries: value \nModel: LM w/ ARIMA(2,0,0) errors \n\nCoefficients:\n         ar1      ar2  trend(knot = 1920)trend  trend(knot = 1920)trend_46\n      0.9628  -0.3107                  -0.0572                      0.0633\ns.e.  0.0973   0.0983                   0.0161                      0.0265\n      intercept\n       580.9391\ns.e.     0.5124\n\nsigma^2 estimated as 0.4594:  log likelihood=-98.86\nAIC=209.73   AICc=210.65   BIC=225.24\n\nfit |&gt;\n  forecast(h = 30) |&gt;\n  autoplot(huron) + labs(y = \"feet\")\n\n\n\n\n\n\n\n\nIt seems unlikely that there was an increasing trend from 1973 to 2002, but the prediction intervals are very wide so they probably capture the actual values. Historical data on the level of Lake Huron can be obtained from the NOAA.\n\n\nfpp3 10.7, Ex 7\n\nFor the retail time series considered in earlier chapters:\n\nDevelop an appropriate dynamic regression model with Fourier terms for the seasonality. Use the AIC to select the number of Fourier terms to include in the model. (You will probably need to use the same Box-Cox transformation you identified previously.)\n\n\n\nset.seed(12345678)\nmyseries &lt;- aus_retail |&gt;\n  filter(\n    `Series ID` == sample(aus_retail$`Series ID`, 1),\n    Month &lt; yearmonth(\"2018 Jan\")\n  )\n\nmyseries |&gt; features(Turnover, guerrero)\n\n# A tibble: 1 × 3\n  State              Industry                                    lambda_guerrero\n  &lt;chr&gt;              &lt;chr&gt;                                                 &lt;dbl&gt;\n1 Northern Territory Clothing, footwear and personal accessory …          0.0776\n\nmyseries |&gt; autoplot(log(Turnover))\n\n\n\n\n\n\n\nfit &lt;- myseries |&gt;\n  model(\n    `K=1` = ARIMA(log(Turnover) ~ trend() + fourier(K = 1) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    `K=2` = ARIMA(log(Turnover) ~ trend() + fourier(K = 2) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    `K=3` = ARIMA(log(Turnover) ~ trend() + fourier(K = 3) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    `K=4` = ARIMA(log(Turnover) ~ trend() + fourier(K = 4) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    `K=5` = ARIMA(log(Turnover) ~ trend() + fourier(K = 5) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    `K=6` = ARIMA(log(Turnover) ~ trend() + fourier(K = 6) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1))\n  )\nglance(fit)\n\n# A tibble: 6 × 10\n  State      Industry .model  sigma2 log_lik   AIC  AICc   BIC ar_roots ma_roots\n  &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;   &lt;list&gt;  \n1 Northern … Clothin… K=1    0.00664    383. -748. -748. -713. &lt;cpl&gt;    &lt;cpl&gt;   \n2 Northern … Clothin… K=2    0.00652    389. -756. -755. -713. &lt;cpl&gt;    &lt;cpl&gt;   \n3 Northern … Clothin… K=3    0.00626    400. -774. -773. -723. &lt;cpl&gt;    &lt;cpl&gt;   \n4 Northern … Clothin… K=4    0.00596    411. -792. -791. -734. &lt;cpl&gt;    &lt;cpl&gt;   \n5 Northern … Clothin… K=5    0.00480    453. -873. -872. -811. &lt;cpl&gt;    &lt;cpl&gt;   \n6 Northern … Clothin… K=6    0.00437    470. -906. -905. -841. &lt;cpl&gt;    &lt;cpl&gt;   \n\n\nIncluding 6 harmonics minimises the AICc (and AIC/BIC) for this series.\n\nfit &lt;- transmute(fit, best = `K=6`)\n\nreport(fit)\n\nSeries: Turnover \nModel: LM w/ ARIMA(1,0,1)(1,0,0)[12] errors \nTransformation: log(Turnover) \n\nCoefficients:\n         ar1      ma1    sar1  trend()  fourier(K = 6)C1_12\n      0.9632  -0.3755  0.1761   0.0041              -0.0809\ns.e.  0.0165   0.0502  0.0542   0.0006               0.0080\n      fourier(K = 6)S1_12  fourier(K = 6)C2_12  fourier(K = 6)S2_12\n                  -0.1258               0.0381              -0.0882\ns.e.               0.0080               0.0052               0.0052\n      fourier(K = 6)C3_12  fourier(K = 6)S3_12  fourier(K = 6)C4_12\n                  -0.0206              -0.0815              -0.0294\ns.e.               0.0045               0.0045               0.0042\n      fourier(K = 6)S4_12  fourier(K = 6)C5_12  fourier(K = 6)S5_12\n                  -0.0538              -0.0554              -0.0540\ns.e.               0.0042               0.0041               0.0041\n      fourier(K = 6)C6_12  intercept\n                  -0.0230     1.3317\ns.e.               0.0029     0.1231\n\nsigma^2 estimated as 0.004368:  log likelihood=470.23\nAIC=-906.46   AICc=-904.65   BIC=-840.54\n\n\nThe chosen model is a linear trend (will be exponential after back-transforming) and fourier terms with 5 harmonics. The error model is ARIMA(1,0,1)(1,0,1).\n\n\nCheck the residuals of the fitted model. Does the residual series look like white noise?\n\n\n\ngg_tsresiduals(fit)\n\n\n\n\n\n\n\n\nThe residuals look well behaved.\n\n\nCompare the forecasts with those you obtained earlier using alternative models.\n\n\n\nfit &lt;- myseries |&gt;\n  model(\n    dynamic = ARIMA(log(Turnover) ~ trend() + fourier(K = 6) +\n      pdq(0:2, 0, 0:2) + PDQ(0:1, 0, 0:1)),\n    arima = ARIMA(log(Turnover)),\n    ets = ETS(Turnover)\n  )\nfit |&gt;\n  forecast() |&gt;\n  autoplot(filter(myseries, year(Month) &gt; 2010), level = 80, alpha = 0.5)"
  },
  {
    "objectID": "exams/index.html",
    "href": "exams/index.html",
    "title": "Exam preparation",
    "section": "",
    "text": "Past exams\n\n2021\n2022\n2023\n2024\n\nThis year’s exam will follow the same format.\n\n\nExam conditions\nThe final assessment will be an on-campus eExam. It will be closed book with the following permitted items:\n\nUp to 5 blank pages for use as working sheets.\n1 A4 double-sided page containing notes. A physical page only is allowed. It may be typed or hand-written.\n4 pre-printed answer sheets (available in the exam room) for handwritten responses (e.g., equations or diagrams).\nA physical calculator of any type or one of the following virtual calculators:\n\nInbuilt Mac/Windows calculator\nWebsite https://www.educalc.net/2336211.page\n10bii Financial Calculator for Mac by K2 Cashflow, https://apps.apple.com/au/app/10bii-financial-calculator/id473144920\n\n\nAI tools are not to be used in the exam.\nThe exam will include one page of ETS formulas. Here they are, so you know exactly what you will see on the exam. No other formulas will be provided, but you can write whatever you like on your single A4 page of notes.\n\n\n\nETS formulas"
  },
  {
    "objectID": "assignments/Project.html",
    "href": "assignments/Project.html",
    "title": "Retail Project",
    "section": "",
    "text": "Objective: To forecast a real time series using ETS and ARIMA models.\nData: The data are monthly measures of retail trade volume in Australia, obtained from the ABS. Each student will be use a different time series, selected using their student ID number as follows. This is the same series that you used in previous assignments.\nlibrary(fpp3)\nget_my_data &lt;- function(student_id) {\n  set.seed(student_id)\n  all_data &lt;- readr::read_rds(\"https://bit.ly/monashretaildata\")\n  while(TRUE) {\n    retail &lt;- filter(all_data, `Series ID` == sample(`Series ID`, 1))\n    if(!any(is.na(fill_gaps(retail)$Turnover))) return(retail)\n  }\n}\n# Replace the argument with your student ID\nretail &lt;- get_my_data(12345678)\nAssignment value: This assignment is worth 12% of the overall unit assessment. You may copy and paste material from previous assignments, but you must take into account any feedback that you have received on these assignments.\nReport:\nYou should produce forecasts of the series using ETS and ARIMA models. Write a report of your analysis in Quarto format explaining carefully what you have done and why you have done it. You may use this file as a starting point.Your report should include the following elements.\n\nProduce some plots of your series, and describe what you learn from each plot. [2 marks]\nDiscuss the statistical features of the data, including the effect of COVID-19 on your series. [2 marks]\nFind an appropriate Box-Cox transformation for your data and explain why you have chosen the particular transformation parameter \\lambda. [2 marks]\nProduce a plot of an STL decomposition of the transformed data. What do you learn from the plot? [2 marks]\nWhat differencing would be required to make the data stationary? You should use a unit-root test as part of the discussion. [2 marks]\nDescribe the methodology used to create a short-list of appropriate ARIMA models and ETS models. Include discussion of AIC values as well as results from applying the models to a test-set consisting of the last 24 months of the data provided. [6 marks]\nChoose one ARIMA model and one ETS model based on this analysis and show parameter estimates, residual diagnostics, forecasts and prediction intervals for both models. Diagnostic checking for both models should include ACF graphs and the Ljung-Box test. [8 marks]\nCompare the results obtained from each of your preferred models. Which method do you think gives the better forecasts? Explain with reference to the test-set. [2 marks]\nApply your two chosen models to the full data set, re-estimating the parameters but not changing the model structure. Produce out-of-sample point forecasts and 80% prediction intervals for each model for two years past the end of the data provided. [4 marks]\nObtain up-to-date data from the ABS website (Table 11). You may need to use the previous release of data, rather than the latest release. Compare your forecasts with the actual numbers. How well did you do? [4 marks]\nDiscuss the benefits and limitations of the models for your data. [3 marks]\nEnsure graphs are properly labelled, including appropriate units of measurement. [3 marks]\n\nETC5550 students\n\nSuppose forecasts of your series were required every year based on the most recent data available at the time. Describe the steps you would undertake each year to produce these forecasts. [5 marks]\n\nNotes\n\nYour submission must include the Quarto file (.qmd), and should run without error.\nThere will be a 5 marks penalty if file does not run without error.\nWhen using the updated ABS data set, do not edit the downloaded file in any way.\nThere is no need to provide the updated ABS data with your submission.\n\n\n\nDue: 30 May 2025  Submit"
  },
  {
    "objectID": "assignments/A3.html",
    "href": "assignments/A3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "This assignment will use the same data that you will use in the retail project later in the semester. Each student will use a different time series, selected using their student ID number as follows.\nlibrary(fpp3)\nget_my_data &lt;- function(student_id) {\n  set.seed(student_id)\n  all_data &lt;- readr::read_rds(\"https://bit.ly/monashretaildata\")\n  while(TRUE) {\n    retail &lt;- filter(all_data, `Series ID` == sample(`Series ID`, 1))\n    if(!any(is.na(fill_gaps(retail)$Turnover))) return(retail)\n  }\n}\n# Replace the argument with your student ID\nretail &lt;- get_my_data(12345678)\nUse a training set up to and including 2018.\n\nWhat transformations (Box-Cox and/or differencing) would be required to make the data stationary? You should use a unit-root test as part of the discussion.\nUse a plot of the ACF and PACF of the (possibly differenced) data to determine two plausible ARIMA models for this data set.\nFit both models, along with an automatically chosen model, and produce forecasts for 2019–2022.\nWhich model is best based on AIC? Which model is best based on the test set RMSE? Which do you think is best to use for future forecasts? Why?\nCheck the residuals from your preferred model, using an ACF plot and a Ljung-Box test. Do the residuals appear to be white noise?\n\nSubmit a Quarto (qmd) file which carries out the above analysis. You need to submit one file which implements all steps above. You may use this file as a starting point.\n\n\nDue: 16 May 2025  Submit"
  },
  {
    "objectID": "assignments/A1.html",
    "href": "assignments/A1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "This assignment will use the same data that you will use in the retail project later in the semester. Each student will use a different time series, selected using their student ID number as follows.\nlibrary(fpp3)\nget_my_data &lt;- function(student_id) {\n  set.seed(student_id)\n  all_data &lt;- readr::read_rds(\"https://bit.ly/monashretaildata\")\n  while(TRUE) {\n    retail &lt;- filter(all_data, `Series ID` == sample(`Series ID`, 1))\n    if(!any(is.na(fill_gaps(retail)$Turnover))) return(retail)\n  }\n}\n# Replace the argument with your student ID\nretail &lt;- get_my_data(12345678)\n\nPlot your time series using the autoplot() command. What do you learn from the plot?\nPlot your time series using the gg_season() command. What do you learn from the plot?\nPlot your time series using the gg_subseries() command. What do you learn from the plot?\nFind an appropriate Box-Cox transformation for your data and explain why you have chosen the particular transformation parameter \\lambda.\nProduce a plot of an STL decomposition of the transformed data. What do you learn from the plot?\n\nFor all plots, please use appropriate axis labels and titles.\nYou need to submit one Quarto (qmd) file which implements all steps above. You may use this file as a starting point.\nTo receive full marks, the qmd file must compile without errors.\n\n\nDue: 28 March 2025  Submit"
  },
  {
    "objectID": "assignments/A2.html",
    "href": "assignments/A2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "This assignment will use the same data that you will use in the retail project later in the semester. Each student will use a different time series, selected using their student ID number as follows.\nlibrary(fpp3)\nget_my_data &lt;- function(student_id) {\n  set.seed(student_id)\n  all_data &lt;- readr::read_rds(\"https://bit.ly/monashretaildata\")\n  while(TRUE) {\n    retail &lt;- filter(all_data, `Series ID` == sample(`Series ID`, 1))\n    if(!any(is.na(fill_gaps(retail)$Turnover))) return(retail)\n  }\n}\n# Replace the argument with your student ID\nretail &lt;- get_my_data(12345678)\n\nUsing a test set of 2019–2022, fit an ETS model chosen automatically, and three benchmark methods to the training data. Which gives the best forecasts on the test set, based on RMSE?\nCheck the residuals from the best model using an ACF plot and a Ljung-Box test. Do the residuals appear to be white noise?\nNow use time-series cross-validation with a minimum sample size of 15 years, a step size of 1 year, and a forecast horizon of 5 years. Calculate the RMSE of the results. Does it change the conclusion you reach based on the test set?\nWhich of these two methods of evaluating accuracy is more reliable? Why?\n\nSubmit a Quarto (qmd) file which carries out the above analysis. You need to submit one file which implements all steps above. You may use this file as a starting point.\nTo receive full marks, the qmd file must compile without errors.\n\n\nDue: 17 April 2025  Submit"
  },
  {
    "objectID": "assignments/Assignment_template.html",
    "href": "assignments/Assignment_template.html",
    "title": "Title of your assignment",
    "section": "",
    "text": "library(fpp3)\nget_my_data &lt;- function(student_id) {\n  set.seed(student_id)\n  all_data &lt;- readr::read_rds(\"https://bit.ly/monashretaildata\")\n  while(TRUE) {\n    retail &lt;- filter(all_data, `Series ID` == sample(`Series ID`, 1))\n    if(!any(is.na(fill_gaps(retail)$Turnover))) return(retail)\n  }\n}\nretail &lt;- get_my_data(rmarkdown::metadata$student_id)"
  },
  {
    "objectID": "assignments/competition.html",
    "href": "assignments/competition.html",
    "title": "Forecasting competition",
    "section": "",
    "text": "You must provide forecasts for the following items:\nFor each of these, give a point forecast and an 80% prediction interval, and explain in a couple of sentences how each was obtained.\nDue: 7 March 2025  Submit"
  },
  {
    "objectID": "assignments/competition.html#forecasts",
    "href": "assignments/competition.html#forecasts",
    "title": "Forecasting competition",
    "section": "Forecasts",
    "text": "Forecasts\n\n\n\n\n\n\nQ1\n\n\nGoogle closing stock price on 24 March 2025\n\n\n\n\n\nQ2\n\n\nMaximum temperature at Melbourne airport on 14 April 2025\n\n\n\n\n\nQ3\n\n\nDifference in points in AFL Anzac Day clash\n\n\n\n\n\nQ4\n\n\nSeasonally adjusted total employment for April 2025\n\n\n\n\n\nQ5\n\n\nGoogle closing stock price on 26 May 2025"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ETC3550/5550 Applied forecasting",
    "section": "",
    "text": "Rob J Hyndman\nEmail: Rob.Hyndman@monash.edu\n\n\n\n\n\n\n\n\n\nMitchell O’Hara-Wild\nEmail: Mitch.OHara-Wild@monash.edu\n\n\n\n\n\n\n\n\nMaliny Po\n\n\nNuwani Palihawadana\n\n\nXiefei (Sapphire) Li"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "ETC3550/5550 Applied forecasting",
    "section": "",
    "text": "Rob J Hyndman\nEmail: Rob.Hyndman@monash.edu\n\n\n\n\n\n\n\n\n\nMitchell O’Hara-Wild\nEmail: Mitch.OHara-Wild@monash.edu\n\n\n\n\n\n\n\n\nMaliny Po\n\n\nNuwani Palihawadana\n\n\nXiefei (Sapphire) Li"
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "ETC3550/5550 Applied forecasting",
    "section": "Weekly schedule",
    "text": "Weekly schedule\n\nPre-recorded videos: approximately 1 hour per week [Slides]\nTutorials: 1 hour per week\nOnline lecture: 12noon Mondays\nWorkshop: 1pm Tuesdays, Lecture Theatre S3, 16 Rainforest Walk.\nRecordings\n\n\n\n\nWeek\nTopic\nChapter\nAssignments\nQuizzes\n\n\n\n\n03 Mar\nIntroduction to forecasting and R\n1. Getting started\nForecasting Competition\n\n\n\n10 Mar\nTime series graphics\n2. Time series graphics\n\nWeek 2\n\n\n17 Mar\nTime series decomposition\n3. Time series decomposition\n\nWeek 3\n\n\n24 Mar\nSimple forecasting methods\n5. The forecaster’s toolbox\nAssignment 1\nWeek 4\n\n\n31 Mar\nAccuracy evaluation\n5. The forecaster’s toolbox\n\nWeek 5\n\n\n07 Apr\nExponential smoothing\n8. Exponential smoothing\n\nWeek 6\n\n\n14 Apr\nExponential smoothing\n8. Exponential smoothing\nAssignment 2\nWeek 7\n\n\n21 Apr\nMid-semester break\n\n\n\n\n\n28 Apr\nARIMA models\n9. ARIMA models\n\nWeek 8\n\n\n05 May\nARIMA models\n9. ARIMA models\n\nWeek 9\n\n\n12 May\nARIMA models\n9. ARIMA models\nAssignment 3\nWeek 10\n\n\n19 May\nMultiple regression and forecasting\n7. Time series regression models\n\nWeek 11\n\n\n26 May\nDynamic regression\n10. Dynamic regression models\nRetail Project"
  },
  {
    "objectID": "index.html#assessments",
    "href": "index.html#assessments",
    "title": "ETC3550/5550 Applied forecasting",
    "section": "Assessments",
    "text": "Assessments\n\nForecasting competition: 2%\nWeekly quizzes: 8%\nAssignment 1: 6%\nAssignment 2: 6%\nAssignment 3: 6%\nRetail project: 12%\nFinal exam: 60%"
  },
  {
    "objectID": "index.html#r-package-installation",
    "href": "index.html#r-package-installation",
    "title": "ETC3550/5550 Applied forecasting",
    "section": "R package installation",
    "text": "R package installation\nHere is the code to install the R packages we will be using in this unit.\ninstall.packages(c(\"tidyverse\",\"fpp3\", \"GGally\"), dependencies = TRUE)"
  },
  {
    "objectID": "week1/activities.html",
    "href": "week1/activities.html",
    "title": "Activities: Week 1",
    "section": "",
    "text": "The pedestrian dataset contains hourly pedestrian counts from 2015-01-01 to 2016-12-31 at 4 sensors in the city of Melbourne.\nThe data is shown below:\n\n\n# A tibble: 66,037 × 5\n   Sensor         Date_Time           Date        Time Count\n   &lt;chr&gt;          &lt;dttm&gt;              &lt;date&gt;     &lt;int&gt; &lt;int&gt;\n 1 Birrarung Marr 2015-01-01 00:00:00 2015-01-01     0  1630\n 2 Birrarung Marr 2015-01-01 01:00:00 2015-01-01     1   826\n 3 Birrarung Marr 2015-01-01 02:00:00 2015-01-01     2   567\n 4 Birrarung Marr 2015-01-01 03:00:00 2015-01-01     3   264\n 5 Birrarung Marr 2015-01-01 04:00:00 2015-01-01     4   139\n 6 Birrarung Marr 2015-01-01 05:00:00 2015-01-01     5    77\n 7 Birrarung Marr 2015-01-01 06:00:00 2015-01-01     6    44\n 8 Birrarung Marr 2015-01-01 07:00:00 2015-01-01     7    56\n 9 Birrarung Marr 2015-01-01 08:00:00 2015-01-01     8   113\n10 Birrarung Marr 2015-01-01 09:00:00 2015-01-01     9   166\n# ℹ 66,027 more rows\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify the index variable, key variable(s), and measured variable(s) of this dataset.\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThe index variable contains the complete time information\nThe key variable(s) identify each time series\nThe measured variable(s) are what you want to explore/forecast.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe aus_accommodation dataset contains quarterly data on Australian tourist accommodation from short-term non-residential accommodation with 15 or more rooms, 1998 Q1 - 2016 Q2. The first few lines are shown below.\n\nThe units of the measured variables are as follows:\n\nTakings are in millions of Australian dollars\nOccupancy is a percentage of rooms occupied\nCPI is an index with value 100 in 2012 Q1.\n\n\n\n\n\n\n\nYour turn!\n\n\n\nComplete the code to convert this dataset into a tsibble.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBegin by loading the fpp3 library to use its time series functions.\nlibrary(fpp3)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter loading the fpp3 package, convert the data frame into a tsibble.\nlibrary(fpp3)\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt; mutate(Date = as.Date(Date))\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember to specify the time index and key for as_tsibble() to function correctly.\nlibrary(fpp3)\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Date = as.Date(Date)) |&gt;\n  as_tsibble(key = State, index = Date)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal granularity\n\n\n\nThe previous exercise produced a dataset with daily frequency - although clearly the data is quarterly! This is because we are using a daily granularity which is inappropriate for this data.\n\n\nCommon temporal granularities can be created with these functions:\n\n\n\n\n\nGranularity\nFunction\n\n\n\n\nAnnual\nas.integer()\n\n\nQuarterly\nyearquarter()\n\n\nMonthly\nyearmonth()\n\n\nWeekly\nyearweek()\n\n\nDaily\nas_date(), ymd()\n\n\nSub-daily\nas_datetime()\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUse the appropriate granularity for the aus_accommodation dataset, and verify that the frequency is now quarterly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nStart by reading the CSV file and transform the data using mutate() and yearquarter() for the Date column.\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Quarter = yearquarter(Date))\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter transforming the Date column, make sure you convert the data frame to a tsibble.\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Quarter = yearquarter(Date)) |&gt;\n  as_tsibble(key = State, index = Quarter)\n\n\n\n\n\n\n\nThe tourism dataset contains the quarterly overnight trips from 1998 Q1 to 2016 Q4 across Australia.\nIt is disaggregated by 3 key variables:\n\nState: States and territories of Australia\nRegion: The tourism regions are formed through the aggregation of Statistical Local Areas (SLAs) which are defined by the various State and Territory tourism authorities according to their research and marketing needs\nPurpose: Stopover purpose of visit: “Holiday”, “Visiting friends and relatives”, “Business”, “Other reason”.\n\nBelow is a preview:\n\n\n# A tsibble: 24,320 x 5 [1Q]\n# Key:       Region, State, Purpose [304]\n   Quarter Region   State           Purpose  Trips\n     &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 1998 Q1 Adelaide South Australia Business  135.\n 2 1998 Q2 Adelaide South Australia Business  110.\n 3 1998 Q3 Adelaide South Australia Business  166.\n 4 1998 Q4 Adelaide South Australia Business  127.\n 5 1999 Q1 Adelaide South Australia Business  137.\n 6 1999 Q2 Adelaide South Australia Business  200.\n 7 1999 Q3 Adelaide South Australia Business  169.\n 8 1999 Q4 Adelaide South Australia Business  134.\n 9 2000 Q1 Adelaide South Australia Business  154.\n10 2000 Q2 Adelaide South Australia Business  169.\n# ℹ 24,310 more rows\n\n\nCalculate the total quarterly tourists visiting Victoria from the tourism dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo start off, filter the tourism dataset for only Victoria.\ntourism |&gt;\n  filter(State == \"Victoria\")\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter filtering, summarise the total trips for Victoria.\ntourism |&gt;\n  filter(State == \"Victoria\") |&gt;\n  summarise(Trips = sum(Trips))\n\n\n\n\n\n\n\nFind what combination of Region and Purpose had the maximum number of overnight trips on average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nStart by using as_tibble() to convert tourism back to a tibble and group it by Region and Purpose.\ntourism |&gt;\n  as_tibble() |&gt;\n  group_by(Region, Purpose)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter grouping, summarise the mean number of trips and filter for maximum trips.\ntourism |&gt;\n  as_tibble() |&gt;\n  group_by(Region, Purpose) |&gt;\n  summarise(Trips = mean(Trips), .groups = \"drop\") |&gt;\n  filter(Trips == max(Trips))\n\n\n\n\n\n\n\nCreate a new tsibble which combines the Purposes and Regions, and just has total trips by State.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo summarise the number of trips by each State, start by grouping the data by State.\ntourism |&gt;\n  group_by(State)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter grouping, use the summarise() function to sum the trips.\ntourism |&gt;\n  group_by(State) |&gt;\n  summarise(Trips = sum(Trips))"
  },
  {
    "objectID": "week1/activities.html#exercise-1",
    "href": "week1/activities.html#exercise-1",
    "title": "Activities: Week 1",
    "section": "",
    "text": "The pedestrian dataset contains hourly pedestrian counts from 2015-01-01 to 2016-12-31 at 4 sensors in the city of Melbourne.\nThe data is shown below:\n\n\n# A tibble: 66,037 × 5\n   Sensor         Date_Time           Date        Time Count\n   &lt;chr&gt;          &lt;dttm&gt;              &lt;date&gt;     &lt;int&gt; &lt;int&gt;\n 1 Birrarung Marr 2015-01-01 00:00:00 2015-01-01     0  1630\n 2 Birrarung Marr 2015-01-01 01:00:00 2015-01-01     1   826\n 3 Birrarung Marr 2015-01-01 02:00:00 2015-01-01     2   567\n 4 Birrarung Marr 2015-01-01 03:00:00 2015-01-01     3   264\n 5 Birrarung Marr 2015-01-01 04:00:00 2015-01-01     4   139\n 6 Birrarung Marr 2015-01-01 05:00:00 2015-01-01     5    77\n 7 Birrarung Marr 2015-01-01 06:00:00 2015-01-01     6    44\n 8 Birrarung Marr 2015-01-01 07:00:00 2015-01-01     7    56\n 9 Birrarung Marr 2015-01-01 08:00:00 2015-01-01     8   113\n10 Birrarung Marr 2015-01-01 09:00:00 2015-01-01     9   166\n# ℹ 66,027 more rows\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify the index variable, key variable(s), and measured variable(s) of this dataset.\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nThe index variable contains the complete time information\nThe key variable(s) identify each time series\nThe measured variable(s) are what you want to explore/forecast."
  },
  {
    "objectID": "week1/activities.html#exercise-2",
    "href": "week1/activities.html#exercise-2",
    "title": "Activities: Week 1",
    "section": "",
    "text": "The aus_accommodation dataset contains quarterly data on Australian tourist accommodation from short-term non-residential accommodation with 15 or more rooms, 1998 Q1 - 2016 Q2. The first few lines are shown below.\n\nThe units of the measured variables are as follows:\n\nTakings are in millions of Australian dollars\nOccupancy is a percentage of rooms occupied\nCPI is an index with value 100 in 2012 Q1.\n\n\n\n\n\n\n\nYour turn!\n\n\n\nComplete the code to convert this dataset into a tsibble.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBegin by loading the fpp3 library to use its time series functions.\nlibrary(fpp3)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter loading the fpp3 package, convert the data frame into a tsibble.\nlibrary(fpp3)\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt; mutate(Date = as.Date(Date))\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember to specify the time index and key for as_tsibble() to function correctly.\nlibrary(fpp3)\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Date = as.Date(Date)) |&gt;\n  as_tsibble(key = State, index = Date)"
  },
  {
    "objectID": "week1/activities.html#exercise-3",
    "href": "week1/activities.html#exercise-3",
    "title": "Activities: Week 1",
    "section": "",
    "text": "Temporal granularity\n\n\n\nThe previous exercise produced a dataset with daily frequency - although clearly the data is quarterly! This is because we are using a daily granularity which is inappropriate for this data.\n\n\nCommon temporal granularities can be created with these functions:\n\n\n\n\n\nGranularity\nFunction\n\n\n\n\nAnnual\nas.integer()\n\n\nQuarterly\nyearquarter()\n\n\nMonthly\nyearmonth()\n\n\nWeekly\nyearweek()\n\n\nDaily\nas_date(), ymd()\n\n\nSub-daily\nas_datetime()\n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nUse the appropriate granularity for the aus_accommodation dataset, and verify that the frequency is now quarterly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nStart by reading the CSV file and transform the data using mutate() and yearquarter() for the Date column.\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Quarter = yearquarter(Date))\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter transforming the Date column, make sure you convert the data frame to a tsibble.\naus_accommodation &lt;- read.csv(\n  \"https://workshop.nectric.com.au/user2024/data/aus_accommodation.csv\"\n) |&gt;\n  mutate(Quarter = yearquarter(Date)) |&gt;\n  as_tsibble(key = State, index = Quarter)"
  },
  {
    "objectID": "week1/activities.html#exercise-4",
    "href": "week1/activities.html#exercise-4",
    "title": "Activities: Week 1",
    "section": "",
    "text": "The tourism dataset contains the quarterly overnight trips from 1998 Q1 to 2016 Q4 across Australia.\nIt is disaggregated by 3 key variables:\n\nState: States and territories of Australia\nRegion: The tourism regions are formed through the aggregation of Statistical Local Areas (SLAs) which are defined by the various State and Territory tourism authorities according to their research and marketing needs\nPurpose: Stopover purpose of visit: “Holiday”, “Visiting friends and relatives”, “Business”, “Other reason”.\n\nBelow is a preview:\n\n\n# A tsibble: 24,320 x 5 [1Q]\n# Key:       Region, State, Purpose [304]\n   Quarter Region   State           Purpose  Trips\n     &lt;qtr&gt; &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;\n 1 1998 Q1 Adelaide South Australia Business  135.\n 2 1998 Q2 Adelaide South Australia Business  110.\n 3 1998 Q3 Adelaide South Australia Business  166.\n 4 1998 Q4 Adelaide South Australia Business  127.\n 5 1999 Q1 Adelaide South Australia Business  137.\n 6 1999 Q2 Adelaide South Australia Business  200.\n 7 1999 Q3 Adelaide South Australia Business  169.\n 8 1999 Q4 Adelaide South Australia Business  134.\n 9 2000 Q1 Adelaide South Australia Business  154.\n10 2000 Q2 Adelaide South Australia Business  169.\n# ℹ 24,310 more rows\n\n\nCalculate the total quarterly tourists visiting Victoria from the tourism dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo start off, filter the tourism dataset for only Victoria.\ntourism |&gt;\n  filter(State == \"Victoria\")\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter filtering, summarise the total trips for Victoria.\ntourism |&gt;\n  filter(State == \"Victoria\") |&gt;\n  summarise(Trips = sum(Trips))"
  },
  {
    "objectID": "week1/activities.html#exercise-5",
    "href": "week1/activities.html#exercise-5",
    "title": "Activities: Week 1",
    "section": "",
    "text": "Find what combination of Region and Purpose had the maximum number of overnight trips on average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nStart by using as_tibble() to convert tourism back to a tibble and group it by Region and Purpose.\ntourism |&gt;\n  as_tibble() |&gt;\n  group_by(Region, Purpose)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter grouping, summarise the mean number of trips and filter for maximum trips.\ntourism |&gt;\n  as_tibble() |&gt;\n  group_by(Region, Purpose) |&gt;\n  summarise(Trips = mean(Trips), .groups = \"drop\") |&gt;\n  filter(Trips == max(Trips))"
  },
  {
    "objectID": "week1/activities.html#exercise-6",
    "href": "week1/activities.html#exercise-6",
    "title": "Activities: Week 1",
    "section": "",
    "text": "Create a new tsibble which combines the Purposes and Regions, and just has total trips by State.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo summarise the number of trips by each State, start by grouping the data by State.\ntourism |&gt;\n  group_by(State)\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nAfter grouping, use the summarise() function to sum the trips.\ntourism |&gt;\n  group_by(State) |&gt;\n  summarise(Trips = sum(Trips))"
  },
  {
    "objectID": "week2/index.html",
    "href": "week2/index.html",
    "title": "Week 2: Time series graphics",
    "section": "",
    "text": "Different types of plots for time series including time plots, season plots, subseries plots, lag plots and ACF plots\nThe difference between seasonal patterns and cyclic patterns in time series\nWhat is “white noise” and how to identify it."
  },
  {
    "objectID": "week2/index.html#what-you-will-learn-this-week",
    "href": "week2/index.html#what-you-will-learn-this-week",
    "title": "Week 2: Time series graphics",
    "section": "",
    "text": "Different types of plots for time series including time plots, season plots, subseries plots, lag plots and ACF plots\nThe difference between seasonal patterns and cyclic patterns in time series\nWhat is “white noise” and how to identify it."
  },
  {
    "objectID": "week2/index.html#pre-class-activities",
    "href": "week2/index.html#pre-class-activities",
    "title": "Week 2: Time series graphics",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Chapter 2 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week2/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week2/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 2: Time series graphics",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 1-5 from Section 2.10 of the book.\n\nCheck your understanding quiz"
  },
  {
    "objectID": "week2/index.html#slides-for-monday-lecture",
    "href": "week2/index.html#slides-for-monday-lecture",
    "title": "Week 2: Time series graphics",
    "section": "Slides for Monday lecture",
    "text": "Slides for Monday lecture\n\n\nDownload pdf\nCode used in lecture\nCode used in workshop"
  },
  {
    "objectID": "week2/index.html#activities-for-tuesday-workshop",
    "href": "week2/index.html#activities-for-tuesday-workshop",
    "title": "Week 2: Time series graphics",
    "section": "Activities for Tuesday workshop",
    "text": "Activities for Tuesday workshop"
  },
  {
    "objectID": "week2/index.html#assignments",
    "href": "week2/index.html#assignments",
    "title": "Week 2: Time series graphics",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Friday 28 March."
  },
  {
    "objectID": "week2/index.html#weekly-quiz",
    "href": "week2/index.html#weekly-quiz",
    "title": "Week 2: Time series graphics",
    "section": "Weekly quiz",
    "text": "Weekly quiz\n\nWeek 2 quiz is due on Sunday 16 March."
  },
  {
    "objectID": "week3/activities.html",
    "href": "week3/activities.html",
    "title": "Activities: Week 3",
    "section": "",
    "text": "The first Melbourne COVID-19 lockdown was from 31 March 2020 to 12 May 2020. We are interested in whether more babies than usual were born 9 months later — in January or February 2021.\nFirst extract the data for Victoria from the aus_births dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis shows the number of births in Victoria each month from January 1975 to December 2021.\nNow produce a time plot and a season plot of the data, to better understand the trend and seasonal patterns.\n\n\n\n\n\n\n\n\nThe see-saw seasonal pattern is due to month length variation.\nSo let’s remove it by taking the average of the daily births over the month. First we create a tibble with the number of days in each month, then we join it to the vic_births data so we can compute the daily average for each month.\n\n\n\n\n\n\n\n\nTry to understand what each line of the above code is doing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow repeat the time and season plot, but using the daily average variable, rather than the monthly total.\n\n\n\n\n\n\n\n\n\nHas the peak month for having babies changed compared to the previous gg_season plot? Why?\nWhy has the unusual fluctuation after January 2020 apparently increased in size compared to the previous gg_season plot?\n\nLet’s do an STL decomposition. Experiment with the value of window and robust to see how the seasonal component changes over time. We need robust  = TRUE here, so the unusual behaviour near the end of the series does not have a strong effect on the trend or seasonal components\n\n\n\n\n\n\n\n\nWith your preferred values of window and robust, plot the remainder component\n\n\n\n\n\n\n\n\nOnce we remove the trend and seasonal component, and just look at the remainder, we can see any effects that are not simply seasonality or trend. Here we are plotting the remainder for the last year.\nHow many extra births per month than normal were there in February 2021?"
  },
  {
    "objectID": "week3/activities.html#was-there-a-covid-baby-boom-in-victoria",
    "href": "week3/activities.html#was-there-a-covid-baby-boom-in-victoria",
    "title": "Activities: Week 3",
    "section": "",
    "text": "The first Melbourne COVID-19 lockdown was from 31 March 2020 to 12 May 2020. We are interested in whether more babies than usual were born 9 months later — in January or February 2021.\nFirst extract the data for Victoria from the aus_births dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis shows the number of births in Victoria each month from January 1975 to December 2021.\nNow produce a time plot and a season plot of the data, to better understand the trend and seasonal patterns.\n\n\n\n\n\n\n\n\nThe see-saw seasonal pattern is due to month length variation.\nSo let’s remove it by taking the average of the daily births over the month. First we create a tibble with the number of days in each month, then we join it to the vic_births data so we can compute the daily average for each month.\n\n\n\n\n\n\n\n\nTry to understand what each line of the above code is doing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow repeat the time and season plot, but using the daily average variable, rather than the monthly total.\n\n\n\n\n\n\n\n\n\nHas the peak month for having babies changed compared to the previous gg_season plot? Why?\nWhy has the unusual fluctuation after January 2020 apparently increased in size compared to the previous gg_season plot?\n\nLet’s do an STL decomposition. Experiment with the value of window and robust to see how the seasonal component changes over time. We need robust  = TRUE here, so the unusual behaviour near the end of the series does not have a strong effect on the trend or seasonal components\n\n\n\n\n\n\n\n\nWith your preferred values of window and robust, plot the remainder component\n\n\n\n\n\n\n\n\nOnce we remove the trend and seasonal component, and just look at the remainder, we can see any effects that are not simply seasonality or trend. Here we are plotting the remainder for the last year.\nHow many extra births per month than normal were there in February 2021?"
  },
  {
    "objectID": "week3/activities.html#canadian-gas-production",
    "href": "week3/activities.html#canadian-gas-production",
    "title": "Activities: Week 3",
    "section": "Canadian gas production",
    "text": "Canadian gas production\nThis exercise uses the canadian_gas data (monthly Canadian gas production in billions of cubic metres, January 1960 – February 2005).\nTry to find a Box-Cox transformation to stabilise the variance. Why isn’t it possible?\n\n\n\n\n\n\n\n\nWithout using a transformation, do a time plot, season plot and subseries plot to look at the effect of the changing seasonality over time.\n\n\n\n\n\n\n\n\nDo an STL decomposition of the data. You will need to choose a seasonal window to allow for the changing shape of the seasonal component.\n\n\n\n\n\n\n\n\nHow does the seasonal shape change over time?\n\n\n\n\n\n\n\n\nCan you produce a plausible seasonally adjusted series?"
  },
  {
    "objectID": "week3/activities.html#canberra-public-transport-usage",
    "href": "week3/activities.html#canberra-public-transport-usage",
    "title": "Activities: Week 3",
    "section": "Canberra public transport usage",
    "text": "Canberra public transport usage\n\nDo Exam 2024, Question B2."
  },
  {
    "objectID": "week8/index.html",
    "href": "week8/index.html",
    "title": "Week 8: ARIMA models",
    "section": "",
    "text": "AR, MA, ARMA and ARIMA models\nSelecting model orders manually and automatically"
  },
  {
    "objectID": "week8/index.html#what-you-will-learn-this-week",
    "href": "week8/index.html#what-you-will-learn-this-week",
    "title": "Week 8: ARIMA models",
    "section": "",
    "text": "AR, MA, ARMA and ARIMA models\nSelecting model orders manually and automatically"
  },
  {
    "objectID": "week8/index.html#pre-class-activities",
    "href": "week8/index.html#pre-class-activities",
    "title": "Week 8: ARIMA models",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Sections 9.3-9.8 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week8/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week8/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 8: ARIMA models",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 1-5 from Section 9.11 of the book."
  },
  {
    "objectID": "week9/index.html",
    "href": "week9/index.html",
    "title": "Week 9: ARIMA models",
    "section": "",
    "text": "Seasonal ARIMA models\nComputing forecasts for ARIMA models\nARIMA vs ETS models"
  },
  {
    "objectID": "week9/index.html#what-you-will-learn-this-week",
    "href": "week9/index.html#what-you-will-learn-this-week",
    "title": "Week 9: ARIMA models",
    "section": "",
    "text": "Seasonal ARIMA models\nComputing forecasts for ARIMA models\nARIMA vs ETS models"
  },
  {
    "objectID": "week9/index.html#pre-class-activities",
    "href": "week9/index.html#pre-class-activities",
    "title": "Week 9: ARIMA models",
    "section": "Pre-class activities",
    "text": "Pre-class activities\nRead Sections 9.8-9.10 of the textbook and watch all embedded videos"
  },
  {
    "objectID": "week9/index.html#exercises-on-your-own-or-in-tutorial",
    "href": "week9/index.html#exercises-on-your-own-or-in-tutorial",
    "title": "Week 9: ARIMA models",
    "section": "Exercises (on your own or in tutorial)",
    "text": "Exercises (on your own or in tutorial)\nComplete Exercises 6-10 from Section 9.11 of the book."
  }
]