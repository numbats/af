{
  "hash": "2100b0ee5852243f7110d47095dabb2d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercise Week 12: Solutions\"\n---\n\n\n\n# fpp3 10.7, Ex 2\n\n> Repeat Exercise 4 from Section 7.10, but this time adding in ARIMA errors to address the autocorrelations in the residuals.\n\n>    a. How much difference does the ARIMA error process make to the regression coefficients?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- souvenirs |>\n  mutate(festival = month(Month) == 3 & year(Month) != 1987) |>\n  model(\n    reg = TSLM(log(Sales) ~ trend() + season() + festival),\n    dynreg = ARIMA(log(Sales) ~ trend() + season() + festival)\n  )\ntidy(fit) |> print(n=50)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 31 × 6\n   .model term           estimate std.error statistic  p.value\n   <chr>  <chr>             <dbl>     <dbl>     <dbl>    <dbl>\n 1 reg    (Intercept)      7.62    0.0742     103.    4.67e-78\n 2 reg    trend()          0.0220  0.000827    26.6   2.32e-38\n 3 reg    season()year2    0.251   0.0957       2.63  1.06e- 2\n 4 reg    season()year3    0.266   0.193        1.38  1.73e- 1\n 5 reg    season()year4    0.384   0.0957       4.01  1.48e- 4\n 6 reg    season()year5    0.409   0.0957       4.28  5.88e- 5\n 7 reg    season()year6    0.449   0.0958       4.69  1.33e- 5\n 8 reg    season()year7    0.610   0.0958       6.37  1.71e- 8\n 9 reg    season()year8    0.588   0.0959       6.13  4.53e- 8\n10 reg    season()year9    0.669   0.0959       6.98  1.36e- 9\n11 reg    season()year10   0.747   0.0960       7.79  4.48e-11\n12 reg    season()year11   1.21    0.0960      12.6   1.29e-19\n13 reg    season()year12   1.96    0.0961      20.4   3.39e-31\n14 reg    festivalTRUE     0.502   0.196        2.55  1.29e- 2\n15 dynreg ar1              0.556   0.179        3.11  2.53e- 3\n16 dynreg ma1             -0.129   0.192       -0.670 5.05e- 1\n17 dynreg ma2              0.340   0.114        2.99  3.68e- 3\n18 dynreg trend()          0.0226  0.00150     15.1   1.17e-25\n19 dynreg season()year2    0.252   0.0574       4.38  3.40e- 5\n20 dynreg season()year3    0.297   0.118        2.51  1.42e- 2\n21 dynreg season()year4    0.377   0.0729       5.17  1.56e- 6\n22 dynreg season()year5    0.400   0.0789       5.07  2.30e- 6\n23 dynreg season()year6    0.438   0.0817       5.36  7.19e- 7\n24 dynreg season()year7    0.598   0.0827       7.23  2.04e-10\n25 dynreg season()year8    0.573   0.0821       6.98  6.45e-10\n26 dynreg season()year9    0.651   0.0799       8.16  2.94e-12\n27 dynreg season()year10   0.725   0.0746       9.71  2.18e-15\n28 dynreg season()year11   1.18    0.0629      18.7   1.14e-31\n29 dynreg season()year12   1.93    0.0599      32.2   5.41e-49\n30 dynreg festivalTRUE     0.461   0.119        3.86  2.19e- 4\n31 dynreg intercept        7.60    0.0857      88.7   8.60e-85\n```\n\n\n:::\n:::\n\n\nThe coefficients are all relatively close.\n\n>    b. How much difference does the ARIMA error process make to the forecasts?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfuture_souvenirs <- new_data(souvenirs, n = 24) |>\n  mutate(festival = month(Month) == 3)\nfit |>\n  forecast(new_data = future_souvenirs)  |>\n  autoplot(souvenirs, level=95)\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex2b-1.png){width=672}\n:::\n:::\n\n\nThe forecasts are also extremely close.\n\n>    c. Check the residuals of the fitted model to ensure the ARIMA process has adequately addressed the autocorrelations seen in the `TSLM` model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  select(dynreg) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex2c-1.png){width=672}\n:::\n:::\n\n\nThese look fine.\n\n# fpp3 10.7, Ex 3\n\n> Repeat the daily electricity example, but instead of using a quadratic function of temperature, use a piecewise linear function with the \"knot\" around 25 degrees Celsius (use predictors `Temperature` & `Temp2`). How can you optimize the choice of knot?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvic_elec_daily <- vic_elec |>\n  filter(year(Time) == 2014) |>\n  index_by(Date = date(Time)) |>\n  summarise(\n    Demand = sum(Demand) / 1e3,\n    Temperature = max(Temperature),\n    Holiday = any(Holiday)\n  ) |>\n  mutate(\n    Temp2 = I(pmax(Temperature - 25, 0)),\n    Day_Type = case_when(\n      Holiday ~ \"Holiday\",\n      wday(Date) %in% 2:6 ~ \"Weekday\",\n      TRUE ~ \"Weekend\"\n    )\n  )\n```\n:::\n\n\nFirst, we will leave the knot at 25 and find the best ARIMA model. This will take a while, but we only have to do it once.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvic_elec_daily |>\n  model(\n    fit = ARIMA(Demand ~ Temperature + Temp2 + (Day_Type == \"Weekday\"))) |>\n  report()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Demand \nModel: LM w/ ARIMA(3,1,1)(2,0,0)[7] errors \n\nCoefficients:\n         ar1      ar2     ar3      ma1    sar1    sar2  Temperature   Temp2\n      0.7914  -0.0108  0.0110  -0.9762  0.1995  0.2936      -0.7025  4.4068\ns.e.  0.0730   0.0831  0.0568   0.0213  0.0543  0.0569       0.1744  0.3069\n      Day_Type == \"Weekday\"TRUE\n                        31.4648\ns.e.                     1.3758\n\nsigma^2 estimated as 61.42:  log likelihood=-1262.4\nAIC=2544.81   AICc=2545.43   BIC=2583.78\n```\n\n\n:::\n:::\n\n\nNow we will use that ARIMA(3,1,1)(2,0,0)[7] model and modify the knot until the AICc is optmized.\n\n\nTo optimize the knot position, we need to try many knot locations and select the model with the smallest AICc value. The fit is sensitive to the knot placement, with errors occurring at some knot locations. So we need to do a grid search, allowing for null models to be returned.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_aicc <- Inf\nfor(knot in seq(20, 30, by=0.1)) {\n  elec_model <- vic_elec_daily |>\n    mutate(Temp2 = pmax(Temperature - knot, 0)) |>\n    model(fit = ARIMA(Demand ~ Temperature + Temp2 + (Day_Type == \"Weekday\")))\n  if(!is_null_model(elec_model$fit[[1]])) {\n    aicc <- glance(elec_model) |> pull(AICc)\n    if(aicc < best_aicc) {\n      best_aicc <- aicc\n      best_knot <- knot\n      best_model <- elec_model\n    }\n  }\n}\nbest_model |> report()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Demand \nModel: LM w/ ARIMA(2,1,1)(2,0,0)[7] errors \n\nCoefficients:\n         ar1     ar2      ma1    sar1    sar2  Temperature   Temp2\n      0.7436  0.0465  -0.9699  0.1941  0.3127      -0.0089  5.3667\ns.e.  0.0677  0.0648   0.0241  0.0532  0.0564       0.1336  0.3316\n      Day_Type == \"Weekday\"TRUE\n                        31.0531\ns.e.                     1.3592\n\nsigma^2 estimated as 59.38:  log likelihood=-1256.8\nAIC=2531.6   AICc=2532.1   BIC=2566.67\n```\n\n\n:::\n:::\n\n\nThe optimal knot (to 1 decimal place) is 28.3 degrees Celsius.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(best_model) |>\n  left_join(vic_elec_daily) |>\n  ggplot(aes(x = Temperature)) +\n  geom_point(aes(y = Demand)) +\n  geom_point(aes(y = .fitted), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex3d-1.png){width=672}\n:::\n\n```{.r .cell-code}\naugment(best_model) |>\n  left_join(vic_elec_daily) |>\n  ggplot(aes(x = Demand)) +\n  geom_point(aes(y = .fitted)) +\n  geom_abline(aes(intercept = 0, slope = 1))\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex3d-2.png){width=672}\n:::\n\n```{.r .cell-code}\nbest_model |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex3d-3.png){width=672}\n:::\n\n```{.r .cell-code}\naugment(best_model) |>\n  features(.innov, ljung_box, dof = 6, lag = 21)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .model lb_stat lb_pvalue\n  <chr>    <dbl>     <dbl>\n1 fit       31.1   0.00841\n```\n\n\n:::\n:::\n\n\nThe model fails the residual tests but the significant autocorrelations are relatively small, so it should still give reasonable forecasts. The residuals look like they have some heteroskedasticity, but otherwise look ok.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvic_next_day <- new_data(vic_elec_daily, 1) |>\n  mutate(\n    Temperature = 26,\n    Temp2 = I(pmax(Temperature - best_knot, 0)),\n    Day_Type = \"Holiday\"\n  )\nforecast(best_model, vic_next_day)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A fable: 1 x 7 [1D]\n# Key:     .model [1]\n  .model Date      \n  <chr>  <date>    \n1 fit    2015-01-01\n# ℹ 5 more variables: Demand <dist>, .mean <dbl>, Temperature <dbl>,\n#   Temp2 <I<dbl>>, Day_Type <chr>\n```\n\n\n:::\n\n```{.r .cell-code}\nvic_elec_future <- new_data(vic_elec_daily, 14) |>\n  mutate(\n    Temperature = 26,\n    Temp2 = I(pmax(Temperature - best_knot, 0)),\n    Holiday = c(TRUE, rep(FALSE, 13)),\n    Day_Type = case_when(\n      Holiday ~ \"Holiday\",\n      wday(Date) %in% 2:6 ~ \"Weekday\",\n      TRUE ~ \"Weekend\"\n    )\n  )\nforecast(best_model, vic_elec_future) |>\n  autoplot(vic_elec_daily) + labs(y = \"Electricity demand (GW)\")\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex3e-1.png){width=672}\n:::\n:::\n\n\n# fpp3 10.7, Ex 4\n\n> This exercise concerns `aus_accommodation`: the total quarterly takings from accommodation and the room occupancy level for hotels, motels, and guest houses in Australia, between January 1998 and June 2016. Total quarterly takings are in millions of Australian dollars.\n>    a. Compute the CPI-adjusted takings and plot the result for each state\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_accommodation <- aus_accommodation |>\n  mutate(\n    adjTakings = Takings / CPI * 100\n  )\naus_accommodation |>\n  autoplot(adjTakings)\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex4a-1.png){width=672}\n:::\n:::\n\n\n>    b. For each state, fit a dynamic regression model of CPI-adjusted takings with seasonal dummy variables, a piecewise linear time trend with one knot at 2008 Q1, and ARIMA errors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- aus_accommodation |>\n  model(\n    ARIMA(adjTakings ~ season() + trend(knot = yearquarter(\"2008 Q1\")))\n  )\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A mable: 8 x 2\n# Key:     State [8]\n  State                        ARIMA(adjTakings ~ season() + trend(knot = year…¹\n  <chr>                                                                  <model>\n1 Australian Capital Territory                       <LM w/ ARIMA(1,0,0) errors>\n2 New South Wales                          <LM w/ ARIMA(1,0,0)(0,0,1)[4] errors>\n3 Northern Territory                       <LM w/ ARIMA(0,0,1)(1,0,0)[4] errors>\n4 Queensland                               <LM w/ ARIMA(1,0,0)(0,0,1)[4] errors>\n5 South Australia                          <LM w/ ARIMA(1,0,0)(1,0,0)[4] errors>\n6 Tasmania                                 <LM w/ ARIMA(0,0,1)(1,0,0)[4] errors>\n7 Victoria                                 <LM w/ ARIMA(1,0,0)(0,0,1)[4] errors>\n8 Western Australia                                  <LM w/ ARIMA(1,0,0) errors>\n# ℹ abbreviated name:\n#   ¹​`ARIMA(adjTakings ~ season() + trend(knot = yearquarter(\"2008 Q1\")))`\n```\n\n\n:::\n:::\n\n\nThe seasonal dummy variable has not adequately handled the seasonality, so there are seasonal ARIMA components.\n\n>    c. Check that the residuals of the model look like white noise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  filter(State == \"Victoria\") |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex4c-1.png){width=672}\n:::\n:::\n\n\nNo apparent problems. Similar plots needed for the other states.\n\n>    d. Forecast the takings for each state to the end of 2017. (Hint: You will need to produce forecasts of the CPI first.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# CPI forecasts\ncpif <- aus_accommodation |>\n  model(ARIMA(CPI)) |>\n  forecast(h = 6) |>\n  as_tsibble() |>\n  select(Date, State, CPI = .mean)\nfit |>\n  forecast(new_data = cpif) |>\n  mutate(Takings = adjTakings * CPI / 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A fable: 48 x 7 [1Q]\n# Key:     State, .model [8]\n   State                        .model                                      Date\n   <chr>                        <chr>                                      <qtr>\n 1 Australian Capital Territory \"ARIMA(adjTakings ~ season() + trend(kn… 2016 Q3\n 2 Australian Capital Territory \"ARIMA(adjTakings ~ season() + trend(kn… 2016 Q4\n 3 Australian Capital Territory \"ARIMA(adjTakings ~ season() + trend(kn… 2017 Q1\n 4 Australian Capital Territory \"ARIMA(adjTakings ~ season() + trend(kn… 2017 Q2\n 5 Australian Capital Territory \"ARIMA(adjTakings ~ season() + trend(kn… 2017 Q3\n 6 Australian Capital Territory \"ARIMA(adjTakings ~ season() + trend(kn… 2017 Q4\n 7 New South Wales              \"ARIMA(adjTakings ~ season() + trend(kn… 2016 Q3\n 8 New South Wales              \"ARIMA(adjTakings ~ season() + trend(kn… 2016 Q4\n 9 New South Wales              \"ARIMA(adjTakings ~ season() + trend(kn… 2017 Q1\n10 New South Wales              \"ARIMA(adjTakings ~ season() + trend(kn… 2017 Q2\n# ℹ 38 more rows\n# ℹ 4 more variables: adjTakings <dist>, .mean <dbl>, CPI <dbl>, Takings <dist>\n```\n\n\n:::\n:::\n\n\n>    e. What sources of uncertainty have not been taken into account in the prediction intervals?\n\n * The uncertainty in the CPI forecasts has been ignored.\n * As usual, the estimation of the parameters and the choice of models have also not been accounted for.\n\n\n# fpp3 10.7, Ex 5\n\n> We fitted a harmonic regression model to part of the `us_gasoline` series in Exercise 6 in Section 7.10. We will now revisit this model, and extend it to include more data and ARMA errors.\n>\n>   a. Using `TSLM()`, fit a harmonic regression with a piecewise linear time trend to the full `gasoline` series. Select the position of the knots in the trend and the appropriate number of Fourier terms to include by minimising the AICc or CV value.\n\nLet's optimize using 2 break points and an unknown number of Fourier terms. Because the number of Fourier terms is integer, we can't just use `optim`. Instead, we will loop over a large number of possible values for the breakpoints and Fourier terms. There are more than 2000 models fitted here, but `TSLM` is relatively fast.\n\nNote that the possible values of the knots must be restricted so that knot2 is always much larger than knot1. We have set them to be at least 2 years apart here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_gasoline |> autoplot(Barrels)\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex5a-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Function to compute CV given K and knots.\nget_cv <- function(K, knot1, knot2) {\n  us_gasoline |>\n    model(TSLM(Barrels ~ fourier(K = K) + trend(c(knot1, knot2)))) |>\n    glance() |>\n    pull(CV)\n}\n\nmodels <- expand.grid(\n  K = seq(25),\n  knot1 = yearweek(as.character(seq(1991, 2017, 2))),\n  knot2 = yearweek(as.character(seq(1991, 2017, 2)))\n) |>\n  filter(knot2 - knot1 > 104) |>\n  as_tibble()\nmodels <- models |>\n  mutate(cv = purrr::pmap_dbl(models, get_cv)) |>\n  arrange(cv)\n\n# Best combination\n(best <- head(models, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n      K    knot1    knot2     cv\n  <int>   <week>   <week>  <dbl>\n1     6 2007 W01 2013 W01 0.0641\n```\n\n\n:::\n\n```{.r .cell-code}\nfit <- us_gasoline |>\n  model(\n    TSLM(Barrels ~ fourier(K = best$K) + trend(c(best$knot1, best$knot2)))\n  )\n```\n:::\n\n\n>   b. Now refit the model using `ARIMA()` to allow for correlated errors, keeping the same predictor variables as you used with `TSLM()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- us_gasoline |>\n  model(ARIMA(Barrels ~ fourier(K = best$K) + trend(c(best$knot1, best$knot2)) + PDQ(0, 0, 0)))\nfit |> report()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Barrels \nModel: LM w/ ARIMA(1,0,1) errors \n\nCoefficients:\n         ar1      ma1  fourier(K = best$K)C1_52  fourier(K = best$K)S1_52\n      0.9277  -0.8414                   -0.1144                   -0.2306\ns.e.  0.0256   0.0357                    0.0133                    0.0132\n      fourier(K = best$K)C2_52  fourier(K = best$K)S2_52\n                        0.0418                    0.0309\ns.e.                    0.0105                    0.0105\n      fourier(K = best$K)C3_52  fourier(K = best$K)S3_52\n                        0.0836                    0.0343\ns.e.                    0.0097                    0.0097\n      fourier(K = best$K)C4_52  fourier(K = best$K)S4_52\n                        0.0187                    0.0399\ns.e.                    0.0094                    0.0094\n      fourier(K = best$K)C5_52  fourier(K = best$K)S5_52\n                       -0.0315                    0.0011\ns.e.                    0.0092                    0.0092\n      fourier(K = best$K)C6_52  fourier(K = best$K)S6_52\n                       -0.0523                    0.0001\ns.e.                    0.0092                    0.0092\n      trend(c(best$knot1, best$knot2))trend\n                                     0.0028\ns.e.                                 0.0001\n      trend(c(best$knot1, best$knot2))trend_831\n                                        -0.0051\ns.e.                                     0.0002\n      trend(c(best$knot1, best$knot2))trend_1144  intercept\n                                          0.0055     7.1065\ns.e.                                      0.0006     0.0352\n\nsigma^2 estimated as 0.06051:  log likelihood=-13.38\nAIC=64.76   AICc=65.33   BIC=163.78\n```\n\n\n:::\n:::\n\n\n>   c. Check the residuals of the final model using the `gg_tsdisplay()` function and a Ljung-Box test. Do they look sufficiently like white noise to continue? If not, try modifying your model, or removing the first few years of data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngg_tsresiduals(fit)\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex5c-1.png){width=672}\n:::\n\n```{.r .cell-code}\naugment(fit) |> features(.innov, ljung_box, dof = 2, lag = 24)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .model                                                       lb_stat lb_pvalue\n  <chr>                                                          <dbl>     <dbl>\n1 \"ARIMA(Barrels ~ fourier(K = best$K) + trend(c(best$knot1, …    23.6     0.369\n```\n\n\n:::\n:::\n\n\nThe model looks pretty good, although there is some heteroskedasticity. So the prediction intervals may not have accurate coverage.\n\n>   d. Once you have a model with white noise residuals, produce forecasts for the next year.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  forecast(h = \"1 year\") |>\n  autoplot(us_gasoline)\n```\n\n::: {.cell-output-display}\n![](ex12-sol_files/figure-html/ex5d-1.png){width=672}\n:::\n:::\n\n\n# fpp3 10.7, Ex 6\n\n> Electricity consumption is often modelled as a function of temperature. Temperature is measured by daily heating degrees and cooling degrees. Heating degrees is $18^\\circ$C minus the average daily temperature when the daily average is below $18^\\circ$C; otherwise it is zero.  This provides a measure of our need to heat ourselves as temperature falls.  Cooling degrees measures our need to cool ourselves as the temperature rises.  It is defined as the average daily temperature minus $18^\\circ$C when the daily average is above $18^\\circ$C; otherwise it is zero.  Let $y_t$ denote the monthly total of kilowatt-hours of electricity used, let $x_{1,t}$ denote the monthly total of heating degrees, and let $x_{2,t}$ denote the monthly total of cooling degrees.\n\n>   An analyst fits the following model to a set of such data:\n>   $$y^*_t = \\beta_1x^*_{1,t} + \\beta_2x^*_{2,t} + \\eta_t,$$\n>   where\n>   $$(1-\\Phi_{1}B^{12} - \\Phi_{2}B^{24})(1-B)(1-B^{12})\\eta_t = (1+\\theta_1 B)\\varepsilon_t$$\n>   and $y^*_t = \\log(y_t)$, $x^*_{1,t} = \\sqrt{x_{1,t}}$ and $x^*_{2,t}=\\sqrt{x_{2,t}}$.\n>\n>   a. What sort of ARIMA model is identified for $\\eta_t$?\n\nARIMA(0,1,1)(2,1,0)$_{12}$\n\n>   b. The estimated coefficients are\n>\n>   | Parameter   | Estimate   | s.e.       | $Z$        | $P$-value  |\n>   | :---------- | --------: | :--------: | --------: | :--------: |\n>   | $\\beta_1$       | 0.0077     | 0.0015     | 4.98       | 0.000      |\n>   | $\\beta_2$       | 0.0208     | 0.0023     | 9.23       | 0.000      |\n>   | $\\theta_1$  | 0.5830     | 0.0720     | 8.10       | 0.000      |\n>   | $\\Phi_{1}$ | -0.5373    | 0.0856     | -6.27      | 0.000      |\n>   | $\\Phi_{2}$ | -0.4667    | 0.0862     | -5.41      | 0.000      |\n>\n>   Explain what the estimates of $\\beta_1$ and $\\beta_2$ tell us about electricity consumption.\n\n$b_1$ is the unit increase in $y_t^*$ when $x_{1,t}^*$ increases by 1.  This is a little hard to interpret, but it is clear that monthly total electricity usage goes up when monthly heating degrees goes up. Similarly, for $b_2$, monthly total electricty usage goes up when monthly cooling degrees goes up.\n\n>   c. Write the equation in a form more suitable for forecasting.\n\nThis turned out to be way more messy than I expected, but for what it's worth, here it is in all its ugliness.\n\nFirst apply the differences to the regression equation.\n$$\n(1-B)(1-B^{12}) y_t^* = b_1^*(1-B)(1-B^{12})x_{1,t}^* + b_2^*(1-B)(1-B^{12})x_{2,t}^* + (1-B)(1-B^{12})\\eta_{t}\n$$\nSo\n\\begin{align*}\n(y^*_{t} - y^*_{t-1} - y^*_{t-12} +y^*_{t-13})\n =& b_1(x^*_{1,t} - x^*_{1,t-1} - x^*_{1,t-12} + x^*_{1,t-13})\n + b_2(x^*_{2,t} - x^*_{2,t-1} - x^*_{2,t-12} + x^*_{2,t-13}) + \\eta'_t\n\\end{align*}\nMultiplying by the AR polynomial gives\n\\begin{align*}\n(y^*_{t} & - y^*_{t-1} - y^*_{t-12} +y^*_{t-13})\n-\\Phi_{1}(y^*_{t-12} - y^*_{t-13} - y^*_{t-24} +y^*_{t-25})\n-\\Phi_{2}(y^*_{t-24} - y^*_{t-25} - y^*_{t-36} +y^*_{t-37})\\\\\n = & ~ b_1(x^*_{1,t} - x^*_{1,t-1} - x^*_{1,t-12} + x^*_{1,t-13})\n -\\Phi_{1}b_1(x^*_{1,t-12} - x^*_{1,t-13} - x^*_{1,t-24} + x^*_{1,t-25})\n -\\Phi_{2}b_1(x^*_{1,t-24} - x^*_{1,t-25} - x^*_{1,t-36} + x^*_{1,t-37}) \\\\\n & ~ + b_2(x^*_{2,t} - x^*_{2,t-1} - x^*_{2,t-12} + x^*_{2,t-13})\n  - \\Phi_{1}b_2(x^*_{2,t-12} - x^*_{2,t-13} - x^*_{2,t-24} + x^*_{2,t-25})\n  - \\Phi_{2}b_2(x^*_{2,t-24} - x^*_{2,t-25} - x^*_{2,t-36} + x^*_{2,t-37}) \\\\\n & ~ + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}.\n\\end{align*}\nFinally, we move all but $y_t^*$ to the right hand side:\n\\begin{align*}\ny^*_{t} =  & ~ y^*_{t-1} + y^*_{t-12} - y^*_{t-13}\n+\\Phi_{1}(y^*_{t-12} - y^*_{t-13} - y^*_{t-24} +y^*_{t-25})\n+\\Phi_{2}(y^*_{t-24} - y^*_{t-25} - y^*_{t-36} +y^*_{t-37})\\\\\n & + b_1(x^*_{1,t} - x^*_{1,t-1} - x^*_{1,t-12} + x^*_{1,t-13})\n -\\Phi_{1}b_1(x^*_{1,t-12} - x^*_{1,t-13} - x^*_{1,t-24} + x^*_{1,t-25})\n -\\Phi_{2}b_1(x^*_{1,t-24} - x^*_{1,t-25} - x^*_{1,t-36} + x^*_{1,t-37}) \\\\\n & + b_2(x^*_{2,t} - x^*_{2,t-1} - x^*_{2,t-12} + x^*_{2,t-13})\n  - \\Phi_{1}b_2(x^*_{2,t-12} - x^*_{2,t-13} - x^*_{2,t-24} + x^*_{2,t-25})\n  - \\Phi_{2}b_2(x^*_{2,t-24} - x^*_{2,t-25} - x^*_{2,t-36} + x^*_{2,t-37}) \\\\\n &  + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}.\n\\end{align*}\n\n>   d. Describe how this model could be used to forecast electricity demand for the next 12 months.\n\nFor $t=T+1$, we use the above equation to find a point forecast of $y_{T+1}^*$, setting $\\varepsilon_{T+1}=0$ and $\\hat{\\varepsilon}_T$ to the last residual. The actual $y_t^*$ values are all known, as are all the $x_{1,t}^*$ and $x_{2,t}^*$ values up to time $t=T$. For $x_{1,T+1}^*$ and $x_{2,T+1}^*$, we could use a forecast (for example, a seasonal naive forecast).\n\nFor $t=T+2,\\dots,T+12$, we do something similar, but both $\\varepsilon$ values are set to 0 and $y^*_{T+k}$ ($k\\ge1$) is replaced by the forecasts just calculated.\n\n>   e. Explain why the $\\eta_t$ term should be modelled with an ARIMA model rather than modelling the data using a standard regression package.  In your discussion, comment on the properties of the estimates, the validity of the standard regression results, and the importance of the $\\eta_t$ model in producing forecasts.\n\n* The non-stationarity of $\\eta_t$ means the coefficients in the regression model will be inconsistent if OLS is used.\n* The standard errors will be incorrectly computed.\n* Which means all the p-values will be wrong.\n* Using an ARIMA structure for $\\eta_t$ allows these problems to be corrected, plus the short-term forecasts will be more accurate.\n",
    "supporting": [
      "ex12-sol_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}