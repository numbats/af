{
  "hash": "46d4348c972e95d79fdf574288cfe671",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercise Week 5: Solutions\"\n---\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fpp3)\n```\n:::\n\n\n# fpp3 5.10, Ex 8\n\n> Consider the number of pigs slaughtered in New South Wales (data set `aus_livestock`).\n>\n>   a. Produce some plots of the data in order to become familiar with it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsw_pigs <- aus_livestock |>\n  filter(State == \"New South Wales\", Animal == \"Pigs\")\nnsw_pigs |>\n  autoplot(Count)\n```\n\n::: {.cell-output-display}\n![](ex5-sol_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nData generally follows a downward trend, however there are some periods where the amount of pigs slaughtered changes rapidly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsw_pigs |> gg_season(Count, labels = \"right\")\n```\n\n::: {.cell-output-display}\n![](ex5-sol_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nnsw_pigs |> gg_subseries(Count)\n```\n\n::: {.cell-output-display}\n![](ex5-sol_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\nSome seasonality is apparent, with notable increases in December and decreases during January, February and April.\n\n>   b. Create a training set of 486 observations, withholding a test set of 72 observations (6 years).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsw_pigs_train <- nsw_pigs |> slice(1:486)\n```\n:::\n\n\n>   c. Try using various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- nsw_pigs_train |>\n  model(\n    mean = MEAN(Count),\n    naive = NAIVE(Count),\n    snaive = SNAIVE(Count),\n    drift = RW(Count ~ drift())\n  )\nfit |>\n  forecast(h = \"6 years\") |>\n  accuracy(nsw_pigs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 12\n  .model Animal State       .type      ME   RMSE    MAE    MPE  MAPE  MASE RMSSE\n  <chr>  <fct>  <fct>       <chr>   <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 drift  Pigs   New South … Test   -4685.  8091.  6967.  -7.36  10.1 0.657 0.557\n2 mean   Pigs   New South … Test  -39360. 39894. 39360. -55.9   55.9 3.71  2.75 \n3 naive  Pigs   New South … Test   -6138.  8941.  7840.  -9.39  11.4 0.740 0.615\n4 snaive Pigs   New South … Test   -5838. 10111.  8174.  -8.81  11.9 0.771 0.696\n# ℹ 1 more variable: ACF1 <dbl>\n```\n\n\n:::\n:::\n\n\nThe drift method performed best for all measures of accuracy (although it had a larger first order auto-correlation)\n\n>   d. Check the residuals of your preferred method. Do they resemble white noise?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  select(drift) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex5-sol_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThe residuals do not appear to be white noise as the ACF plot contains many significant lags. It is also clear that the seasonal component is not captured by the drift method, as there exists a strong positive auto-correlation at lag 12 (1 year). The histogram appears to have a slightly long left tail.\n\n# fpp3 5.10, Ex 9\n\n>    a. Create a training set for household wealth (`hh_budget`) by withholding the last four years as a test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- hh_budget |>\n  filter(Year <= max(Year) - 4)\n```\n:::\n\n\n>    b. Fit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- train |>\n  model(\n    naive = NAIVE(Wealth),\n    drift = RW(Wealth ~ drift()),\n    mean = MEAN(Wealth)\n  )\nfc <- fit |> forecast(h = 4)\n```\n:::\n\n\n>    c. Compute the accuracy of your forecasts. Which method does best?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc |>\n  accuracy(hh_budget) |>\n  arrange(Country, MASE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 × 11\n   .model Country   .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n   <chr>  <chr>     <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n 1 drift  Australia Test   29.1  35.5  29.1  7.23  7.23 1.73  1.48   0.210 \n 2 naive  Australia Test   34.7  41.5  34.7  8.64  8.64 2.06  1.73   0.216 \n 3 mean   Australia Test   35.7  42.3  35.7  8.89  8.89 2.12  1.76   0.216 \n 4 drift  Canada    Test   33.3  37.2  33.3  6.09  6.09 1.73  1.57  -0.229 \n 5 naive  Canada    Test   46.2  51.0  46.2  8.46  8.46 2.40  2.15  -0.0799\n 6 mean   Canada    Test   90.4  92.9  90.4 16.7  16.7  4.69  3.92  -0.0799\n 7 drift  Japan     Test   14.7  17.9  14.7  2.44  2.44 0.943 0.967 -0.229 \n 8 naive  Japan     Test   36.3  37.8  36.3  6.06  6.06 2.34  2.04  -0.534 \n 9 mean   Japan     Test  100.  101.  100.  16.8  16.8  6.45  5.46  -0.534 \n10 drift  USA       Test   75.9  76.2  75.9 12.7  12.7  2.88  2.43  -0.561 \n11 naive  USA       Test   82.1  82.5  82.1 13.8  13.8  3.12  2.63  -0.423 \n12 mean   USA       Test   82.9  83.3  82.9 13.9  13.9  3.15  2.65  -0.423 \n```\n\n\n:::\n\n```{.r .cell-code}\nfc |>\n  accuracy(hh_budget) |>\n  group_by(.model) |>\n  summarise(MASE = mean(MASE)) |>\n  ungroup() |>\n  arrange(MASE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  .model  MASE\n  <chr>  <dbl>\n1 drift   1.82\n2 naive   2.48\n3 mean    4.10\n```\n\n\n:::\n:::\n\n\nThe drift method is better for every country, and on average.\n\n>    d. Do the residuals from the best method resemble white noise?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  filter(Country == \"Australia\") |>\n  select(drift) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex5-sol_files/figure-html/ex94-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfit |>\n  filter(Country == \"Canada\") |>\n  select(drift) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex5-sol_files/figure-html/ex94-2.png){width=672}\n:::\n\n```{.r .cell-code}\nfit |>\n  filter(Country == \"Japan\") |>\n  select(drift) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex5-sol_files/figure-html/ex94-3.png){width=672}\n:::\n\n```{.r .cell-code}\nfit |>\n  filter(Country == \"USA\") |>\n  select(drift) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex5-sol_files/figure-html/ex94-4.png){width=672}\n:::\n:::\n\n\nIn all cases, the residuals look like white noise.\n\n# fpp3 5.10, Ex 10\n\n>    a. Create a training set for Australian takeaway food turnover (`aus_retail`) by withholding the last four years as a test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntakeaway <- aus_retail |>\n  filter(Industry == \"Takeaway food services\") |>\n  summarise(Turnover = sum(Turnover))\ntrain <- takeaway |>\n  filter(Month <= max(Month) - 4 * 12)\n```\n:::\n\n\n>    b. Fit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- train |>\n  model(\n    naive = NAIVE(Turnover),\n    drift = RW(Turnover ~ drift()),\n    mean = MEAN(Turnover),\n    snaive = SNAIVE(Turnover)\n  )\nfc <- fit |> forecast(h = \"4 years\")\n```\n:::\n\n\n>    c. Compute the accuracy of your forecasts. Which method does best?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc |>\n  accuracy(takeaway) |>\n  arrange(MASE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  <chr>  <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 naive  Test  -12.4  119.  96.4 -1.49  6.66  2.30  2.25 0.613\n2 drift  Test  -93.7  130. 108.  -6.82  7.67  2.58  2.46 0.403\n3 snaive Test  177.   192. 177.  11.7  11.7   4.22  3.64 0.902\n4 mean   Test  829.   838. 829.  55.7  55.7  19.8  15.8  0.613\n```\n\n\n:::\n:::\n\n\nThe naive method is best here.\n\n>    d. Do the residuals from the best method resemble white noise?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  select(naive) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex5-sol_files/figure-html/ex104-1.png){width=672}\n:::\n:::\n\n\nThis is far from white noise. There is strong seasonality and increasing variance that has not been accounted for by the naive model.\n\n# fpp3 5.10, Ex 12\n\n> `tourism` contains quarterly visitor nights (in thousands) from 1998 to 2017 for 76 regions of Australia.\n>\n>   a. Extract data from the Gold Coast region using `filter()` and aggregate total overnight trips (sum over `Purpose`) using `summarise()`. Call this new dataset `gc_tourism`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngc_tourism <- tourism |>\n  filter(Region == \"Gold Coast\") |>\n  summarise(Trips = sum(Trips))\ngc_tourism\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tsibble: 80 x 2 [1Q]\n   Quarter Trips\n     <qtr> <dbl>\n 1 1998 Q1  827.\n 2 1998 Q2  681.\n 3 1998 Q3  839.\n 4 1998 Q4  820.\n 5 1999 Q1  987.\n 6 1999 Q2  751.\n 7 1999 Q3  822.\n 8 1999 Q4  914.\n 9 2000 Q1  871.\n10 2000 Q2  780.\n# ℹ 70 more rows\n```\n\n\n:::\n:::\n\n\n>   b. Using `slice()` or `filter()`, create three training sets for this data excluding the last 1, 2 and 3 years. For example, `gc_train_1 <- gc_tourism |> slice(1:(n()-4))`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngc_train_1 <- gc_tourism |> slice(1:(n() - 4))\ngc_train_2 <- gc_tourism |> slice(1:(n() - 8))\ngc_train_3 <- gc_tourism |> slice(1:(n() - 12))\n```\n:::\n\n\n>   c. Compute one year of forecasts for each training set using the seasonal naïve (`SNAIVE()`) method. Call these `gc_fc_1`, `gc_fc_2` and `gc_fc_3`, respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngc_fc <- bind_cols(\n  gc_train_1 |> model(gc_fc_1 = SNAIVE(Trips)),\n  gc_train_2 |> model(gc_fc_2 = SNAIVE(Trips)),\n  gc_train_3 |> model(gc_fc_3 = SNAIVE(Trips))\n) |> forecast(h = \"1 year\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngc_fc |> autoplot(gc_tourism)\n```\n\n::: {.cell-output-display}\n![](ex5-sol_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n>   d. Use `accuracy()` to compare the test set forecast accuracy using MAPE. Comment on these.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngc_fc |> accuracy(gc_tourism)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  .model  .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE   ACF1\n  <chr>   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1 gc_fc_1 Test   75.1 167.  154.   6.36 15.1  2.66  2.36  -0.410\n2 gc_fc_2 Test   12.0  43.1  39.5  1.14  4.32 0.670 0.599 -0.792\n3 gc_fc_3 Test   35.8  91.4  83.9  3.56  9.07 1.46  1.30   0.239\n```\n\n\n:::\n:::\n\n\nThe second set of forecasts are most accurate (as can be seen in the previous plot), however this is likely due to chance.\n",
    "supporting": [
      "ex5-sol_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}