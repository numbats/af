{
  "hash": "4cca80111e3a0d962da95ea28464ad55",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Activities: Week 5\"\n---\n\nParticipate in these workshop questions here: <https://partici.fi/61758592>\n\n## Household budget forecasting\n\n1. Create a training set for household wealth (`hh_budget`) by withholding the last four years as a test set.\n2. Fit all the appropriate benchmark methods to the training set and forecast the periods covered by the test set.\n3. Compute the accuracy of your forecasts. Which method does best?\n4. Do the residuals from the best method resemble white noise?\n\n<!-- This is a fairly straightforward exercise, giving students some practice in the steps to follow when comparing forecasting methods. The drift method is better for every country and on average. In all cases the residuals from the drift model resemble white noise.\n-->\n\n\n::: {.cell}\n\n:::\n\n\n## True or false?\n\n1. Good forecast methods should have normally distributed residuals.\n2. A model with small residuals will give good forecasts.\n3. The best measure of forecast accuracy is MAPE.\n4. If your model doesn’t forecast well, you should make it more complicated.\n5. Always choose the model with the best forecast accuracy as measured on the test set.\n\n\n<!-- > Are the following statements true or false? Explain your answer.\n>\n>   a. Good forecast methods should have normally distributed residuals.\n\n- False.\n- Although many good forecasting methods produce normally distributed residuals this is not required to produce good forecasts.\n- When the residuals are not normally distributed, the calculation of prediction intervals becomes more difficult.\n\n>   b. A model with small residuals will give good forecasts.\n\n- False.\n- It is possible to produce a model with small residuals by making a highly complicated (overfitted) model that fits the data extremely well.\n- This highly complicated model will often perform very poorly when forecasting new data.\n\n>   c. The best measure of forecast accuracy is MAPE.\n\n- False.\n- There is no single best measure of accuracy - often you would want to see a collection of accuracy measures as they can reveal different things about your residuals. MAPE in particular has some substantial disadvantages - extreme values can result when $y_t$ is close to zero, and it assumes that the unit being measured has a meaningful zero.\n\n>   d. If your model doesn’t forecast well, you should make it more complicated.\n\n- False.\n- There are many reasons why a model may not forecast well, and making the model more complicated can make the forecasts worse.\n- The model specified should capture structures that are evident in the data. Although adding terms that are unrelated to the structures found in the data will improve the model's residuals, the forecasting performance of the model will not improve.\n- Adding missing features relevant to the data (such as including a seasonal pattern that exists in the data) should improve forecast performance.\n\n>   e. Always choose the model with the best forecast accuracy as measured on the test set.\n\n- False.\n- For a start, the test set may be too small to draw any conclusions. Also, there are many measures of forecast accuracy, and they may suggest different models. The appropriate model is the one which is best suited to the forecasting task. For instance, you may be interested in choosing a model which forecasts well for predictions exactly one year ahead. In this case, using cross-validated accuracy could be a more useful approach to evaluating accuracy. -->\n\n\n## 2024 Exam part A\n\nDiscuss topics A2, A3, and A6\n\n<!--\n2.  *The Ljung-Box test is useful for selecting a good forecasting model.*\n\n    - The Ljung-Box test is used to test whether the residuals from a model are white noise.\n    - It should not be used to select a forecasting model.\n    - If the residuals are not white noise, then the model is not capturing all the information in the data, and can potentially be improved.\n    - A model that has been over-fitted may pass a Ljung-Box test, but be a poor forecasting model.\n    - It may not be possible to find a model that passes the Ljung-Box test, especially with a long time series. But the forecasts from a model that fails the Ljung-Box test may still be good.\n\n3.  *The MAPE is better than the RMSE for measuring forecast accuracy because it is easier to explain.*\n\n    - The MAPE is a percentage, so it is easier to interpret than the RMSE.\n    - However, the MAPE may not make sense for the data, especially if the data contains zeros, small values, or negative values, or if the zero point is arbitrary.\n    - The RMSE is scale-dependent so can't be used to compare forecasts across different data sets, whereas the MAPE is scale-independent.\n    - The RMSE is optimized by the mean, so finding a model that minimizes the RMSE will give the best point forecasts.\n    - The MAPE is not optimal for any meaningful characteristic of the forecasts.\n\n8.  *STL is not useful for forecasting because it only provides a decomposition of the data.*\n\n    - STL is a decomposition method that separates a time series into trend, seasonal, and remainder components.\n    - STL, on its own, is not a forecasting method.\n    - However, STL is useful for forecasting because the decomposition can help us understand the data better.\n    - STL can be coupled with a forecasting method applied to the seasonally adjusted data.\n    - In this case, the seasonal component is usually forecast using the seasonal naive method, and then the seasonal component forecasts and seasonally adjusted forecasts can be added to obtain forecasts of the original data.\n\n -->\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}