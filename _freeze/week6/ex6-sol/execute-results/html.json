{
  "hash": "4b0e06e9d766edd4c96842ec6aa0e014",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercise Week 6: Solutions\"\nexecute:\n  echo: true\n  cache: true\n  warning: false\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fpp3)\n```\n:::\n\n\n# fpp3 8.8, Ex1\n\n> Consider the the number of pigs slaughtered in Victoria, available in the `aus_livestock` dataset.\n\n>   a. Use the `ETS()` function in R to estimate the equivalent model for simple exponential smoothing. Find the optimal values of $\\alpha$ and $\\ell_0$, and generate forecasts for the next four months.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- aus_livestock |>\n  filter(Animal == \"Pigs\", State == \"Victoria\") |>\n  model(ses = ETS(Count ~ error(\"A\") + trend(\"N\") + season(\"N\")))\nreport(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Count \nModel: ETS(A,N,N) \n  Smoothing parameters:\n    alpha = 0.3221247 \n\n  Initial states:\n     l[0]\n 100646.6\n\n  sigma^2:  87480760\n\n     AIC     AICc      BIC \n13737.10 13737.14 13750.07 \n```\n\n\n:::\n:::\n\n\nOptimal values are $\\alpha = 0.3221247$  and $\\ell_0 = 100646.6$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc <- fit |> forecast(h = \"4 months\")\nfc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A fable: 4 x 6 [1M]\n# Key:     Animal, State, .model [1]\n  Animal State    .model    Month\n  <fct>  <fct>    <chr>     <mth>\n1 Pigs   Victoria ses    2019 Jan\n2 Pigs   Victoria ses    2019 Feb\n3 Pigs   Victoria ses    2019 Mar\n4 Pigs   Victoria ses    2019 Apr\n# ℹ 2 more variables: Count <dist>, .mean <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\nfc |>\n  autoplot(filter(aus_livestock, Month >= yearmonth(\"2010 Jan\")))\n```\n\n::: {.cell-output-display}\n![](ex6-sol_files/figure-html/ex1b-1.png){width=672}\n:::\n:::\n\n\n>   b. Compute a 95% prediction interval for the first forecast using $\\hat{y} \\pm 1.96s$ where $s$ is the standard deviation of the residuals. Compare your interval with the interval produced by R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns <- augment(fit) |>\n  pull(.resid) |>\n  sd()\nyhat <- fc |>\n  pull(.mean) |>\n  head(1)\nyhat + c(-1, 1) * 1.96 * s\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  76871.01 113502.10\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc |>\n  head(1) |>\n  mutate(interval = hilo(Count, 95)) |>\n  pull(interval)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<hilo[1]>\n[1] [76854.79, 113518.3]95\n```\n\n\n:::\n:::\n\n\nThe intervals are close but not identical. This is because R estimates the variance of the residuals differently, taking account of the degrees of freedom properly (and also using a more accurate critical value rather than just 1.96).\n\nTry the following.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- augment(fit) |> pull(.resid)\ns <- sqrt(sum(res^2) / (length(res) - NROW(tidy(fit))))\nyhat + c(-1, 1) * qnorm(0.975) * s\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  76854.79 113518.33\n```\n\n\n:::\n:::\n\n\n# fpp3 8.8, Ex2\n\n> Write your own function to implement simple exponential smoothing. The function should take arguments `y` (the response data), `alpha` (the smoothing parameter $\\alpha$) and `level` (the initial level $\\ell_0$). It should return the forecast of the next observation in the series. Does it give the same forecast as `ETS()`?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_ses <- function(y, alpha, level) {\n  yhat <- numeric(length(y) + 1)\n  yhat[1] <- level\n  for (i in 2:(length(yhat))) {\n    yhat[i] <- alpha * y[i - 1] + (1 - alpha) * yhat[i - 1]\n  }\n  return(last(yhat))\n}\n\nvic_pigs_vec <- aus_livestock |>\n  filter(Animal == \"Pigs\", State == \"Victoria\") |>\n  pull(Count)\n\nses_fc <- vic_pigs_vec |>\n  my_ses(alpha = 0.3221, level = 100646.6)\n\nc(my_ses = ses_fc, fable = fc$.mean[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  my_ses    fable \n95186.59 95186.56 \n```\n\n\n:::\n:::\n\n\nYes, the same forecasts are obtained. The slight differences are due to rounding of $\\alpha$ and $\\ell_0$.\n\n# fpp3 8.8, Ex3\n\n> Modify your function from the previous exercise to return the sum of squared errors rather than the forecast of the next observation. Then use the `optim()` function to find the optimal values of $\\alpha$ and $\\ell_0$. Do you get the same values as the `ETS()` function?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_ses_sse <- function(par, y) {\n  alpha <- par[1]\n  level <- par[2]\n  n <- length(y)\n  yhat <- numeric(n)\n  yhat[1] <- level\n  for (i in 2:n) {\n    yhat[i] <- alpha * y[i - 1] + (1 - alpha) * yhat[i - 1]\n  }\n  return(sum((y - yhat)^2))\n}\n\noptim(c(0.1, vic_pigs_vec[1]), my_ses_sse, y = vic_pigs_vec)$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.220344e-01 1.005254e+05\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  Animal State    .model term    estimate\n  <fct>  <fct>    <chr>  <chr>      <dbl>\n1 Pigs   Victoria ses    alpha      0.322\n2 Pigs   Victoria ses    l[0]  100647.   \n```\n\n\n:::\n:::\n\n\nSimilar, but not identical estimates. This is due to different starting values being used.\n\n# fpp3 8.8, Ex4\n\n> Combine your previous two functions to produce a function that both finds the optimal values of $\\alpha$ and $\\ell_0$, and produces a forecast of the next observation in the series.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_ses <- function(y) {\n  par <- optim(c(0.1, y[1]), my_ses_sse, y = y)$par\n  alpha <- par[1]\n  level <- par[2]\n  yhat <- numeric(length(y) + 1)\n  yhat[1] <- level\n  for (i in 2:(length(yhat))) {\n    yhat[i] <- alpha * y[i - 1] + (1 - alpha) * yhat[i - 1]\n  }\n  return(last(yhat))\n}\nmy_ses(vic_pigs_vec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 95186.69\n```\n\n\n:::\n:::\n\n\n# fpp3 8.8, Ex5\n\n>  Data set `global_economy` contains the annual Exports from many countries. Select one country to analyse.\n>\n>    a. Plot the Exports series and discuss the main features of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_economy |>\n  filter(Country == \"Argentina\") |>\n  autoplot(Exports)\n```\n\n::: {.cell-output-display}\n![](ex6-sol_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThere is a huge jump in Exports in 2002, due to the deregulation of the Argentinian peso. Since then, Exports (as a percentage of GDP) has gradually returned to 1990 levels.\n\n>    b. Use an ETS(A,N,N) model to forecast the series, and plot the forecasts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\netsANN <- global_economy |>\n  filter(Country == \"Argentina\") |>\n  model(ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")))\netsANN |>\n  forecast(h = 10) |>\n  autoplot(global_economy)\n```\n\n::: {.cell-output-display}\n![](ex6-sol_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n>    c. Compute the RMSE values for the training data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy(etsANN) |> select(RMSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n   RMSE\n  <dbl>\n1  2.78\n```\n\n\n:::\n:::\n\n\n>    d. Compare the results to those from an ETS(A,A,N) model. (Remember that the trended model is using one more parameter than the simpler model.) Discuss the merits of the two forecasting methods for this data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- global_economy |>\n  filter(Country == \"Argentina\") |>\n  model(\n    ses = ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    holt = ETS(Exports ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n  )\naccuracy(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 11\n  Country   .model .type         ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n  <fct>     <chr>  <chr>      <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 Argentina ses    Training 0.0762   2.78  1.62 -1.73  15.7 0.983 0.986 0.00902\n2 Argentina holt   Training 0.00795  2.78  1.64 -2.51  15.9 0.994 0.986 0.0271 \n```\n\n\n:::\n:::\n\n\nThere is very little difference in training RMSE between these models. So the extra parameter is not doing much.\n\n>    e. Compare the forecasts from both methods. Which do you think is best?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  forecast(h = 10) |>\n  autoplot(global_economy)\n```\n\n::: {.cell-output-display}\n![](ex6-sol_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n- The forecasts are similar. In this case, the *simpler* model is preferred.\n\n>    f. Calculate a 95% prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using R.\n\n1.  standard error. (from RMSE)\n2.  mean (from forecast)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns <- accuracy(fit) |> pull(RMSE)\nyhat <- forecast(fit, h = 1) |> pull(.mean)\n# SES\nyhat[1] + c(-1, 1) * qnorm(0.975) * s[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5.882074 16.764136\n```\n\n\n:::\n\n```{.r .cell-code}\n# Holt\nyhat[2] + c(-1, 1) * qnorm(0.975) * s[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5.989515 16.872908\n```\n\n\n:::\n\n```{.r .cell-code}\nfit |>\n  forecast(h = 1) |>\n  mutate(PI = hilo(Exports, level = 95))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A fable: 2 x 6 [1Y]\n# Key:     Country, .model [2]\n  Country   .model  Year\n  <fct>     <chr>  <dbl>\n1 Argentina ses     2018\n2 Argentina holt    2018\n# ℹ 3 more variables: Exports <dist>, .mean <dbl>, PI <hilo>\n```\n\n\n:::\n:::\n\n\n-   Using RMSE yields narrower prediction interval while using the values from\n    `hilo()` function gives wider prediction interval.\n\n-   Using RMSE has failed to take account of the degrees of freedom for each\n    model. Compare the following\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse <- augment(fit) |>\n  as_tibble() |>\n  group_by(.model) |>\n  summarise(s = sum(.resid^2)) |>\n  pull(s)\n\nn <- global_economy |>\n  filter(Country == \"Argentina\") |>\n    nrow()\n\n# sse method= alpha, level=> 2\n# holt linear = alpha, level, trend, b => 4\n\ns <- sqrt(sse / (n - c(2, 4)))\n\n# SES\nyhat[1] + c(-1, 1) * qnorm(0.975) * s[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5.785088 16.861122\n```\n\n\n:::\n\n```{.r .cell-code}\n# Holt\nyhat[2] + c(-1, 1) * qnorm(0.975) * s[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5.79226 17.07016\n```\n\n\n:::\n:::\n\n\n# fpp3 8.8, Ex6\n\n> Forecast the Chinese GDP from the `global_economy` data set using an ETS model. Experiment with the various options in the `ETS()` function to see how much the forecasts change with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each is doing to the forecasts.\n\n> [Hint: use `h=20` when forecasting, so you can clearly see the differences between the various options when plotting the forecasts.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchina <- global_economy |>\n  filter(Country == \"China\")\nchina |> autoplot(GDP)\n```\n\n::: {.cell-output-display}\n![](ex6-sol_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n-   It clearly needs a relatively strong transformation due to the increasing\n    variance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchina |> autoplot(box_cox(GDP, 0.2))\n```\n\n::: {.cell-output-display}\n![](ex6-sol_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nchina |> features(GDP, guerrero)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  Country lambda_guerrero\n  <fct>             <dbl>\n1 China           -0.0345\n```\n\n\n:::\n:::\n\n\n-   Making $\\lambda=0.2$ looks ok.\n\n-   The Guerrero method suggests an even stronger transformation. Let's also try\n    a log.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- china |>\n  model(\n    ets = ETS(GDP),\n    ets_damped = ETS(GDP ~ trend(\"Ad\")),\n    ets_bc = ETS(box_cox(GDP, 0.2)),\n    ets_log = ETS(log(GDP))\n  )\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A mable: 1 x 5\n# Key:     Country [1]\n  Country          ets    ets_damped       ets_bc      ets_log\n  <fct>        <model>       <model>      <model>      <model>\n1 China   <ETS(M,A,N)> <ETS(M,Ad,N)> <ETS(A,A,N)> <ETS(A,A,N)>\n```\n\n\n:::\n\n```{.r .cell-code}\naugment(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tsibble: 232 x 7 [1Y]\n# Key:       Country, .model [4]\n   Country .model  Year          GDP      .fitted        .resid   .innov\n   <fct>   <chr>  <dbl>        <dbl>        <dbl>         <dbl>    <dbl>\n 1 China   ets     1960 59716467625. 49001691297.  10714776328.  0.219  \n 2 China   ets     1961 50056868958. 66346643194. -16289774236. -0.246  \n 3 China   ets     1962 47209359006. 51607368186.  -4398009180. -0.0852 \n 4 China   ets     1963 50706799903. 47386494407.   3320305495.  0.0701 \n 5 China   ets     1964 59708343489. 51919091574.   7789251914.  0.150  \n 6 China   ets     1965 70436266147. 63350421234.   7085844913.  0.112  \n 7 China   ets     1966 76720285970. 76289186599.    431099371.  0.00565\n 8 China   ets     1967 72881631327. 82708375812.  -9826744486. -0.119  \n 9 China   ets     1968 70846535056. 75804820984.  -4958285928. -0.0654 \n10 China   ets     1969 79705906247. 72222259470.   7483646777.  0.104  \n# ℹ 222 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nfit |>\n  forecast(h = \"20 years\") |>\n  autoplot(china, level = NULL)\n```\n\n::: {.cell-output-display}\n![](ex6-sol_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n-   The transformations have a big effect, with small lambda values creating big\n    increases in the forecasts.\n-   The damping has relatively a small effect.\n\n# fpp3 8.8, Ex16\n\n> Show that the forecast variance for an ETS(A,N,N) model is given by\n$$\n\\sigma^2\\left[1+\\alpha^2(h-1)\\right].\n$$\n\nAn ETS(A,N,N) model is defined as\n  \\begin{align*}\n    y_t      & = \\ell_{t-1} + \\varepsilon_{t} \\\\\n    \\ell_{t} & = \\ell_{t-1} + \\alpha\\varepsilon_{t},\n  \\end{align*}\nwhere $\\varepsilon_t \\sim \\text{N}(0,\\sigma^2)$, and $h$-step forecasts are  given by\n$$\n \\hat{y}_{T+h|T} = \\ell_T.\n$$\nSo\n  \\begin{align*}\n     y_{T+h} & = \\ell_{T+h-1} + \\varepsilon_{T+h} \\\\\n             & = \\ell_{T+h-2} + \\alpha \\varepsilon_{T+h-1} +  \\varepsilon_{T+h} \\\\\n             & = \\ell_{T+h-3} + \\alpha \\varepsilon_{T+h-2}  + \\alpha \\varepsilon_{T+h-1} +  \\varepsilon_{T+h} \\\\\n             & \\dots \\\\\n             & = \\ell_{T} + \\alpha \\sum_{j=1}^{h-1} \\varepsilon_{T+h-j} +  \\varepsilon_{T+h}.\n  \\end{align*}\nTherefore\n  \\begin{align*}\n    \\text{Var}(y_{T+h} | y_1,\\dots,y_T) & = \\alpha^2 \\sum_{j=1}^{h-1} \\sigma^2 +  \\sigma^2 \\\\\n                                        & =  \\sigma^2\\left[ 1 + \\alpha^2 (h-1)\\right ].\n  \\end{align*}\n\n# fpp3 8.8, Ex17\n\n> Write down 95\\% prediction intervals for an ETS(A,N,N) model as a function of $\\ell_T$, $\\alpha$, $h$ and $\\sigma$, assuming normally distributed errors.\n\nUsing previous result:\n$$\n \\ell_T \\pm 1.96 \\sigma \\sqrt{ 1 + \\alpha^2 (h-1)}\n$$\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}