{
  "hash": "caeed6894e5eb7bbeeaf234d88594b43",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercise Week 4: Solutions\"\n---\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fpp3)\n```\n:::\n\n\n# fpp3 5.10, Ex 1\n\n> Produce forecasts for the following series using whichever of `NAIVE(y)`, `SNAIVE(y)` or `RW(y ~ drift())` is more appropriate in each case:\n>\n>   * Australian Population (`global_economy`)\n>   * Bricks (`aus_production`)\n>   * NSW Lambs (`aus_livestock`)\n>   * Household wealth (`hh_budget`)\n>   * Australian takeaway food turnover (`aus_retail`)\n\n## Australian population\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_economy |>\n  filter(Country == \"Australia\") |>\n  autoplot(Population)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nData has trend and no seasonality. Random walk with drift model is appropriate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_economy |>\n  filter(Country == \"Australia\") |>\n  model(RW(Population ~ drift())) |>\n  forecast(h = \"10 years\") |>\n  autoplot(global_economy)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Australian clay brick production\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_production |>\n  filter(!is.na(Bricks)) |>\n  autoplot(Bricks) +\n  labs(title = \"Clay brick production\")\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThis data appears to have more seasonality than trend, so of the models available, seasonal naive is most appropriate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_production |>\n  filter(!is.na(Bricks)) |>\n  model(SNAIVE(Bricks)) |>\n  forecast(h = \"5 years\") |>\n  autoplot(aus_production)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## NSW Lambs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsw_lambs <- aus_livestock |>\n  filter(State == \"New South Wales\", Animal == \"Lambs\")\nnsw_lambs |>\n  autoplot(Count)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThis data appears to have more seasonality than trend, so of the models available, seasonal naive is most appropriate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsw_lambs |>\n  model(SNAIVE(Count)) |>\n  forecast(h = \"5 years\") |>\n  autoplot(nsw_lambs)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Household wealth\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhh_budget |>\n  autoplot(Wealth)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nAnnual data with trend upwards, so we can use a random walk with drift.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhh_budget |>\n  model(RW(Wealth ~ drift())) |>\n  forecast(h = \"5 years\") |>\n  autoplot(hh_budget)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Australian takeaway food turnover\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntakeaway <- aus_retail |>\n  filter(Industry == \"Takeaway food services\") |>\n  summarise(Turnover = sum(Turnover))\ntakeaway |> autoplot(Turnover)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThis data has strong seasonality and strong trend, so we will use a seasonal naive model with drift.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntakeaway |>\n  model(SNAIVE(Turnover ~ drift())) |>\n  forecast(h = \"5 years\") |>\n  autoplot(takeaway)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThis is actually not one of the four benchmark methods discussed in the book, but is sometimes a useful benchmark when there is strong seasonality and strong trend.\n\nThe corresponding equation is\n$$\n  \\hat{y}_{T+h|T} = y_{T+h-m(k+1)} + \\frac{h}{T-m}\\sum_{t=m+1}^T(y_t - y_{t-m}),\n$$\nwhere $m=12$ and $k$ is the integer part of $(h-1)/m$ (i.e., the number of complete years in the forecast period prior to time $T+h$).\n\n# fpp3 5.10, Ex 2\n\n> Use the Facebook stock price (data set `gafa_stock`) to do the following:\n\n>   a. Produce a time plot of the series.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfb_stock <- gafa_stock |>\n  filter(Symbol == \"FB\")\nfb_stock |>\n  autoplot(Close)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\nAn upward trend is evident until mid-2018, after which the closing stock price drops.\n\n>   b. Produce forecasts using the drift method and plot them.\n\nThe data must be made regular before it can be modelled. We will use trading days as our regular index.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfb_stock <- fb_stock |>\n  mutate(trading_day = row_number()) |>\n  update_tsibble(index = trading_day, regular = TRUE)\n```\n:::\n\n\nTime to model a random walk with drift.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfb_stock |>\n  model(RW(Close ~ drift())) |>\n  forecast(h = 100) |>\n  autoplot(fb_stock)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n>   c. Show that the forecasts are identical to extending the line drawn between the first and last observations.\n\nProve drift methods are extrapolations from the first and last observation. First, we will demonstrate it graphically.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfb_stock |>\n  model(RW(Close ~ drift())) |>\n  forecast(h = 100) |>\n  autoplot(fb_stock) +\n  geom_line(\n    aes(y = Close),\n    linetype = \"dashed\", colour = \"blue\",\n    data = fb_stock |> filter(trading_day %in% range(trading_day))\n  )\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nTo prove it algebraically, note that\n\\begin{align*}\n \\hat{y}_{T+h|T} = y_T + h\\left(\\frac{y_T-y_1}{T-1}\\right)\n\\end{align*}\nwhich is a straight line with slope $(y_T-y_1)/(T-1)$ that goes through the point $(T,y_T)$.\n\nTherefore, it must also go through the point $(1,c)$ where\n$$\n  (y_T-c)/(T-1) = (y_T - y_1) / (T-1),\n$$\nso $c=y_1$.\n\n>   d. Try using some of the other benchmark functions to forecast the same data set. Which do you think is best? Why?\n\nUse other appropriate benchmark methods. The most appropriate benchmark method is the naive model. The mean forecast is terrible for this type of data, and the data is non-seasonal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfb_stock |>\n  model(NAIVE(Close)) |>\n  forecast(h = 100) |>\n  autoplot(fb_stock)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe naive method is most appropriate, and will also be best if the efficient market hypothesis holds true.\n\n# fpp3 5.10, Ex 3\n\n> Apply a seasonal na√Øve method to the quarterly Australian beer production data from 1992. Check if the residuals look like white noise, and plot the forecasts. The following code will help.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract data of interest\nrecent_production <- aus_production |>\n  filter(year(Quarter) >= 1992)\n# Define and estimate a model\nfit <- recent_production |> model(SNAIVE(Beer))\n# Look at the residuals\nfit |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n* The residuals are not centred around 0 (typically being slightly below it), this is due to the model failing to capture the negative trend in the data.\n* Peaks and troughs in residuals spaced roughly 4 observations apart are apparent leading to a negative spike at lag 4 in the ACF. So they do not resemble white noise. Lags 1 and 3 are also significant, however they are very close to the threshold and are of little concern.\n* The distribution of the residuals does not appear very normal, however it is probably close enough for the accuracy of our intervals (it being not centred on 0 is more concerning).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look at some forecasts\nfit |>\n  forecast() |>\n  autoplot(recent_production)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nThe forecasts look reasonable, although the intervals may be a bit wide. This is likely due to the slight trend not captured by the model (which subsequently violates the assumptions imposed on the residuals).\n\n# fpp3 5.10, Ex 4\n\n> Repeat the exercise for the Australian Exports series from `global_economy` and the Bricks series from `aus_production`. Use whichever of `NAIVE()` or `SNAIVE()` is more appropriate in each case.\n\n## Australian exports\n\nThe data does not contain seasonality, so the naive model is more appropriate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract data of interest\naus_exports <- filter(global_economy, Country == \"Australia\")\n# Define and estimate a model\nfit <- aus_exports |> model(NAIVE(Exports))\n# Check residuals\nfit |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n- The ACF plot reveals that the first lag exceeds the significance threshold.\n- This data may still be white noise, as it is the only lag that exceeds the blue dashed lines (5\\% of the lines are expected to exceed it). However as it is the first lag, it is probable that there exists some real auto-correlation in the residuals that can be modelled.\n- The distribution appears normal.\n- The residual plot appears mostly random, however more observations appear to be above zero. This again, is due to the model not capturing the trend.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look at some forecasts\nfit |>\n  forecast() |>\n  autoplot(aus_exports)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n- The forecasts appear reasonable as the series appears to have flattened in recent years.\n- The intervals are also reasonable --- despite the assumptions behind them having been violated.\n\n## Australian brick production\n\nThe data is seasonal, so the seasonal naive model is more appropriate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove the missing values at the end of the series\ntidy_bricks <- aus_production |>\n  filter(!is.na(Bricks))\n# Define and estimate a model\nfit <- tidy_bricks |>\n  model(SNAIVE(Bricks))\n# Look at the residuals\nfit |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n- The residual plot does not appear random. Periods of low production and high production are evident, leading to autocorrelation in the residuals.\n\n- The residuals from this model are not white noise. The ACF plot shows a strong sinusoidal pattern of decay, indicating that the residuals are auto-correlated.\n- The histogram is also not normally distributed, as it has a long left tail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look at some forecasts\nfit |>\n  forecast() |>\n  autoplot(tidy_bricks)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n- The point forecasts appear reasonable as the series appears to have flattened in recent years.\n- The intervals appear much larger than necessary.\n\n# fpp3 5.10, Ex 5\n\n> Produce forecasts for the 7 Victorian series in `aus_livestock` using `SNAIVE()`. Plot the resulting forecasts including the historical data. Is this a reasonable benchmark for these series?\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_livestock |>\n  filter(State == \"Victoria\") |>\n  model(SNAIVE(Count)) |>\n  forecast(h = \"5 years\") |>\n  autoplot(aus_livestock)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n- Most point forecasts look reasonable from the seasonal naive method.\n- Some series are more seasonal than others, and for the series with very weak seasonality it may be better to consider using a naive or drift method.\n- The prediction intervals in some cases go below zero, so perhaps a log transformation would have been better for these series.\n\n# fpp3 5.10, Ex 11\n\n> We will use the bricks data from `aus_production` (Australian quarterly clay brick production 1956--2005) for this exercise.\n>\n>   a. Use an STL decomposition to calculate the trend-cycle and seasonal indices. (Experiment with having fixed or changing seasonality.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_bricks <- aus_production |>\n  filter(!is.na(Bricks))\ntidy_bricks |>\n  model(STL(Bricks)) |>\n  components() |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nData is multiplicative, and so a transformation should be used.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndcmp <- tidy_bricks |>\n  model(STL(log(Bricks))) |>\n  components()\ndcmp |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nSeasonality varies slightly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndcmp <- tidy_bricks |>\n  model(stl = STL(log(Bricks) ~ season(window = \"periodic\"))) |>\n  components()\ndcmp |> autoplot()\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nThe seasonality looks fairly stable, so I've used a periodic season (window). The decomposition still performs well when the seasonal component is fixed. The remainder term does not appear to contain a substantial amount of seasonality.\n\n>   b. Compute and plot the seasonally adjusted data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndcmp |>\n  as_tsibble() |>\n  autoplot(season_adjust)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n>   c. Use a na√Øve method to produce forecasts of the seasonally adjusted data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- dcmp |>\n  select(-.model) |>\n  model(naive = NAIVE(season_adjust)) |>\n  forecast(h = \"5 years\")\ndcmp |>\n  as_tsibble() |>\n  autoplot(season_adjust) + autolayer(fit)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n>   d. Use `decomposition_model()` to reseasonalise the results, giving forecasts for the original data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- tidy_bricks |>\n  model(stl_mdl = decomposition_model(STL(log(Bricks)), NAIVE(season_adjust)))\nfit |>\n  forecast(h = \"5 years\") |>\n  autoplot(tidy_bricks)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n>   e. Do the residuals look uncorrelated?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nThe residuals do not appear uncorrelated as there are several lags of the ACF which exceed the significance threshold.\n\n>   f. Repeat with a robust STL decomposition. Does it make much difference?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_robust <- tidy_bricks |>\n  model(stl_mdl = decomposition_model(STL(log(Bricks)), NAIVE(season_adjust)))\n\nfit_robust |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\nThe residuals appear slightly less auto-correlated, however there is still significant auto-correlation at lag 8.\n\n>   g. Compare forecasts from `decomposition_model()` with those from `SNAIVE()`, using a test set comprising the last 2 years of data. Which is better?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_bricks_train <- tidy_bricks |>\n  slice(1:(n() - 8))\nfit <- tidy_bricks_train |>\n  model(\n    stl_mdl = decomposition_model(STL(log(Bricks)), NAIVE(season_adjust)),\n    snaive = SNAIVE(Bricks)\n  )\n\nfc <- fit |>\n  forecast(h = \"2 years\")\nfc |>\n  autoplot(tidy_bricks, level = NULL)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nThe decomposition forecasts appear to more closely follow the actual future data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc |>\n  accuracy(tidy_bricks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 √ó 10\n  .model  .type    ME  RMSE   MAE     MPE  MAPE  MASE RMSSE    ACF1\n  <chr>   <chr> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 snaive  Test  2.75   20    18.2  0.395   4.52 0.504 0.407 -0.0503\n2 stl_mdl Test  0.368  18.1  15.1 -0.0679  3.76 0.418 0.368  0.115 \n```\n\n\n:::\n:::\n\n\nThe STL decomposition forecasts are more accurate than the seasonal naive forecasts across all accuracy measures.\n\n\n# fpp3 7.10, Ex 2\n\n> Data set `olympic_running` contains the winning times (in seconds) in each Olympic Games sprint, middle-distance and long-distance track events from 1896 to 2016.\n>\n>   a. Plot the winning time against the year. Describe the main features of the plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nolympic_running |>\n  ggplot(aes(x = Year, y = Time, colour = Sex)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(~Length, scales = \"free_y\", nrow = 2) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\n  labs(y = \"Running time (seconds)\")\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\nThe running times are generally decreasing as time progresses (although the rate of this decline is slowing down in recent olympics). There are some missing values in the data corresponding to the World Wars (in which the Olympic Games were not held).\n\n>   b. Fit a regression line to the data. Obviously the winning times have been decreasing, but at what *average* rate per year?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- olympic_running |>\n  model(TSLM(Time ~ trend()))\ntidy(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 28 √ó 8\n   Length Sex   .model               term  estimate std.error statistic  p.value\n    <int> <chr> <chr>                <chr>    <dbl>     <dbl>     <dbl>    <dbl>\n 1    100 men   TSLM(Time ~ trend()) (Int‚Ä¶  11.1      0.0909     123.   1.86e-37\n 2    100 men   TSLM(Time ~ trend()) tren‚Ä¶  -0.0504   0.00479    -10.5  7.24e-11\n 3    100 women TSLM(Time ~ trend()) (Int‚Ä¶  12.4      0.163       76.0  4.58e-25\n 4    100 women TSLM(Time ~ trend()) tren‚Ä¶  -0.0567   0.00749     -7.58 3.72e- 7\n 5    200 men   TSLM(Time ~ trend()) (Int‚Ä¶  22.3      0.140      159.   4.06e-39\n 6    200 men   TSLM(Time ~ trend()) tren‚Ä¶  -0.0995   0.00725    -13.7  3.80e-13\n 7    200 women TSLM(Time ~ trend()) (Int‚Ä¶  25.5      0.525       48.6  8.34e-19\n 8    200 women TSLM(Time ~ trend()) tren‚Ä¶  -0.135    0.0227      -5.92 2.17e- 5\n 9    400 men   TSLM(Time ~ trend()) (Int‚Ä¶  50.3      0.445      113.   1.53e-36\n10    400 men   TSLM(Time ~ trend()) tren‚Ä¶  -0.258    0.0235     -11.0  2.75e-11\n# ‚Ñπ 18 more rows\n```\n\n\n:::\n:::\n\n\nThe men's 100 running time has been decreasing by an average of 0.013 seconds each year.<br>\nThe women's 100 running time has been decreasing by an average of 0.014 seconds each year.<br>\nThe men's 200 running time has been decreasing by an average of 0.025 seconds each year.<br>\nThe women's 200 running time has been decreasing by an average of 0.034 seconds each year.<br>\nThe men's 400 running time has been decreasing by an average of 0.065 seconds each year.<br>\nThe women's 400 running time has been decreasing by an average of 0.04 seconds each year.<br>\nThe men's 800 running time has been decreasing by an average of 0.152 seconds each year.<br>\nThe women's 800 running time has been decreasing by an average of 0.198 seconds each year.<br>\nThe men's 1500 running time has been decreasing by an average of 0.315 seconds each year.<br>\nThe women's 1500 running time has been increasing by an average of 0.147 seconds each year.<br>\nThe men's 5000 running time has been decreasing by an average of 1.03 seconds each year.<br>\nThe women's 5000 running time has been decreasing by an average of 0.303 seconds each year.<br>\nThe men's 10000 running time has been decreasing by an average of 2.666 seconds each year.<br>\nThe women's 10000 running time has been decreasing by an average of 3.496 seconds each year.<br>\n\n>   c. Plot the residuals against the year. What does this indicate about the suitability of the fitted line?\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(fit) |>\n  ggplot(aes(x = Year, y = .innov, colour = Sex)) +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(~Length, scales = \"free_y\", nrow = 2) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme(legend.position = \"bottom\", legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\nIt doesn't seem that the linear trend is appropriate for this data.\n\n>   d. Predict the winning time for each race in the 2020 Olympics. Give a prediction interval for your forecasts. What assumptions have you made in these calculations?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  forecast(h = 1) |>\n  mutate(PI = hilo(Time, 95)) |>\n  select(-.model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A fable: 14 x 6 [4Y]\n# Key:     Length, Sex [14]\n   Length Sex    Year\n    <int> <chr> <dbl>\n 1    100 men    2020\n 2    100 women  2020\n 3    200 men    2020\n 4    200 women  2020\n 5    400 men    2020\n 6    400 women  2020\n 7    800 men    2020\n 8    800 women  2020\n 9   1500 men    2020\n10   1500 women  2020\n11   5000 men    2020\n12   5000 women  2020\n13  10000 men    2020\n14  10000 women  2020\n# ‚Ñπ 3 more variables: Time <dist>, .mean <dbl>, PI <hilo>\n```\n\n\n:::\n:::\n\n\nUsing a linear trend we assume that winning times will decrease in a linear fashion which is unrealistic for running times. As we saw from the residual plots above there are mainly large positive residuals for the last few years, indicating that the decreases in the winning times are not linear. We also assume that the residuals are normally distributed.\n\n# fpp3 7.10, Ex 6\n\n> The annual population of Afghanistan is available in the `global_economy` data set.\n\n>    a. Plot the data and comment on its features. Can you observe the effect of the Soviet-Afghan war?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_economy |>\n  filter(Country == \"Afghanistan\") |>\n  autoplot(Population / 1e6) +\n  labs(y = \"Population (millions)\") +\n  geom_ribbon(aes(xmin = 1979.98, xmax = 1989.13), fill = \"pink\", alpha = 0.4) +\n  annotate(\"text\", x = 1984.5, y = 10, label = \"Soviet-Afghan war\", col = \"red\", size = 3)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\nThe population increases slowly from 1960 to 1980, then decreases during the Soviet-Afghan war (24 Dec 1979 -- 15 Feb 1989), and has increased relatively rapidly since then. The last 30 years has shown an almost linear increase in population.\n\n>    b. Fit a linear trend model and compare this to a piecewise linear trend model with knots at 1980 and 1989.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- global_economy |>\n  filter(Country == \"Afghanistan\") |>\n  model(\n    linear = TSLM(Population ~ trend()),\n    piecewise = TSLM(Population ~ trend(knots = c(1980, 1989)))\n  )\naugment(fit) |>\n  autoplot(.fitted) +\n  geom_line(aes(y = Population), colour = \"black\")\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\nThe fitted values show that the piecewise linear model has tracked the data very closely, while the linear model is inaccurate.\n\n>    c. Generate forecasts from these two models for the five years after the end of the data, and comment on the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc <- fit |> forecast(h = \"5 years\")\nautoplot(fc) +\n  autolayer(filter(global_economy |> filter(Country == \"Afghanistan\")), Population)\n```\n\n::: {.cell-output-display}\n![](ex4-sol_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\nThe linear model is clearly incorrect with prediction intervals too wide, and the point forecasts too low.\n\nThe piecewise linear model looks good, but the prediction intervals are probably too narrow. This model assumes that the trend since the last knot will continue unchanged, which is unrealistic.\n",
    "supporting": [
      "ex4-sol_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}