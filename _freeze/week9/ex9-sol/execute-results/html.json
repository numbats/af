{
  "hash": "b14ef85c97f8508bb906010b3836d090",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercise Week 9: Solutions\"\n---\n\n\n\n```r\nlibrary(fpp3)\n```\n\n# fpp3 9.11, Ex6\n\n> Simulate and plot some data from simple ARIMA models.\n>   a. Use the following R code to generate data from an AR(1) model with $\\phi_{1} = 0.6$ and $\\sigma^2=1$. The process starts with $y_1=0$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nar1 <- function(phi, n = 100L) {\n  y <- numeric(n)\n  e <- rnorm(n)\n  for (i in 2:n) {\n    y[i] <- phi * y[i - 1] + e[i]\n  }\n  tsibble(idx = seq_len(n), y = y, index = idx)\n}\n```\n:::\n\n\n>   b. Produce a time plot for the series. How does the plot change as you change $\\phi_1$?\n\nSome examples of changing $\\phi_1$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nar1(0.6) |> autoplot(y)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex6b-1.png){width=672}\n:::\n\n```{.r .cell-code}\nar1(0.95) |> autoplot(y)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex6b-2.png){width=672}\n:::\n\n```{.r .cell-code}\nar1(0.05) |> autoplot(y)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex6b-3.png){width=672}\n:::\n\n```{.r .cell-code}\nar1(-0.65) |> autoplot(y)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex6b-4.png){width=672}\n:::\n:::\n\n\n>   c. Write your own code to generate data from an MA(1) model with $\\theta_{1}  =  0.6$ and $\\sigma^2=1$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nma1 <- function(theta, n = 100L) {\n  y <- numeric(n)\n  e <- rnorm(n)\n  for (i in 2:n) {\n    y[i] <- theta * e[i - 1] + e[i]\n  }\n  tsibble(idx = seq_len(n), y = y, index = idx)\n}\n```\n:::\n\n\n>   d. Produce a time plot for the series. How does the plot change as you change $\\theta_1$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nma1(0.6) |> autoplot(y)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex6d-1.png){width=672}\n:::\n\n```{.r .cell-code}\nma1(0.95) |> autoplot(y)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex6d-2.png){width=672}\n:::\n\n```{.r .cell-code}\nma1(0.05) |> autoplot(y)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex6d-3.png){width=672}\n:::\n\n```{.r .cell-code}\nma1(-0.65) |> autoplot(y)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex6d-4.png){width=672}\n:::\n:::\n\n\n>   e. Generate data from an ARMA(1,1) model with $\\phi_{1} = 0.6$, $\\theta_{1}  = 0.6$ and $\\sigma^2=1$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narma11 <- function(phi, theta, n = 100) {\n  y <- numeric(n)\n  e <- rnorm(n)\n  for (i in 2:n) {\n    y[i] <- phi * y[i - 1] + theta * e[i - 1] + e[i]\n  }\n  tsibble(idx = seq_len(n), y = y, index = idx)\n}\narma11(0.6, 0.6) |> autoplot(y)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex6e-1.png){width=672}\n:::\n:::\n\n\n>   f. Generate data from an AR(2) model with $\\phi_{1} =-0.8$, $\\phi_{2} = 0.3$ and $\\sigma^2=1$. (Note that these parameters will give a non-stationary series.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nar2 <- function(phi1, phi2, n = 100) {\n  y <- numeric(n)\n  e <- rnorm(n)\n  for (i in 3:n) {\n    y[i] <- phi1 * y[i - 1] + phi2 * y[i - 2] + e[i]\n  }\n  tsibble(idx = seq_len(n), y = y, index = idx)\n}\nar2(-0.8, 0.3) |> autoplot(y)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex6f-1.png){width=672}\n:::\n:::\n\n\n>   g. Graph the latter two series and compare them.\n\nSee graphs above. The non-stationarity of the AR(2) process has led to increasing oscillations\n\n# fpp3 9.11, Ex7\n\n> Consider `aus_airpassengers`, the total number of passengers (in millions) from Australian air carriers for the period 1970-2011.\n\n>   a. Use `ARIMA()` to find an appropriate ARIMA model. What model was selected. Check that the residuals look like white noise. Plot forecasts for the next 10 periods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_airpassengers |> autoplot(Passengers)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex7a-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfit <- aus_airpassengers |>\n  model(arima = ARIMA(Passengers))\nreport(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Passengers \nModel: ARIMA(0,2,1) \n\nCoefficients:\n          ma1\n      -0.8963\ns.e.   0.0594\n\nsigma^2 estimated as 4.308:  log likelihood=-97.02\nAIC=198.04   AICc=198.32   BIC=201.65\n```\n\n\n:::\n\n```{.r .cell-code}\nfit |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex7a-2.png){width=672}\n:::\n\n```{.r .cell-code}\nfit |> forecast(h = 10) |> autoplot(aus_airpassengers)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex7a-3.png){width=672}\n:::\n:::\n\n\n>    b. Write the model in terms of the backshift operator.\n\n\n::: {.cell}\n\n:::\n\n\n$$(1-B)^2y_t = (1+\\theta B)\\varepsilon_t$$\nwhere $\\varepsilon\\sim\\text{N}(0,\\sigma^2)$, $\\theta = -0.90$ and $\\sigma^2 = 4.31$.\n\n>    c. Plot forecasts from an ARIMA(0,1,0) model with drift and compare these to part a.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_airpassengers |>\n  model(arima = ARIMA(Passengers ~ 1 + pdq(0,1,0))) |>\n  forecast(h = 10) |>\n  autoplot(aus_airpassengers)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex7c-1.png){width=672}\n:::\n:::\n\n\nBoth containing increasing trends, but the ARIMA(0,2,1) model has an implicit trend due to the double-differencing, while the ARIMA(0,1,0) with drift models the trend directly via the trend coefficient. The intervals are narrower when there are fewer differences.\n\n>    d. Plot forecasts from an ARIMA(2,1,2) model with drift and compare these to part b. Remove the constant and see what happens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_airpassengers |>\n  model(arima = ARIMA(Passengers ~ 1 + pdq(2,1,2))) |>\n  forecast(h = 10) |>\n  autoplot(aus_airpassengers)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex7d-1.png){width=672}\n:::\n\n```{.r .cell-code}\naus_airpassengers |>\n  model(arima = ARIMA(Passengers ~ 0 + pdq(2,1,2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A mable: 1 x 1\n         arima\n       <model>\n1 <NULL model>\n```\n\n\n:::\n:::\n\n\nThere is little difference between ARIMA(2,1,2) with drift and ARIMA(0,1,0) with drift. Removing the constant causes an error because the model cannot be estimated.\n\n>    e. Plot forecasts from an ARIMA(0,2,1) model with a constant. What happens?\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_airpassengers |>\n  model(arima = ARIMA(Passengers ~ 1 + pdq(0,2,1))) |>\n  forecast(h = 10) |>\n  autoplot(aus_airpassengers)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Model specification induces a quadratic or higher order polynomial trend. \nThis is generally discouraged, consider removing the constant or reducing the number of differences.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex7e-1.png){width=672}\n:::\n:::\n\n\nThe forecast trend is now quadratic, and there is a warning that this is generally a bad idea.\n\n# fpp3 9.11, Ex8\n\n> For the United States GDP series (from `global_economy`):\n>\n>  a. If necessary, find a suitable Box-Cox transformation for the data;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_economy <- global_economy |>\n  filter(Code == \"USA\")\nus_economy |>\n  autoplot(GDP)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex8a-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlambda <- us_economy |>\n  features(GDP, features = guerrero) |>\n  pull(lambda_guerrero)\nlambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2819443\n```\n\n\n:::\n\n```{.r .cell-code}\nus_economy |>\n  autoplot(box_cox(GDP, lambda))\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex8a-2.png){width=672}\n:::\n:::\n\nIt seems that a Box-Cox transformation may be useful here.\n\n>  b. fit a suitable ARIMA model to the transformed data using `ARIMA()`;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- us_economy |>\n  model(ARIMA(box_cox(GDP, lambda)))\nreport(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: GDP \nModel: ARIMA(1,1,0) w/ drift \nTransformation: box_cox(GDP, lambda) \n\nCoefficients:\n         ar1  constant\n      0.4586  118.1822\ns.e.  0.1198    9.5047\n\nsigma^2 estimated as 5479:  log likelihood=-325.32\nAIC=656.65   AICc=657.1   BIC=662.78\n```\n\n\n:::\n:::\n\n\n>  c. try some other plausible models by experimenting with the orders chosen;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- us_economy |>\n  model(\n    arima010 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(0, 1, 0)),\n    arima011 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(0, 1, 1)),\n    arima012 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(0, 1, 2)),\n    arima013 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(0, 1, 3)),\n    arima110 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 0)),\n    arima111 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 1)),\n    arima112 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 2)),\n    arima113 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 3)),\n    arima210 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(2, 1, 0)),\n    arima211 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(2, 1, 1)),\n    arima212 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(2, 1, 2)),\n    arima213 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(2, 1, 3)),\n    arima310 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(3, 1, 0)),\n    arima311 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(3, 1, 1)),\n    arima312 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(3, 1, 2)),\n    arima313 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(3, 1, 3))\n  )\n```\n:::\n\n\n>  d. choose what you think is the best model and check the residual diagnostics;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  glance() |>\n  arrange(AICc) |>\n  select(.model, AICc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 16 Ã— 2\n   .model    AICc\n   <chr>    <dbl>\n 1 arima110  657.\n 2 arima011  659.\n 3 arima111  659.\n 4 arima210  659.\n 5 arima012  660.\n 6 arima112  661.\n 7 arima211  661.\n 8 arima310  662.\n 9 arima013  662.\n10 arima312  663.\n11 arima311  664.\n12 arima113  664.\n13 arima212  664.\n14 arima313  665.\n15 arima213  666.\n16 arima010  668.\n```\n\n\n:::\n:::\n\n\nThe best according to the AICc values is the ARIMA(1,1,0) w/ drift model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_fit <- us_economy |>\n  model(ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 0)))\nbest_fit |> report()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: GDP \nModel: ARIMA(1,1,0) w/ drift \nTransformation: box_cox(GDP, lambda) \n\nCoefficients:\n         ar1  constant\n      0.4586  118.1822\ns.e.  0.1198    9.5047\n\nsigma^2 estimated as 5479:  log likelihood=-325.32\nAIC=656.65   AICc=657.1   BIC=662.78\n```\n\n\n:::\n\n```{.r .cell-code}\nbest_fit |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex8d2-1.png){width=672}\n:::\n\n```{.r .cell-code}\naugment(best_fit) |> features(.innov, ljung_box, dof = 1, lag = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 4\n  Country       .model                                         lb_stat lb_pvalue\n  <fct>         <chr>                                            <dbl>     <dbl>\n1 United States ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 0))    3.81     0.923\n```\n\n\n:::\n:::\n\n\nThe residuals pass the Ljung-Box test, but the histogram looks like negatively skewed.\n\n>  e. produce forecasts of your fitted model. Do the forecasts look reasonable?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_fit |>\n  forecast(h = 10) |>\n  autoplot(us_economy)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex8e-1.png){width=672}\n:::\n:::\n\n\nThese look reasonable. Let's compare a model with no transformation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- us_economy |> model(ARIMA(GDP))\nfit1 |>\n  forecast(h = 10) |>\n  autoplot(us_economy)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex8e2-1.png){width=672}\n:::\n:::\n\n\nNotice the effect of the transformation on the forecasts. Increase the forecast horizon to  see what happens. Notice also the width of the prediction intervals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_economy |>\n  model(\n    ARIMA(GDP),\n    ARIMA(box_cox(GDP, lambda))\n  ) |>\n  forecast(h = 20) |>\n  autoplot(us_economy)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex8e3-1.png){width=672}\n:::\n:::\n\n\n>  f. compare the results with what you would obtain using `ETS()` (with no transformation).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_economy |>\n  model(ETS(GDP)) |>\n  forecast(h = 10) |>\n  autoplot(us_economy)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex8f-1.png){width=672}\n:::\n:::\n\n\nThe point forecasts are similar, however the ETS forecast intervals are much wider.\n\n# fpp3 9.11, Ex9\n\n> Consider `aus_arrivals`, the quarterly number of international visitors to Australia from several countries for the period 1981 Q1 -- 2012 Q3.\n>    a. Select one country and describe the time plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_arrivals |>\n  filter(Origin == \"Japan\") |>\n  autoplot(Arrivals)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex9a-1.png){width=672}\n:::\n:::\n\n\n* There is an increasing trend to about 1996, and slowly decreasing thereafter.\n* The seasonal shape has changed considerably over time.\n\n>    b. What can you learn from the ACF graph?\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_arrivals |>\n  filter(Origin == \"Japan\") |>\n  ACF(Arrivals) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex9b-1.png){width=672}\n:::\n:::\n\n\n* The slow decay of significant positive values is typical of trended series.\n* The local increases at lags 4, 8, 12, ... are typical of a series with quarterly seasonality.\n\n>    c. What can you learn from the PACF graph?\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_arrivals |>\n  filter(Origin == \"Japan\") |>\n  PACF(Arrivals) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex9c-1.png){width=672}\n:::\n:::\n\n\n* The spikes at lags 5, 9, 13 and 17 indicate quarterly seasonality (as they are 4 quarters apart).\n\n>    d. Produce plots of the double differenced data $(1-B)(1 - B^{4})Y_{t}$. What model do these graphs suggest?\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_arrivals |>\n  filter(Origin == \"Japan\") |>\n  gg_tsdisplay(Arrivals |> difference(lag=4) |> difference(), plot_type = \"partial\")\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex9d-1.png){width=672}\n:::\n:::\n\n\n* The non-seasonal lags suggest an MA(1) component.\n* The seasonal lags suggest a seasonal MA(1) component\n* So the suggested model is an ARIMA(0,1,1)(0,1,1).\n\n>    e. Does `ARIMA()` give the same model that you chose? If not, which model do you think is better?\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_arrivals |>\n  filter(Origin == \"Japan\") |>\n  model(ARIMA(Arrivals))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A mable: 1 x 2\n# Key:     Origin [1]\n  Origin        `ARIMA(Arrivals)`\n  <chr>                   <model>\n1 Japan  <ARIMA(0,1,1)(1,1,1)[4]>\n```\n\n\n:::\n:::\n\n\nThe resulting model has an additional seasonal AR(1) component compared to what I guessed. We can compare the two models based on the AICc statistic:\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_arrivals |>\n  filter(Origin == \"Japan\") |>\n  model(\n    guess = ARIMA(Arrivals ~ pdq(0,1,1) + PDQ(0,1,1)),\n    auto = ARIMA(Arrivals)\n  ) |>\n  glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 9\n  Origin .model     sigma2 log_lik   AIC  AICc   BIC ar_roots  ma_roots \n  <chr>  <chr>       <dbl>   <dbl> <dbl> <dbl> <dbl> <list>    <list>   \n1 Japan  guess  177223035.  -1332. 2670. 2670. 2678. <cpl [0]> <cpl [5]>\n2 Japan  auto   174801727.  -1331. 2669. 2670. 2681. <cpl [4]> <cpl [5]>\n```\n\n\n:::\n:::\n\n\nThe automatic model is only slightly better than my guess based on the AICc statistic.\n\n>    f. Write the model in terms of the backshift operator, then without using the backshift operator.\n\n$$\n  (1-B)(1-B^4)(1-\\Phi B^4)y_t =  (1+\\theta B)(1+\\Theta B^4) \\varepsilon_t\n$$\n$$\n  \\left[1-B - (1 + \\Phi)B^4 + (1 + \\Phi) B^5 + \\Phi B^8 - \\Phi B^9\\right]y_t =  (1+\\theta B + \\Theta B^4 + \\theta\\Theta B^5) \\varepsilon_t\n$$\n$$\n  y_t - y_{t-1} - (1 + \\Phi)y_{t-4} + (1 + \\Phi) y_{t-5} + \\Phi y_{t-8} - \\Phi y_{t-9} =  \\varepsilon_t + \\theta \\varepsilon_{t-1} + \\Theta \\varepsilon_{t-4} + \\theta\\Theta \\varepsilon_{t-5}.\n$$\n$$\n  y_t = y_{t-1} + (1 + \\Phi)y_{t-4} - (1 + \\Phi) y_{t-5} - \\Phi y_{t-8} + \\Phi y_{t-9} +  \\varepsilon_t + \\theta \\varepsilon_{t-1} + \\Theta \\varepsilon_{t-4} + \\theta\\Theta \\varepsilon_{t-5}.\n$$\n\n\n# fpp3 9.11 Ex 10\n\n> Choose a series from `us_employment`, the total employment in different industries in the United States.\n>\n>    a. Produce an STL decomposition of the data and describe the trend and seasonality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleisure <- us_employment |>\n  filter(Title == \"Leisure and Hospitality\")\nleisure |>\n  autoplot(Employed)\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex10a-1.png){width=672}\n:::\n:::\n\n\nThe sudden change in the seasonal pattern is probably due to some change in the definition of who is counted in this group. So our STL decomposition will need to have a small seasonal window to handle that. In addition, the variation changes a little as the level increases, so we will also use a square root transformation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleisure |>\n  model(STL(sqrt(Employed) ~ season(window=7))) |>\n  components() |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex10a2-1.png){width=672}\n:::\n:::\n\n\nWith such a long series, it is not surprising to see the seasonality change a lot over time. The seasonal pattern changed in the 1990s to what is is now. The period of change was rapid, and the seasonal component hasn't fully captured the change, leading to some seasonality ending up in the remainder series. The trend is increasing.\n\n>    b. Do the data need transforming? If so, find a suitable transformation.\n\nYes. A square root did ok -- the remainder series is relatively homoscedastic. No transformation or log transformations led to the remainder series appearing to be heteroscedastic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleisure |> features(Employed, guerrero)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 2\n  Series_ID     lambda_guerrero\n  <chr>                   <dbl>\n1 CEU7000000001          -0.216\n```\n\n\n:::\n:::\n\n\nThe automatically selected transformation is close to logs. My preference is for something a little larger. I think the automatic procedure is confusing the changing seasonality with the increasing variance.\n\n>    c. Are the data stationary? If not, find an appropriate differencing which yields stationary data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleisure |>\n  autoplot(sqrt(Employed) |> difference(lag=12) |> difference())\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex10c-1.png){width=672}\n:::\n:::\n\n\nThe double differenced logged data is close to stationary, although the variance has decreased over time.\n\n>    d. Identify a couple of ARIMA models that might be useful in describing the time series. Which of your models is the best according to their AICc values?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleisure |>\n  gg_tsdisplay(sqrt(Employed) |> difference(lag=12) |> difference(), plot_type=\"partial\")\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex10d-1.png){width=672}\n:::\n:::\n\n\n* This suggests that an ARIMA(2,1,0)(0,1,1) would be a good start.\n* An alternative would be an ARIMA(0,1,2)(0,1,1).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- leisure |>\n  model(\n    arima210011 = ARIMA(sqrt(Employed) ~ pdq(2,1,0) + PDQ(0,1,1)),\n    arima012011 = ARIMA(sqrt(Employed) ~ pdq(0,1,2) + PDQ(0,1,1))\n  )\nglance(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 9\n  Series_ID     .model      sigma2 log_lik   AIC  AICc   BIC ar_roots  ma_roots\n  <chr>         <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl> <list>    <list>  \n1 CEU7000000001 arima210011 0.0380    207. -406. -406. -386. <cpl [2]> <cpl>   \n2 CEU7000000001 arima012011 0.0381    206. -404. -404. -384. <cpl [0]> <cpl>   \n```\n\n\n:::\n:::\n\n\nThe ARIMA(2,1,0)(0,1,1) model is better.\n\n>    e. Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  select(arima210011) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex10e-1.png){width=672}\n:::\n:::\n\n\nThe tails of the residual distribution are too long, and there is significant autocorrelation at lag 11, as well as some smaller significant spikes elsewhere.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- leisure |>\n  model(\n    arima210011 = ARIMA(sqrt(Employed) ~ pdq(2,1,0) + PDQ(0,1,1)),\n    arima012011 = ARIMA(sqrt(Employed) ~ pdq(0,1,2) + PDQ(0,1,1)),\n    auto = ARIMA(sqrt(Employed))\n  )\nglance(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 9\n  Series_ID     .model      sigma2 log_lik   AIC  AICc   BIC ar_roots  ma_roots\n  <chr>         <chr>        <dbl>   <dbl> <dbl> <dbl> <dbl> <list>    <list>  \n1 CEU7000000001 arima210011 0.0380    207. -406. -406. -386. <cpl [2]> <cpl>   \n2 CEU7000000001 arima012011 0.0381    206. -404. -404. -384. <cpl [0]> <cpl>   \n3 CEU7000000001 auto        0.0365    226. -440. -440. -411. <cpl [2]> <cpl>   \n```\n\n\n:::\n\n```{.r .cell-code}\nfit |> select(auto) |> report()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Employed \nModel: ARIMA(2,1,2)(0,1,1)[12] \nTransformation: sqrt(Employed) \n\nCoefficients:\n         ar1      ar2      ma1     ma2     sma1\n      1.6261  -0.9132  -1.4773  0.7937  -0.5443\ns.e.  0.0400   0.0309   0.0535  0.0352   0.0340\n\nsigma^2 estimated as 0.03655:  log likelihood=226.22\nAIC=-440.44   AICc=-440.35   BIC=-411.26\n```\n\n\n:::\n:::\n\n\nThe automatically selected ARIMA(2,1,2)(0,1,1) model is better than either of my selections.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  select(auto) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex10e3-1.png){width=672}\n:::\n:::\n\n\nThe residuals look better, although there is still a significant spike at lag 11.\n\n>    f. Forecast the next 3 years of data. Get the latest figures from https://fred.stlouisfed.org/categories/11 to check the accuracy of your forecasts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc <- fit |>\n  forecast(h = \"3 years\")\nfc |>\n  filter(.model==\"auto\") |>\n  autoplot(us_employment |> filter(year(Month) > 2000))\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex10f-1.png){width=672}\n:::\n:::\n\n\n* Data downloaded from https://fred.stlouisfed.org/series/CEU7000000001\n\n\n::: {.cell}\n\n```{.r .cell-code}\nupdate <- readr::read_csv(\"CEU7000000001.csv\") |>\n  mutate(\n    Month = yearmonth(DATE),\n    Employed = CEU7000000001\n  ) |>\n  select(Month, Employed) |>\n  as_tsibble(index=Month) |>\n  filter(Month >= min(fc$Month))\nfc |> accuracy(update)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 10\n  .model      .type     ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  <chr>       <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 arima012011 Test  -3116. 4110. 3116. -27.2  27.2   NaN   NaN 0.656\n2 arima210011 Test  -3114. 4109. 3114. -27.2  27.2   NaN   NaN 0.656\n3 auto        Test  -3141. 4131. 3141. -27.4  27.4   NaN   NaN 0.656\n```\n\n\n:::\n\n```{.r .cell-code}\nfc |>\n  filter(.model==\"auto\") |>\n  autoplot(us_employment |> filter(year(Month) > 2000)) +\n  geom_line(data=update, aes(x=Month, y=Employed), col='red')\n```\n\n::: {.cell-output-display}\n![](ex9-sol_files/figure-html/ex10f2-1.png){width=672}\n:::\n:::\n\n\nThe initial forecasts look great, but then the pandemic led to a huge impact on the employment in this industry.\n\n>    g. Eventually, the prediction intervals are so wide that the forecasts are not particularly useful. How many years of forecasts do you think are sufficiently accurate to be usable?\n\nGiven the pandemic, about 5 months. Otherwise, perhaps 2--3 years.\n",
    "supporting": [
      "ex9-sol_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}