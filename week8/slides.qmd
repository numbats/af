---
title: ETC3550/ETC5550 Applied forecasting
author: "Week 8: ARIMA models"
format:
  presentation-beamer:
    knitr:
      opts_chunk:
        dev: "cairo_pdf"
    pdf-engine: pdflatex
    fig-width: 7.5
    fig-height: 3.5
    include-in-header: ../header.tex
    keep_tex: yes
---

```{r setup, include=FALSE}
source(here::here("setup.R"))
library(patchwork)
library(purrr)
```

# ARIMA models

## ARIMA models

\begin{tabular}{@{}rl}
\textbf{AR}: & autoregressive (lagged observations as inputs)\\
\textbf{I}: & integrated (differencing to make series stationary)\\
\textbf{MA}: & moving average (lagged errors as inputs)
\end{tabular}

###
An ARIMA model is rarely interpretable in terms of visible data structures like trend and seasonality. But it can capture a huge range of time series patterns.

# Stationarity

## Stationarity
\fontsize{14}{16}\sf
\begin{block}{Definition}
If $\{y_t\}$ is a stationary time series, then for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not depend on $t$.
\end{block}\pause\vspace*{-0.3cm}

Transformations help to **stabilize the variance**.\newline
For ARIMA modelling, we also need to **stabilize the mean**.

## Non-stationarity in the mean
\alert{Identifying non-stationary series}

* time plot.
* The ACF of stationary data drops to zero relatively quickly
* The ACF of non-stationary data decreases slowly.
* For non-stationary data, the value of $r_1$ is often large and positive.

# Differencing

### Differencing

* Differencing helps to **stabilize the mean**.
* First differencing: *change* between consecutive observations: $y'_t = y_t - y_{t-1}$.
* Seasonal differencing: *change* between years: $y'_t = y_t - y_{t-m}$.
* Sometimes two differences need to be applied (but never more).

## Automatic differencing
\fontsize{13}{16}\sf
Statistical tests to determine the required order of differencing.

  1. Augmented Dickey Fuller test: null hypothesis is that the data are \alert{non-stationary} and non-seasonal.
  2. Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test: null hypothesis is that the data are \alert{stationary} and non-seasonal.

\only<2>{\begin{textblock}{4}(10.2,2.7)
\begin{alertblock}{}
H$_0$: non-stationary
\end{alertblock}
\end{textblock}}

\only<2>{\begin{textblock}{3}(12.4,4.05)
\begin{alertblock}{}
H$_0$: stationary
\end{alertblock}
\end{textblock}}

### Seasonal strength

STL decomposition: $y_t = T_t+S_t+R_t$

Seasonal strength $F_s = \max\big(0, 1-\frac{\text{Var}(R_t)}{\text{Var}(S_t+R_t)}\big)$

If $F_s > 0.64$, do one seasonal difference.

## R commands

* Lag 1 difference:\hfill `difference(y)`
* Seasonal difference:\hfill `difference(y, lag = 4)`
* KPSS test:\hfill `unitroot_kpss(y)`
* Seasonal strength:\hfill `feat_stl(y, .period = 4)`
* Automatic first differencing:\hfill `unitroot_ndiffs(y)`
* Automatic seasonal differencing:\newline\mbox{}\hfill `unitroot_nsdiffs(y, .period = 4)`

## Relationship to random walks

A random walk is the process:
$$y_t = y_{t-1} + \varepsilon_t$$
where $\varepsilon_t$ is a white noise variable.

So if data did come from such a process, differencing would give white noise:
$$y_t - y_{t-1} = \varepsilon_t$$

## Relationship to random walks

A seasonal random walk is the process
$$y_t = y_{t-m} + \varepsilon_t$$
where $\varepsilon_t$ is a white noise variable.

So if data did come from such a process, seasonal differencing would give white noise:
$$y_t - y_{t-m} = \varepsilon_t$$
